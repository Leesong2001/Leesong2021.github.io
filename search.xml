<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[DIEN_Code_Read]]></title>
    <url>%2F2021%2F11%2F13%2FDIEN-Code-Read%2F</url>
    <content type="text"><![CDATA[Deep interest evolution network for click-through rate prediction源码阅读 代码实现：DIEN 官方实现 参考博客：罗西的思考 文件简介data file数据文件主要包括：uid_voc.pkl：用户字典，用户名对应的id；mid_voc.pkl：movie字典,item对应的id；cat_voc.pkl：种类字典，category对应的id；item-info：item对应的category信息；reviews-info：review 元数据，格式为：userName，itemName，评分，时间戳，用于进行负采样的数据；local_train_splitByUser：训练数据，一行格式为：label、userName、targetItemName、 目标item类别Name、历史itemName、历史item对应类别Name；local_test_splitByUser：测试数据，格式同训练数据； 1234567891011121314151617181920212223242526272829303132333435363738394041424344import numpyimport jsonimport _pickle as pklimport randomimport gzipdef unicode_to_utf8(d): return dict((key.encode("UTF-8"), value) for (key, value) in d.items())def load_dict(filename): try: with open(filename, 'rb') as f: return unicode_to_utf8(json.load(f)) except: with open(filename, 'rb') as f: return unicode_to_utf8(pkl.load(f))uid_voc="./dien/uid_voc.pkl"mid_voc="./dien/mid_voc.pkl"cat_voc="./dien/cat_voc.pkl"for source_dict in [uid_voc, mid_voc, cat_voc]: dictTmp=load_dict(source_dict) print(source_dict) for key in list(dictTmp.keys())[0:5]: print(key.decode('utf8')," ",dictTmp[key]) with open('./dien/item-info') as itemInfo: print("itemInfo") for i in range(5): line=itemInfo.readline().strip() print(line)with open('./dien/local_test_splitByUser') as local_test_splitByUser: print('local_test_splitByUser') for i in range(5): line=local_test_splitByUser.readline().strip() print(line)with open('./dien/reviews-info') as reviewsInfo: print('reviewsInfo') for i in range(5): line=reviewsInfo.readline().strip() print(line) 各个数据文件的头部输出 1234567891011121314151617181920212223242526272829303132333435363738394041./dien/uid_voc.pklASEARD9XL1EWO 449136AZPJ9LUT0FEPY 0A2NRV79GKAU726 16A2GEQVDX2LL4V3 266686A3R04FKEYE19T6 354817./dien/mid_voc.pkl1594483752 473960738700797 1597161439110239 1934760768240018 2966821572243678 280207./dien/cat_voc.pklResidential 1281Poetry 250Winter Sports 1390Olympics 1128Body Art &amp; Tattoo 947itemInfo0001048791 Books0001048775 Books0001048236 Books0000401048 Books0001019880 Bookslocal_test_splitByUser0 A3BI7R43VUZ1TY B00JNHU0T2 Literature &amp; Fiction 0989464105B00B01691C14778097321608442845 BooksLiterature &amp; FictionBooksBooks1 A3BI7R43VUZ1TY 0989464121 Books 0989464105B00B01691C14778097321608442845 BooksLiterature &amp; FictionBooksBooks0 A2Z3AHJPXG3ZNP B0072YSPJ0 Literature &amp; Fiction 147831096014922314521477603425B00FRKLA6Q BooksBooksBooksUrban1 A2Z3AHJPXG3ZNP B00G4I4I5U Urban 147831096014922314521477603425B00FRKLA6Q BooksBooksBooksUrban0 A2KDDPJUNWC5CA 0316228532 Books 0141326085031026622X031607704609886491791477848150 BooksBooksBooksBooksBooksreviewsInfoA10000012B7CGYKOMPQ4L 000100039X 5.0 1355616000A2S166WSCFIFP5 000100039X 5.0 1071100800A1BM81XB4QHOA3 000100039X 5.0 1390003200A1MOSTXNIO5MPJ 000100039X 5.0 1317081600A2XQ5LZHTD4AFT 000100039X 5.0 1033948800 code filernn.py：对tensorflow中原始的rnn进行修改，目的是将attention同rnn进行结合vecAttGruCell.py: 对GRU源码进行修改，将attention加入其中，设计AUGRU结构data_iterator.py: 数据迭代器，用于数据的不断输入utils.py:一些辅助函数，如dice激活函数、attention score计算等model.py:DIEN模型文件train.py:模型的入口，用于训练数据、保存模型和测试数据 整体架构 模型基类model.Model sekf.mask: tf.placeholder(tf.float32, [None, None], name=&#39;mask&#39;)，形状为[B,T]，T是整个batch中最长的用户历史行为序列长度，mask是一个0-1矩阵，为1的位置表示样本中真实的用户历史行为，为0的位置为填充值，表示该样本数据中用户的历史行为长度小于T 追溯mask的引用和赋值，如下： model.Model.__init__初始化self.mask的占位符 utils.prepare_data准备mask的实际值 train.train在with tf.Session中填充占位符。语句为...mid_mask...=prepare_data(...)得到mask，model.train填充mask。 Attention_layer_1 AUGRU中的din_fcn_attention传递self.mask 12with tf.name_scope('Attention_layer_1'): att_outputs, alphas = din_fcn_attention(self.item_eb, rnn_outputs, ATTENTION_SIZE, self.mask,softmax_stag=1, stag='1_1', mode='LIST', return_alphas=True) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175class Model(object): def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, use_negsampling=False): with tf.name_scope('Inputs'): self.mid_his_batch_ph = tf.placeholder(tf.int32, [None, None], name='mid_his_batch_ph') self.cat_his_batch_ph = tf.placeholder(tf.int32, [None, None], name='cat_his_batch_ph') self.uid_batch_ph = tf.placeholder(tf.int32, [None, ], name='uid_batch_ph') self.mid_batch_ph = tf.placeholder(tf.int32, [None, ], name='mid_batch_ph') self.cat_batch_ph = tf.placeholder(tf.int32, [None, ], name='cat_batch_ph') self.mask = tf.placeholder(tf.float32, [None, None], name='mask') self.seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph') self.target_ph = tf.placeholder(tf.float32, [None, None], name='target_ph') self.lr = tf.placeholder(tf.float64, []) self.use_negsampling = use_negsampling if use_negsampling: self.noclk_mid_batch_ph = tf.placeholder(tf.int32, [None, None, None], name='noclk_mid_batch_ph') # generate 3 item IDs from negative sampling. self.noclk_cat_batch_ph = tf.placeholder(tf.int32, [None, None, None], name='noclk_cat_batch_ph') # Embedding layer with tf.name_scope('Embedding_layer'): self.uid_embeddings_var = tf.get_variable("uid_embedding_var", [n_uid, EMBEDDING_DIM]) tf.summary.histogram('uid_embeddings_var', self.uid_embeddings_var) self.uid_batch_embedded = tf.nn.embedding_lookup(self.uid_embeddings_var, self.uid_batch_ph) self.mid_embeddings_var = tf.get_variable("mid_embedding_var", [n_mid, EMBEDDING_DIM]) tf.summary.histogram('mid_embeddings_var', self.mid_embeddings_var) self.mid_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var, self.mid_batch_ph) self.mid_his_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var, self.mid_his_batch_ph) if self.use_negsampling: self.noclk_mid_his_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var, self.noclk_mid_batch_ph) self.cat_embeddings_var = tf.get_variable("cat_embedding_var", [n_cat, EMBEDDING_DIM]) tf.summary.histogram('cat_embeddings_var', self.cat_embeddings_var) self.cat_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cat_batch_ph) self.cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cat_his_batch_ph) if self.use_negsampling: self.noclk_cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.noclk_cat_batch_ph) self.item_eb = tf.concat([self.mid_batch_embedded, self.cat_batch_embedded], 1) self.item_his_eb = tf.concat([self.mid_his_batch_embedded, self.cat_his_batch_embedded], 2) self.item_his_eb_sum = tf.reduce_sum(self.item_his_eb, 1) if self.use_negsampling: self.noclk_item_his_eb = tf.concat( [self.noclk_mid_his_batch_embedded[:, :, 0, :], self.noclk_cat_his_batch_embedded[:, :, 0, :]], -1) # 0 means only using the first negative item ID. 3 item IDs are inputed in the line 24. self.noclk_item_his_eb = tf.reshape(self.noclk_item_his_eb, [-1, tf.shape(self.noclk_mid_his_batch_embedded)[1], 36]) # cat embedding 18 concate item embedding 18. self.noclk_his_eb = tf.concat([self.noclk_mid_his_batch_embedded, self.noclk_cat_his_batch_embedded], -1) self.noclk_his_eb_sum_1 = tf.reduce_sum(self.noclk_his_eb, 2) self.noclk_his_eb_sum = tf.reduce_sum(self.noclk_his_eb_sum_1, 1) def build_fcn_net(self, inp, use_dice=False): bn1 = tf.layers.batch_normalization(inputs=inp, name='bn1') dnn1 = tf.layers.dense(bn1, 200, activation=None, name='f1') if use_dice: dnn1 = dice(dnn1, name='dice_1') else: dnn1 = prelu(dnn1, 'prelu1') dnn2 = tf.layers.dense(dnn1, 80, activation=None, name='f2') if use_dice: dnn2 = dice(dnn2, name='dice_2') else: dnn2 = prelu(dnn2, 'prelu2') dnn3 = tf.layers.dense(dnn2, 2, activation=None, name='f3') self.y_hat = tf.nn.softmax(dnn3) + 0.00000001 with tf.name_scope('Metrics'): # Cross-entropy loss and optimizer initialization ctr_loss = - tf.reduce_mean(tf.log(self.y_hat) * self.target_ph) self.loss = ctr_loss if self.use_negsampling: self.loss += self.aux_loss tf.summary.scalar('loss', self.loss) self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss) # Accuracy metric self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(self.y_hat), self.target_ph), tf.float32)) tf.summary.scalar('accuracy', self.accuracy) self.merged = tf.summary.merge_all() def auxiliary_loss(self, h_states, click_seq, noclick_seq, mask, stag=None): mask = tf.cast(mask, tf.float32) click_input_ = tf.concat([h_states, click_seq], -1) noclick_input_ = tf.concat([h_states, noclick_seq], -1) click_prop_ = self.auxiliary_net(click_input_, stag=stag)[:, :, 0] noclick_prop_ = self.auxiliary_net(noclick_input_, stag=stag)[:, :, 0] click_loss_ = - tf.reshape(tf.log(click_prop_), [-1, tf.shape(click_seq)[1]]) * mask noclick_loss_ = - tf.reshape(tf.log(1.0 - noclick_prop_), [-1, tf.shape(noclick_seq)[1]]) * mask loss_ = tf.reduce_mean(click_loss_ + noclick_loss_) return loss_ def auxiliary_net(self, in_, stag='auxiliary_net'): bn1 = tf.layers.batch_normalization(inputs=in_, name='bn1' + stag, reuse=tf.AUTO_REUSE) dnn1 = tf.layers.dense(bn1, 100, activation=None, name='f1' + stag, reuse=tf.AUTO_REUSE) dnn1 = tf.nn.sigmoid(dnn1) dnn2 = tf.layers.dense(dnn1, 50, activation=None, name='f2' + stag, reuse=tf.AUTO_REUSE) dnn2 = tf.nn.sigmoid(dnn2) dnn3 = tf.layers.dense(dnn2, 2, activation=None, name='f3' + stag, reuse=tf.AUTO_REUSE) y_hat = tf.nn.softmax(dnn3) + 0.00000001 return y_hat def train(self, sess, inps): if self.use_negsampling: loss, accuracy, aux_loss, _ = sess.run([self.loss, self.accuracy, self.aux_loss, self.optimizer], feed_dict=&#123; self.uid_batch_ph: inps[0], self.mid_batch_ph: inps[1], self.cat_batch_ph: inps[2], self.mid_his_batch_ph: inps[3], self.cat_his_batch_ph: inps[4], self.mask: inps[5], self.target_ph: inps[6], self.seq_len_ph: inps[7], self.lr: inps[8], self.noclk_mid_batch_ph: inps[9], self.noclk_cat_batch_ph: inps[10], &#125;) return loss, accuracy, aux_loss else: loss, accuracy, _ = sess.run([self.loss, self.accuracy, self.optimizer], feed_dict=&#123; self.uid_batch_ph: inps[0], self.mid_batch_ph: inps[1], self.cat_batch_ph: inps[2], self.mid_his_batch_ph: inps[3], self.cat_his_batch_ph: inps[4], self.mask: inps[5], self.target_ph: inps[6], self.seq_len_ph: inps[7], self.lr: inps[8], &#125;) return loss, accuracy, 0 def calculate(self, sess, inps): if self.use_negsampling: probs, loss, accuracy, aux_loss = sess.run([self.y_hat, self.loss, self.accuracy, self.aux_loss], feed_dict=&#123; self.uid_batch_ph: inps[0], self.mid_batch_ph: inps[1], self.cat_batch_ph: inps[2], self.mid_his_batch_ph: inps[3], self.cat_his_batch_ph: inps[4], self.mask: inps[5], self.target_ph: inps[6], self.seq_len_ph: inps[7], self.noclk_mid_batch_ph: inps[8], self.noclk_cat_batch_ph: inps[9], &#125;) return probs, loss, accuracy, aux_loss else: probs, loss, accuracy = sess.run([self.y_hat, self.loss, self.accuracy], feed_dict=&#123; self.uid_batch_ph: inps[0], self.mid_batch_ph: inps[1], self.cat_batch_ph: inps[2], self.mid_his_batch_ph: inps[3], self.cat_his_batch_ph: inps[4], self.mask: inps[5], self.target_ph: inps[6], self.seq_len_ph: inps[7] &#125;) return probs, loss, accuracy, 0 def save(self, sess, path): saver = tf.train.Saver() saver.save(sess, save_path=path) def restore(self, sess, path): saver = tf.train.Saver() saver.restore(sess, save_path=path) print('model restored from %s' % path) data_iterator.py描述：读取local_test/train_splitByUser文件，并随机负采样5次得到负例的用户历史行为序列和对应的物品种类序列，作为source。target则是由一行记录中的第一个属性clk or noclk计算出的二分类label。 返回值 ：return source,target 其中 source.append([uid, mid, cat, mid_list, cat_list, noclk_mid_list, noclk_cat_list])target.append([float(ss[0]), 1 - float(ss[0])])#二分类问题的label值 clk or noclk 123456789101112131415161718192021222324252627import numpyimport jsonimport _pickle as pklimport randomimport gzipfrom script import shuffledef unicode_to_utf8(d): return dict((key.encode("UTF-8"), value) for (key, value) in d.items())def load_dict(filename): try: with open(filename, 'rb') as f: return unicode_to_utf8(json.load(f)) except: with open(filename, 'rb') as f: return unicode_to_utf8(pkl.load(f))def fopen(filename, mode='r'): if filename.endswith('.gz'): return gzip.open(filename, mode) return open(filename, mode) DataIterator调用实例: train.train的with tf.Session开头 train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen, shuffle_each_epoch=False) test_data = DataIterator(test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen) 其中DataIterator接收的初始化参数时上面介绍的数据文件的路径 12345train_file="local_train_splitByUser",test_file="local_test_splitByUser",uid_voc="uid_voc.pkl",mid_voc="mid_voc.pkl",cat_voc="cat_voc.pkl", 123456789101112131415161718class DataIterator: def __init__(self, source, uid_voc, mid_voc, cat_voc, batch_size=128, maxlen=100, skip_empty=False, shuffle_each_epoch=False, sort_by_length=True, max_batch_size=20, minlen=None): if shuffle_each_epoch: self.source_orig = source self.source = shuffle.main(self.source_orig, temporary=True) else: self.source = fopen(source, 'r') source即local_[train,test]_splitByUser，格式为 clk/noclk,userName,targetItemName,targetItemCatName,HistList[itemName],HistList[itemCategoryName] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189 self.source_dicts = [] for source_dict in [uid_voc, mid_voc, cat_voc]: self.source_dicts.append(load_dict(source_dict))#加载 source_dicts=[&#123;userName:uid&#125;,&#123;movieName,mid&#125;,&#123;categoryName:catId&#125;] f_meta = open("item-info", "r")#[itemName,catName] meta_map = &#123;&#125; for line in f_meta: arr = line.strip().split("\t") if arr[0] not in meta_map: meta_map[arr[0]] = arr[1] #meta_map = dict&#123;itemName,catName&#125; #item=(movie，可能是之前的代码 所以叫mid)=(advertise，DIEN中是为了应用在广告CTR预测) self.meta_id_map = &#123;&#125; for key in meta_map: val = meta_map[key] if key in self.source_dicts[1]:#key 是itemId，查询&#123;movieName,mid&#125;是否包含 mid_idx = self.source_dicts[1][key]#包含，则取出mid else: mid_idx = 0#没查到 就置mid_idx=0 if val in self.source_dicts[2]:# val 是catName，查询&#123;categoryName:catId&#125;是否包含 cat_idx = self.source_dicts[2][val] else: cat_idx = 0 self.meta_id_map[mid_idx] = cat_idx #从meta_map=dict&#123;itemName,catName&#125;得到meta_id_map=dict&#123;mid:catid&#125; #f_review数据格式：userName，itemName，评分，时间戳，用于进行负采样的数据； #e.g., A10000012B7CGYKOMPQ4L 000100039X 5.0 1355616000 f_review = open("reviews-info", "r") self.mid_list_for_random = [] for line in f_review: arr = line.strip().split("\t") tmp_idx = 0 if arr[1] in self.source_dicts[1]:#query in &#123;movieName,mid&#125; tmp_idx = self.source_dicts[1][arr[1]] self.mid_list_for_random.append(tmp_idx)#mid_list_for_random 为所有用户交互过的 物品ID self.batch_size = batch_size self.maxlen = maxlen self.minlen = minlen self.skip_empty = skip_empty self.n_uid = len(self.source_dicts[0]) self.n_mid = len(self.source_dicts[1]) self.n_cat = len(self.source_dicts[2]) self.shuffle = shuffle_each_epoch self.sort_by_length = sort_by_length self.source_buffer = [] self.k = batch_size * max_batch_size self.end_of_data = False def get_n(self): return self.n_uid, self.n_mid, self.n_cat def __iter__(self): return self def reset(self): if self.shuffle: self.source = shuffle.main(self.source_orig, temporary=True) else: self.source.seek(0) def next(self): if self.end_of_data: self.end_of_data = False self.reset() raise StopIteration source = [] target = [] if len(self.source_buffer) == 0: for k_ in xrange(self.k): # self.k = batch_size * max_batch_size ss = self.source.readline() #source即local_[train,test]_splitByUser，格式为 clk/noclk,userName,targetItemName,targetItemCatName,HistList[itemName],HistList[itemCategoryName] if ss == "": break self.source_buffer.append(ss.strip("\n").split("\t")) ''' source_buffer=list[list[ clk/noclk,userName,targetItemName,targetItemCatName,HistList[itemName],HistList[itemCategoryName] ],] ''' # sort by history behavior length if self.sort_by_length: #s[4]是HistList[itemId]， his_length = numpy.array([len(s[4].split("")) for s in self.source_buffer]) tidx = his_length.argsort() #argsort函数得到排序后的索引 e.g., argsort([4,2,3,5])==&gt;[2,0,1,3] _sbuf = [self.source_buffer[i] for i in tidx] self.source_buffer = _sbuf else: self.source_buffer.reverse()#?为什么sort_by_length为False 就要reverse if len(self.source_buffer) == 0: self.end_of_data = False self.reset() raise StopIteration try: # actual work here while True: # read from source file and map to word index try: ss = self.source_buffer.pop() except IndexError: break ## ss[1:4]=[userName,targetItemName,targetItemCatName] ## map ss[1:4]--&gt; [uid,mid,catId] uid = self.source_dicts[0][ss[1]] if ss[1] in self.source_dicts[0] else 0 mid = self.source_dicts[1][ss[2]] if ss[2] in self.source_dicts[1] else 0 cat = self.source_dicts[2][ss[3]] if ss[3] in self.source_dicts[2] else 0 #ss[4] = HistList[itemName] #Name 2 idx tmp = [] for fea in ss[4].split(""): m = self.source_dicts[1][fea] if fea in self.source_dicts[1] else 0 tmp.append(m) mid_list = tmp #ss[5] = HistList[catName] #Name 2 idx tmp1 = [] for fea in ss[5].split(""): c = self.source_dicts[2][fea] if fea in self.source_dicts[2] else 0 tmp1.append(c) cat_list = tmp1 # read from source file and map to word index # if len(mid_list) &gt; self.maxlen: # continue if self.minlen != None: if len(mid_list) &lt;= self.minlen: continue if self.skip_empty and (not mid_list): continue noclk_mid_list = [] noclk_cat_list = [] #mid_list是local_train/test_splitByUser文件中的一行数据中的HistList[itemName]对应的HistList[itemId] #即用户的历史行为对应的物品ID序列 for pos_mid in mid_list: noclk_tmp_mid = [] noclk_tmp_cat = [] noclk_index = 0 while True: #随机负采样，但是是从mid_list_for_random中随机选取一个，碰撞则继续随机选取 noclk_mid_indx = random.randint(0, len(self.mid_list_for_random) - 1) noclk_mid = self.mid_list_for_random[noclk_mid_indx] if noclk_mid == pos_mid:#和当前位置的Item 碰撞，稍微有点疑问的就是这样做可能极小概率会和历史列表中的物品碰撞，但又被认为是负例。但应该可以忽略不计，可能需要多学习下其他人的负采样策略。 continue noclk_tmp_mid.append(noclk_mid) noclk_tmp_cat.append(self.meta_id_map[noclk_mid]) noclk_index += 1 if noclk_index &gt;= 5:#负采样5次， break noclk_mid_list.append(noclk_tmp_mid) noclk_cat_list.append(noclk_tmp_cat) #该随机采样循环，为每一个mid_list中的pos_id负采样5次 #clk:histList[item1,item2,....itemN] #noclk:noclkHistList[noHistList1[],noHistList2[],...] source.append([uid, mid, cat, mid_list, cat_list, noclk_mid_list, noclk_cat_list]) target.append([float(ss[0]), 1 - float(ss[0])])#二分类问题的label值 clk or noclk if len(source) &gt;= self.batch_size or len(target) &gt;= self.batch_size: #当前batch数据取满 退出循环 break except IOError: self.end_of_data = True # all sentence pairs in maxibatch filtered out because of length if len(source) == 0 or len(target) == 0: source, target = self.next() return source, target train.py123456789101112import numpyfrom script.data_iterator import DataIteratorfrom script.model import *import timeimport randomimport sysfrom script.utils import *EMBEDDING_DIM = 18HIDDEN_SIZE = 18 * 2ATTENTION_SIZE = 18 * 2best_auc = 0.0 prepare_data描述：读取source数据，返回numpy array格式的 [uid, mid, cat]、[mid_his, cat_his, mid_mask]、target、lengths_x、[noclk_mid_his, noclk_cat_his]，其中[mid_his, cat_his, mid_mask]为padding后的矩阵，形状都为[B,T]。 输入输出：source,target—&gt;input,target 12source=[[uid, mid, cat, mid_list, cat_list, noclk_mid_list, noclk_cat_list],]target=[[ float(ss[0]), 1 - float(ss[0]) ],]#clk or noclk 二分类label 12345if return_neg: return uids, mids, cats, mid_his, cat_his, mid_mask, numpy.array(target), numpy.array(lengths_x), noclk_mid_his, noclk_cat_his else: return uids, mids, cats, mid_his, cat_his, mid_mask, numpy.array(target), numpy.array(lengths_x) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def prepare_data(input, target, maxlen=None, return_neg=False): # x: a list of sentences # B:Batchsize lengths_x = [len(s[4]) for s in input]#历史行为序列长度 seqs_mid = [inp[3] for inp in input]#历史行为 物品序列 seqs_cat = [inp[4] for inp in input]#历史行为 物品种类序列 noclk_seqs_mid = [inp[5] for inp in input]#历史行为 负采样物品序列 noclk_seqs_cat = [inp[6] for inp in input]#历史行为 负采样物品种类序列 if maxlen is not None: new_seqs_mid = [] new_seqs_cat = [] new_noclk_seqs_mid = [] new_noclk_seqs_cat = [] new_lengths_x = [] for l_x, inp in zip(lengths_x, input): #最大历史行为序列长度maxlen，截断过长的历史行为序列，取后面maxlen长度的行为序列 if l_x &gt; maxlen: new_seqs_mid.append(inp[3][l_x - maxlen:]) new_seqs_cat.append(inp[4][l_x - maxlen:]) new_noclk_seqs_mid.append(inp[5][l_x - maxlen:]) new_noclk_seqs_cat.append(inp[6][l_x - maxlen:]) new_lengths_x.append(maxlen) else: new_seqs_mid.append(inp[3]) new_seqs_cat.append(inp[4]) new_noclk_seqs_mid.append(inp[5]) new_noclk_seqs_cat.append(inp[6]) new_lengths_x.append(l_x) lengths_x = new_lengths_x seqs_mid = new_seqs_mid seqs_cat = new_seqs_cat noclk_seqs_mid = new_noclk_seqs_mid noclk_seqs_cat = new_noclk_seqs_cat if len(lengths_x) &lt; 1: return None, None, None, None n_samples = len(seqs_mid)#batchsize--&gt;n_samples maxlen_x = numpy.max(lengths_x)#最长行为序列长度--&gt;maxlen_x neg_samples = len(noclk_seqs_mid[0][0])# [B,T,5] 这个neg_samples的值是负采样个数5 在下面zero矩阵生成时得到验证 mid_his = numpy.zeros((n_samples, maxlen_x)).astype('int64')# [B,T] cat_his = numpy.zeros((n_samples, maxlen_x)).astype('int64')# [B,T] noclk_mid_his=numpy.zeros((n_samples, maxlen_x, neg_samples)).astype('int64')#[B,T,5] noclk_cat_his=numpy.zeros((n_samples, maxlen_x, neg_samples)).astype('int64')#[B,T,5] mid_mask = numpy.zeros((n_samples, maxlen_x)).astype('float32')#[B,T] 这里简单测试一下zip函数，可以看见zip函数对于维度不同的嵌套List，总是按照第一维度zip成list[tuple] 12345678seqs_mid=[[1,2,3],[4,5,6],[7,8,9]]#[B=3,T=3]noclk_seqs_mid=[ [ ['noclk1-1','noclk1-2'],['noclk2-1','noclk2-2'],['noclk3-1','noclk3-2'] ] ,[ ['noclk4-1','noclk4-2'],['noclk5-1','noclk5-2'],['noclk6-1','noclk6-2'] ] ,[ ['noclk7-1','noclk7-2'],['noclk8-1','noclk8-2'],['noclk9-1','noclk9-2'] ] ]##[B=3,T=3,neg_sample=2]#[1,2,3],[['noclk1-1', 'noclk1-2'], ['noclk2-1', 'noclk2-2'], ['noclk3-1', 'noclk3-2']]for ele in list(zip(seqs_mid,noclk_seqs_mid)): print(ele) 打印结果 123([1, 2, 3], [['noclk1-1', 'noclk1-2'], ['noclk2-1', 'noclk2-2'], ['noclk3-1', 'noclk3-2']])([4, 5, 6], [['noclk4-1', 'noclk4-2'], ['noclk5-1', 'noclk5-2'], ['noclk6-1', 'noclk6-2']])([7, 8, 9], [['noclk7-1', 'noclk7-2'], ['noclk8-1', 'noclk8-2'], ['noclk9-1', 'noclk9-2']]) enumerate示例 12345&gt;&gt;&gt;seasons = ['Spring', 'Summer', 'Fall', 'Winter']&gt;&gt;&gt; list(enumerate(seasons))[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]&gt;&gt;&gt; list(enumerate(seasons, start=1)) # 下标从 1 开始[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')] 12345678910111213141516for idx, [s_x, s_y, no_sx, no_sy] in enumerate(zip(seqs_mid, seqs_cat, noclk_seqs_mid, noclk_seqs_cat)): mid_mask[idx, :lengths_x[idx]] = 1.#padding mask，对于历史行为序列不够长的部分padding 0 mid_his[idx, :lengths_x[idx]] = s_x#padding 向zero矩阵中填充历史行为物品ID序列 cat_his[idx, :lengths_x[idx]] = s_y#同上 noclk_mid_his[idx, :lengths_x[idx], :] = no_sx#同上 noclk_cat_his[idx, :lengths_x[idx], :] = no_sy#同上 uids = numpy.array([inp[0] for inp in input])mids = numpy.array([inp[1] for inp in input])cats = numpy.array([inp[2] for inp in input])if return_neg: return uids, mids, cats, mid_his, cat_his, mid_mask, numpy.array(target), numpy.array(lengths_x), noclk_mid_his, noclk_cat_hiselse: return uids, mids, cats, mid_his, cat_his, mid_mask, numpy.array(target), numpy.array(lengths_x) eval1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def eval(sess, test_data, model, model_path): loss_sum = 0. accuracy_sum = 0. aux_loss_sum = 0. nums = 0 stored_arr = [] for src, tgt in test_data: nums += 1 uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(src, tgt, return_neg=True) prob, loss, acc, aux_loss = model.calculate(sess, [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats]) loss_sum += loss aux_loss_sum = aux_loss accuracy_sum += acc prob_1 = prob[:, 0].tolist() target_1 = target[:, 0].tolist() for p, t in zip(prob_1, target_1): stored_arr.append([p, t]) test_auc = calc_auc(stored_arr) accuracy_sum = accuracy_sum / nums loss_sum = loss_sum / nums aux_loss_sum / nums global best_auc if best_auc &lt; test_auc: best_auc = test_auc model.save(sess, model_path) return test_auc, loss_sum, accuracy_sum, aux_loss_sumdef train( train_file="local_train_splitByUser", test_file="local_test_splitByUser", uid_voc="uid_voc.pkl", mid_voc="mid_voc.pkl", cat_voc="cat_voc.pkl", batch_size=128, maxlen=100, test_iter=100, save_iter=100, model_type='DNN', seed=2,): model_path = "dnn_save_path/ckpt_noshuff" + model_type + str(seed) best_model_path = "dnn_best_model/ckpt_noshuff" + model_type + str(seed) gpu_options = tf.GPUOptions(allow_growth=True) with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess: train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen, shuffle_each_epoch=False) test_data = DataIterator(test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen) n_uid, n_mid, n_cat = train_data.get_n() train/test_data: source, target 12source=[[uid, mid, cat, mid_list, cat_list, noclk_mid_list, noclk_cat_list],]target=[[ float(ss[0]), 1 - float(ss[0]) ],]#clk or noclk 二分类label 123456789101112131415161718192021222324252627282930313233343536373839if model_type == 'DNN': model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'PNN': model = Model_PNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'Wide': model = Model_WideDeep(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'DIN': model = Model_DIN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'DIN-V2-gru-att-gru': model = Model_DIN_V2_Gru_att_Gru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'DIN-V2-gru-gru-att': model = Model_DIN_V2_Gru_Gru_att(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'DIN-V2-gru-qa-attGru': model = Model_DIN_V2_Gru_QA_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'DIN-V2-gru-vec-attGru': model = Model_DIN_V2_Gru_Vec_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)elif model_type == 'DIEN': model = Model_DIN_V2_Gru_Vec_attGru_Neg(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)else: print("Invalid model_type : %s", model_type) return# model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE)sess.run(tf.global_variables_initializer())sess.run(tf.local_variables_initializer())sys.stdout.flush()print( ' test_auc: %.4f ---- test_loss: %.4f ---- test_accuracy: %.4f ---- test_aux_loss: %.4f' % eval( sess, test_data, model, best_model_path))sys.stdout.flush()start_time = time.time()iter = 0lr = 0.001for itr in range(3): loss_sum = 0.0 accuracy_sum = 0. aux_loss_sum = 0. for src, tgt in train_data: uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(src,tgt,maxlen,return_neg=True) 训练数据local_test_splitByUser中的一行数据为[uid, mid, cat, mid_list, cat_list, noclk_mid_list, noclk_cat_list] uids, mids, cats分别为uid, mid, cat的Lits，形状为[B,] mid_his, cat_his分别为不等长mid_list、cat_list填充的numpy矩阵，形状为[B, maxHistLen]， 记做[B,T] mid_mask则为mid_list的padding mask，形状为[B,T]，具体的可参见train.prepare_data target为 numpy.array(target)表示二分类label值，clk or noclk，形状为[B,2] sl 为 numpy.array(lengths_x)表示真实的用户历史行为序列长度（seq_len），形状为[B,] noclk_mids, noclk_cats为noclk_mid_his, noclk_cat_his表示负采样历史行为序列，形状为[B,T,neg_sample=5] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081 loss, acc, aux_loss = model.train(sess, [uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, lr, noclk_mids, noclk_cats]) loss_sum += loss accuracy_sum += acc aux_loss_sum += aux_loss iter += 1 sys.stdout.flush() if (iter % test_iter) == 0: print('iter: %d ----&gt; train_loss: %.4f ---- train_accuracy: %.4f ---- tran_aux_loss: %.4f' % \ (iter, loss_sum / test_iter, accuracy_sum / test_iter, aux_loss_sum / test_iter)) print( ' test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.4f ---- test_aux_loss: %.4f' % eval( sess, test_data, model, best_model_path)) loss_sum = 0.0 accuracy_sum = 0.0 aux_loss_sum = 0.0 if (iter % save_iter) == 0: print('save model iter: %d' % (iter)) model.save(sess, model_path + "--" + str(iter)) lr *= 0.5def test( train_file="local_train_splitByUser", test_file="local_test_splitByUser", uid_voc="uid_voc.pkl", mid_voc="mid_voc.pkl", cat_voc="cat_voc.pkl", batch_size=128, maxlen=100, model_type='DNN', seed=2): model_path = "dnn_best_model/ckpt_noshuff" + model_type + str(seed) gpu_options = tf.GPUOptions(allow_growth=True) with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess: train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen) test_data = DataIterator(test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen) n_uid, n_mid, n_cat = train_data.get_n() if model_type == 'DNN': model = Model_DNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'PNN': model = Model_PNN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'Wide': model = Model_WideDeep(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'DIN': model = Model_DIN(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'DIN-V2-gru-att-gru': model = Model_DIN_V2_Gru_att_Gru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'DIN-V2-gru-gru-att': model = Model_DIN_V2_Gru_Gru_att(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'DIN-V2-gru-qa-attGru': model = Model_DIN_V2_Gru_QA_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'DIN-V2-gru-vec-attGru': model = Model_DIN_V2_Gru_Vec_attGru(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) elif model_type == 'DIEN': model = Model_DIN_V2_Gru_Vec_attGru_Neg(n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE) else: print("Invalid model_type : %s", model_type) return model.restore(sess, model_path) print('test_auc: %.4f ----test_loss: %.4f ---- test_accuracy: %.4f ---- test_aux_loss: %.4f' % eval(sess, test_data, model, model_path))if __name__ == '__main__': if len(sys.argv) == 4: SEED = int(sys.argv[3]) else: SEED = 3 tf.set_random_seed(SEED) numpy.random.seed(SEED) random.seed(SEED) if sys.argv[1] == 'train': train(model_type=sys.argv[2], seed=SEED) elif sys.argv[1] == 'test': test(model_type=sys.argv[2], seed=SEED) else: print('do nothing...') utils.py a_{t}=\frac{exp(h_{t}We_{a})}{\sum_{j=1}^{T}exp(h_{j}We_{a})}该注意力公式的作用是对不同兴趣加以区分，因为用户在面对不同目标广告时，不同方向的兴趣向量作用/影响/信息量不同。（家居兴趣和运动兴趣对于运动服装广告的贡献不同） 12with tf.name_scope('Attention_layer_1'): att_outputs, alphas = din_fcn_attention(self.item_eb, rnn_outputs, ATTENTION_SIZE, self.mask,softmax_stag=1, stag='1_1', mode='LIST', return_alphas=True) 1234567891011121314151617181920212223242526272829303132def din_fcn_attention(query, facts, attention_size, mask, stag='null', mode='SUM', softmax_stag=1, time_major=False, return_alphas=False, forCnn=False) ''' query ：候选广告，shape: [BatchSize, embSize], 即i_emb； facts ：用户历史行为，shape: [B, T, H], 即兴趣提取层输出的h_emb，T是padding后的长度，H是hiddenSize，每个长H的emb代表一个item背后的直接兴趣； mask : 用户的真实历史行为序列置1，不够长的padding 0。具体参见prepare_data，shape: [B, T]； ''' if isinstance(facts, tuple): # In case of Bi-RNN, concatenate the forward and the backward RNN outputs. facts = tf.concat(facts, 2) if len(facts.get_shape().as_list()) == 2: facts = tf.expand_dims(facts, 1) if time_major: # (T,B,D) =&gt; (B,T,D) facts = tf.array_ops.transpose(facts, [1, 0, 2]) # Trainable parameters mask = tf.equal(mask, tf.ones_like(mask)) facts_size = facts.get_shape().as_list()[-1] # hidden size of the RNN layer querry_size = query.get_shape().as_list()[-1] # embSize ''' 因为超参数设置为 EMBEDDING_DIM = 18 HIDDEN_SIZE = 18 * 2 item_embd=concat(itemIDembd,catEmbd) 所以此处的query的embSize的维度在数值上相等。但由于超参数可以修改，下面两行query过了一个prelu(MLLP)后，维度变为facts_size=hiddensize ''' query = tf.layers.dense(query, facts_size, activation=None, name='f1' + stag) query = prelu(query) #prelu(MLLP(query))--&gt;shape=[B,H] 如上图所示，目标物品（广告）的嵌入向量需要拷贝T份，分别与兴趣提取层输出$h(i)$一起送入attention模块 123456789101112131415161718192021222324252627282930313233343536# 1. 转换query维度，变成历史维度T # query是[B, H]，转换到 queries 维度为(B, T, H)，为了让pos_item和用户行为序列中每个元素计算权重 # 此时query是 Tensor("concat:0", shape=(?, 36), dtype=float32) # tf.shape(keys)[1] 结果就是 T，query是[B, H]，经过tile，就是把第一维按照 T 展开，得到[B, T * H] queries = tf.tile(query, [1, tf.shape(facts)[1]]) # [B, T * H], 想象成贴瓷砖 # 此时 queries 是 Tensor("Attention_layer/Tile:0", shape=(?, ?), dtype=float32) # queries 需要 reshape 成和 facts 相同的大小: [B, T, H] queries = tf.reshape(queries, tf.shape(facts)) # [B, T * H] -&gt; [B, T, H] # 此时 queries 是 Tensor("Attention_layer/Reshape:0", shape=(?, ?, 36), dtype=float32) # 2. 这部分目的就是为了在MLP之前多做一些捕获行为item和候选item之间关系的操作：加减乘除等。 # 得到 Local Activation Unit 的输入。即 候选广告 queries 对应的 emb，用户历史行为序列 facts# 对应的 embed, 再加上它们之间的交叉特征, 进行 concat 后的结果 din_all = tf.concat([queries, facts, queries-facts, queries*facts], axis=-1)#concat([[[B, T, H] ],[[B, T, H] ],[[B, T, H] ],[[B, T, H] ]],axis=-1)--&gt;[B,T,4*H] # 3. attention操作，通过几层MLP获取权重，这个DNN 网络的输出节点为 1 d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att' + stag) d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att' + stag) d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att' + stag) # 上一层 d_layer_3_all 的 shape 为 [B, T, 1] # 下一步 reshape 为 [B, 1, T], axis=2 这一维表示 T 个用户行为序列分别对应的权重参数 d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(facts)[1]]) scores = d_layer_3_all # attention的输出, [B, 1, T] # 4. 得到有真实意义的score # key_masks = tf.sequence_mask(facts_length, tf.shape(facts)[1]) # [B, T] key_masks = tf.expand_dims(mask, 1) # [B, 1, T] # padding的mask后补一个很小的负数，这样后面计算 softmax 时, e^&#123;x&#125; 结果就约等于 0 paddings = tf.ones_like(scores) * (-2 ** 32 + 1) # 注意初始化为极小值# [B, 1, T] padding操作，为了忽略了padding对总体的影响，代码中利用tf.where将padding的向量(每个样本序列中空缺的商品)权重置为极小值(-2 ** 32 + 1)，而不是直接置0 ,因为还需要经过softmax层计算if not forCnn: scores = tf.where(key_masks, scores, paddings) # [B, 1, T] tf.where( condition, x=None, y=None, name=None ) 根据传入的bool矩阵mask的值，返回对应位置的x或者y的值。若condition[idxList]为True，返回对应位置的x[idxList]，反之返回y[idxList]。 具体到DIEN，scores = tf.where(key_masks, scores, paddings),对于每一个长度不够T的用户行为序列，其真实行为序列处的attention分数为计算出的真实值scores，而长度不够的剩余T-len_x的部分填充为极小值-2 ** 32 + 1 12345678910111213141516171819202122# 5. Scale # attention的标准操作，做完scaled后再送入softmax得到最终的权重。 # scores = scores / (facts.get_shape().as_list()[-1] ** 0.5) # 6. Activation，得到归一化后的权重 if softmax_stag: scores = tf.nn.softmax(scores) # [B, 1, T] # 7. 得到了正确的权重 scores 以及用户历史行为序列 facts, 再进行矩阵相乘得到用户的兴趣表征 # Weighted sum， if mode == 'SUM': # scores 的大小为 [B, 1, T], 表示每条历史行为的权重, # facts 为历史行为序列, 大小为 [B, T, H]; # 两者用矩阵乘法做, 得到的结果 output 就是 [B, 1, H] # B * 1 * H 三维矩阵相乘，相乘发生在后两维，即 B * (( 1 * T ) * ( T * H )) # 这里的output是attention计算出来的权重，即论文公式(3)里的w， output = tf.matmul(scores, facts) # [B, 1, H] # output = tf.reshape(output, [-1, tf.shape(facts)[-1]]) else: # scores shape [B, 1, T]==&gt; [B,T] scores = tf.reshape(scores, [-1, tf.shape(facts)[1]]) # facts shape [B,T,H] # 先把scores在最后增加一维，然后进行哈达码积，[B, T, H] x [B, T, 1] = [B, T, H] output = facts * tf.expand_dims(scores, -1) # [B, T, H] x [B, T, 1] 广播机制 相当于对每一个用户兴趣提取层的输出h_&#123;i&#125;乘上一个attention分数 output = tf.reshape(output, tf.shape(facts)) # [B, T, H] return output]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LatexLearn]]></title>
    <url>%2F2021%2F11%2F12%2FLatexLearn%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[PytorchLearn]]></title>
    <url>%2F2021%2F11%2F12%2FPytorchLearn%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Knowledge_Graph_based_Intent_Network_Code]]></title>
    <url>%2F2021%2F11%2F10%2FKnowledge-Graph-based-Intent-Network-Code-Read%2F</url>
    <content type="text"><![CDATA[Knowledge_Graph_based_Intent_Network 源码阅读笔记 源码目录结构12345678910111213141516Knowledge_Graph_based_Intent_Network|--data| |--alibaba-fashion| |--amazon-book| |--last-fm|--modules| |--KGIN.py|--training_log|--utils| |--data_loader.py| |--evaluate.py| |--helper.py| |--metric.py| |--parser.py|--main.py|--README.m\d data \Large e_{i}^{(l)}=\frac{1}{|N_{i}|}\sum_{(r,v)\in N_{i}}{e_{r}\odot e_{v}^{l-1}}\\ \Large N_{i}=\{(r,v)|(i,r,v)\in G\}\\ \Large e_{i}^{(l)}=f_{KG}(\{(e_{i}^{l-1},e_{r},e_{v}^{l-1})|(r,v)\in N_{i}\})\\ \Large e_{i}^{(l)}=\sum_{s\in N_{i}^{l}}{\frac{e_{r_{1}}}{|N_{s_{1}}|}\odot\frac{e_{r_{2}}}{|N_{s_{2}}|}\odot \cdots \odot\frac{e_{r_{l}}}{|N_{s_{l}}|}\odot e_{s_{l}}^{(0)}}data文件夹下存放着三个数据集：alibaba-fashion，amazon-book，last-fm 格式大抵相同，以amazon-book为例，完整数据在Amazon Review Data 可能的格式如下 123456789101112131415161718&#123; "asin": "0000031852", "title": "Girls Ballet Tutu Zebra Hot Pink", "feature": ["Botiquecutie Trademark exclusive Brand", "Hot Pink Layered Zebra Print Tutu", "Fits girls up to a size 4T", "Hand wash / Line Dry", "Includes a Botiquecutie TM Exclusive hair flower bow"], "description": "This tutu is great for dress up play for your little ballerina. Botiquecute Trade Mark exclusive brand. Hot Pink Zebra print tutu.", "price": 3.17, "imageURL": "http://ecx.images-amazon.com/images/I/51fAmVkTbyL._SY300_.jpg", "imageURLHighRes": "http://ecx.images-amazon.com/images/I/51fAmVkTbyL.jpg", "also_buy": ["B00JHONN1S", "B002GZGI4E", "B001T9NUFS", "B002R0F7FE", "B00E1YRI4C", "B008UBQZKU", "B00D103F8U", "B007R2RM8W"], "also_viewed": ["B002BZX8Z6" "B00E79VW6Q", "B00D10CLVW", "B00B0AVO54", "B00E95LC8Q", "B00GOR92SO", "B007ZN5Y56", "B00AL2569W", "B00B608000", "B008F0SMUC", "B00BFXLZ8M"], "salesRank": &#123;"Toys &amp; Games": 211836&#125;, "brand": "Coxlures", "categories": [["Sports &amp; Outdoors", "Other Sports", "Dance"]]&#125; 这里下载imageURL中指示的图片，如下 符合description中描述的This tutu is great for dress up play for your little ballerina. Botiquecute Trade Mark exclusive brand. Hot Pink Zebra print tutu.（这套芭蕾短裙非常适合你的小芭蕾舞演员装扮。Botiquecute商标独家品牌。粉色斑马印花短裙） amazon-bookentity_list.txt 1234org_id remap_idm.045wq1q 0m.03_28m 1... item_list.txt 12345org_id remap_id freebase_id0553092626 0 m.045wq1q0393316041 1 m.03_28m038548254X 2 m.0h2q1cq... kg_final.txt 1234524915 0 2491624917 1 511724918 0 2491724919 1 24920... relation_list.txt 123456org_id remap_idhttp://rdf.freebase.com/ns/type.object.type 0http://rdf.freebase.com/ns/type.type.instance 1http://rdf.freebase.com/ns/book.written_work.copyright_date 2http://www.w3.org/1999/02/22-rdf-syntax-ns#type 3`... train.txt 123450 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 311 32 33 34 352 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 513 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66... test.txt 123450 4828 774 207 7460 7465 3768 10221 224351 28252 10184 3500 10241 10185 17003 5365 699 3016 6675... user_list.txt 123456org_id remap_idA3RTKL9KB8KLID 0A38LAIK2N83NH0 1A3PPXVR5J6U2JD 2A2ULDDL3MLJPUR 3... 源码中的一些技术/库简介tqdmTqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)。 123for char in tqdm(["a", "b", "c", "d"]): #do something pass 效果 1100%|███████████████████████████████████| 857K/857K [00:04&lt;00:00, 246Kloc/s] 图结构表示拉普拉斯矩阵 稀疏矩阵1scipy.CSR main.py设置随机种子12345678"""fix the random seed""" seed = 2020 random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False 从命令行中读取参数 命令行示例：python main.py --dataset last-fm --dim 64 --lr 0.0001 --sim_regularity 0.0001 --batch_size 1024 --node_dropout True --node_dropout_rate 0.5 --mess_dropout True --mess_dropout_rate 0.1 --gpu_id 0 --context_hops 3 1234"""read args""" global args, device args = parse_args() device = torch.device("cuda:"+str(args.gpu_id)) if args.cuda else torch.device("cpu") 构建数据集 123456789"""build dataset""" train_cf, test_cf, user_dict, n_params, graph, mat_list = load_data(args) adj_mat_list, norm_mat_list, mean_mat_list = mat_list n_users = n_params['n_users'] n_items = n_params['n_items'] n_entities = n_params['n_entities'] n_relations = n_params['n_relations'] n_nodes = n_params['n_nodes'] 深入到load_data(args)中查看 12345678910def load_data(model_args): global args args = model_args directory = args.data_path + args.dataset + '/' print('reading train and test user-item set ...') train_cf = read_cf(directory + 'train.txt') test_cf = read_cf(directory + 'test.txt') remap_item(train_cf, test_cf) read_cf()在data_loader.py中，输入参数为file_name，读取的数据集文件介绍如下 train.txt Train file. Each line is a user with her/his positive interactions with items: (userID and a list of itemID). test.txt Test file (positive instances). Each line is a user with her/his positive interactions with items: (userID and a list of itemID). Note that here we treat all unobserved interactions as the negative instances when reporting performance. read_cf()的作用是得到每个用户的交互记录，具体的做法是将a list of itemID打散和userID配对，返回numpyArray[,] remap_item的作用是输入 numpyArray[,]，返回一个KV字典结构dict{ uid : List[itemid] }，同时给n_users和n_items分别赋值为测试集和训练集并集中最大的的用户ID和物品ID 12print('combinating train_cf and kg data ...')triplets = read_triplets(directory + 'kg_final.txt') read_triplets根据输入参数inverse_r决定最后得到的triplets的relation是否反向，若inverse_r为False，则直接读入data/kg_final.txt中的三元组数据，同时完成n_entities, n_relations, n_nodes的赋值 12print('building the graph ...')graph, relation_dict = build_graph(train_cf, triplets) ckg_graph.add_edge(h_id, t_id, key=r_id) 如上，graph是调用networkx，根据triplets三元组中的头结点、尾结点、关系ID建立的有向图，该对象对应着知识图谱。通过将关系ID传入add_edge函数的key参数以区分不同的边。relation_dict则是一个字典结构dict{ relation : List[]}，其中relation_dict[0]存储的是用户的交互数据，即List[, ]，而relation[非0]存储的是知识图谱中的指向关系，即List[, ] 12print('building the adj mat ...')adj_mat_list, norm_mat_list, mean_mat_list = build_sparse_relational_graph(relation_dict) 深入到build_sparse_relational_graph()中分析 1234567891011121314151617181920212223def build_sparse_relational_graph(relation_dict): def _bi_norm_lap(adj): # D^&#123;-1/2&#125;AD^&#123;-1/2&#125; rowsum = np.array(adj.sum(1)) d_inv_sqrt = np.power(rowsum, -0.5).flatten() d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0. d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt) bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt) return bi_lap.tocoo() def _si_norm_lap(adj): # D^&#123;-1&#125;A rowsum = np.array(adj.sum(1)) d_inv = np.power(rowsum, -1).flatten() d_inv[np.isinf(d_inv)] = 0. d_mat_inv = sp.diags(d_inv) norm_adj = d_mat_inv.dot(adj) return norm_adj.tocoo() _bi_norm_lap(adj)：对每一个矩阵A（adj）进行归一化，生成拉普拉斯矩阵；先算出度矩阵D（rowsum），然后对D做处理，没值的赋0，然后铺在对角线，最后对adj进行对称归一化，生成拉普拉斯矩阵 。拉普拉斯矩阵是图论中用到的一种重要矩阵，给定一个有n个顶点的图 G=(V,E)，其拉普拉斯矩阵被定义为 L = D-A，D为图的度矩阵，A为图的邻接矩阵。 123456789adj_mat_list = []print("Begin to build sparse relation matrix ...")for r_id in tqdm(relation_dict.keys()): np_mat = np.array(relation_dict[r_id]) if r_id == 0:# user-item 交互数据 cf = np_mat.copy() cf[:, 1] = cf[:, 1] + n_users # [0, n_items) -&gt; [n_users, n_users+n_items) vals = [1.] * len(cf)# &lt;user,item,interactionRate=1.0&gt; adj = sp.coo_matrix((vals, (cf[:, 0], cf[:, 1])), shape=(n_nodes, n_nodes)) scipy.coo_matrix可根据输入构建稀疏矩阵，输入格式为[row,col,value]，具体的调用方式如上所示coo_matrix((val, (row, col)), shape=(rowSize, colSize)) 12345else: #知识图谱中的实体-实体关系 vals = [1.] * len(np_mat) adj = sp.coo_matrix((vals, (np_mat[:, 0], np_mat[:, 1])), shape=(n_nodes, n_nodes))adj_mat_list.append(adj) adj_mat_list：List[coo_matrix[user,item],coo_matrix[entity,entity],coo_matrix[entity,entity]……] 其索引为 0号relation：user-item interaction 以及 非0号relation：entity-entity知识图谱上的实体关系 1234567norm_mat_list = [_bi_norm_lap(mat) for mat in adj_mat_list]mean_mat_list = [_si_norm_lap(mat) for mat in adj_mat_list]# interaction: user-&gt;item, [n_users, n_entities]norm_mat_list[0] = norm_mat_list[0].tocsr()[:n_users, n_users:].tocoo()mean_mat_list[0] = mean_mat_list[0].tocsr()[:n_users, n_users:].tocoo() return adj_mat_list, norm_mat_list, mean_mat_list norm_mat_list[0] = norm_mat_list[0].tocsr()[:n_users, n_users:].tocoo()看起来是将矩阵[user&amp;item,user&amp;item]切分，仅保留[user,item]交互矩阵，即行代表user，列代表item。对这部分代码做一个简单测试，输出切分前后的矩阵形状。 12norm_mat_list[0] = norm_mat_list[0].tocsr().shape=(129955, 129955)norm_mat_list[0] = norm_mat_list[0].tocsr()[:n_users, n_users:].shape=(23566, 106389) 验证了上面的分析。 1234567891011121314n_params = &#123; 'n_users': int(n_users), 'n_items': int(n_items), 'n_entities': int(n_entities), 'n_nodes': int(n_nodes), 'n_relations': int(n_relations)&#125;user_dict = &#123; 'train_user_set': train_user_set, 'test_user_set': test_user_set&#125; return train_cf, test_cf, user_dict, n_params, graph, \ [adj_mat_list, norm_mat_list, mean_mat_list] 协同过滤数据123"""cf data""" train_cf_pairs = torch.LongTensor(np.array([[cf[0], cf[1]] for cf in train_cf], np.int32)) test_cf_pairs = torch.LongTensor(np.array([[cf[0], cf[1]] for cf in test_cf], np.int32)) 模型定义12"""define model"""model = Recommender(n_params, args, graph, mean_mat_list[0]).to(device) 优化器定义12"""define optimizer""" optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) 模型训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263cur_best_pre_0 = 0 stopping_step = 0 should_stop = False print("start training ...") for epoch in range(args.epoch): """training CF""" # shuffle training data #train_cf &lt;user,item&gt; 每一行对应着一个用户物品对的交互记录 index = np.arange(len(train_cf)) np.random.shuffle(index)# 打乱 train_cf_pairs = train_cf_pairs[index]# 取打乱后的train_cf_pairs """training""" loss, s, cor_loss = 0, 0, 0 train_s_t = time() while s + args.batch_size &lt;= len(train_cf): batch = get_feed_dict(train_cf_pairs, s, s + args.batch_size, user_dict['train_user_set']) batch_loss, _, _, batch_cor = model(batch) batch_loss = batch_loss optimizer.zero_grad() batch_loss.backward() optimizer.step() loss += batch_loss cor_loss += batch_cor s += args.batch_size train_e_t = time() if epoch % 10 == 9 or epoch == 1: """testing""" test_s_t = time() ret = test(model, user_dict, n_params) test_e_t = time() train_res = PrettyTable() train_res.field_names = ["Epoch", "training time", "tesing time", "Loss", "recall", "ndcg", "precision", "hit_ratio"] train_res.add_row( [epoch, train_e_t - train_s_t, test_e_t - test_s_t, loss.item(), ret['recall'], ret['ndcg'], ret['precision'], ret['hit_ratio']] ) print(train_res) # ********************************************************* # early stopping when cur_best_pre_0 is decreasing for ten successive steps. cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0, stopping_step, expected_order='acc', flag_step=10) if should_stop: break """save weight""" if ret['recall'][0] == cur_best_pre_0 and args.save: torch.save(model.state_dict(), args.out_dir + 'model_' + args.dataset + '.ckpt') else: # logging.info('training loss at epoch %d: %f' % (epoch, loss.item())) print('using time %.4f, training loss at epoch %d: %.4f, cor: %.6f' % (train_e_t - train_s_t, epoch, loss.item(), cor_loss.item())) print('early stopping at %d, recall@20:%.4f' % (epoch, cur_best_pre_0)) data_loader.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171import numpy as npfrom tqdm import tqdmimport networkx as nximport scipy.sparse as spimport randomfrom time import timefrom collections import defaultdictimport warningswarnings.filterwarnings('ignore')n_users = 0n_items = 0n_entities = 0n_relations = 0n_nodes = 0train_user_set = defaultdict(list)test_user_set = defaultdict(list)def read_cf(file_name): inter_mat = list() lines = open(file_name, "r").readlines() for l in lines: tmps = l.strip() inters = [int(i) for i in tmps.split(" ")] u_id, pos_ids = inters[0], inters[1:] pos_ids = list(set(pos_ids)) for i_id in pos_ids: inter_mat.append([u_id, i_id]) return np.array(inter_mat)def remap_item(train_data, test_data): global n_users, n_items n_users = max(max(train_data[:, 0]), max(test_data[:, 0])) + 1 n_items = max(max(train_data[:, 1]), max(test_data[:, 1])) + 1 for u_id, i_id in train_data: train_user_set[int(u_id)].append(int(i_id)) for u_id, i_id in test_data: test_user_set[int(u_id)].append(int(i_id))def read_triplets(file_name): global n_entities, n_relations, n_nodes can_triplets_np = np.loadtxt(file_name, dtype=np.int32) can_triplets_np = np.unique(can_triplets_np, axis=0) if args.inverse_r: # get triplets with inverse direction like &lt;entity, is-aspect-of, item&gt; inv_triplets_np = can_triplets_np.copy() inv_triplets_np[:, 0] = can_triplets_np[:, 2] inv_triplets_np[:, 2] = can_triplets_np[:, 0] inv_triplets_np[:, 1] = can_triplets_np[:, 1] + max(can_triplets_np[:, 1]) + 1 # consider two additional relations --- 'interact' and 'be interacted' can_triplets_np[:, 1] = can_triplets_np[:, 1] + 1 inv_triplets_np[:, 1] = inv_triplets_np[:, 1] + 1 # get full version of knowledge graph triplets = np.concatenate((can_triplets_np, inv_triplets_np), axis=0) else: # consider two additional relations --- 'interact'. can_triplets_np[:, 1] = can_triplets_np[:, 1] + 1 triplets = can_triplets_np.copy() n_entities = max(max(triplets[:, 0]), max(triplets[:, 2])) + 1 # including items + users n_nodes = n_entities + n_users n_relations = max(triplets[:, 1]) + 1 return tripletsdef build_graph(train_data, triplets): ckg_graph = nx.MultiDi_init_weightGraph() rd = defaultdict(list) print("Begin to load interaction triples ...") for u_id, i_id in tqdm(train_data, ascii=True): rd[0].append([u_id, i_id]) print("\nBegin to load kno_init_weightwledge graph triples ...") for h_id, r_id, t_id in tqdm(triplets, ascii=True): ckg_graph.add_edge(h_id, t_id, key=r_id) rd[r_id].append([h_id, t_id]) return ckg_graph, rddef build_sparse_relational_graph(relation_dict): def _bi_norm_lap(adj): # D^&#123;-1/2&#125;AD^&#123;-1/2&#125; rowsum = np.array(adj.sum(1)) d_inv_sqrt = np.power(rowsum, -0.5).flatten() d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0. d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt) bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt) return bi_lap.tocoo() def _si_norm_lap(adj): # D^&#123;-1&#125;A rowsum = np.array(adj.sum(1)) d_inv = np.power(rowsum, -1).flatten() d_inv[np.isinf(d_inv)] = 0. d_mat_inv = sp.diags(d_inv) norm_adj = d_mat_inv.dot(adj) return norm_adj.tocoo() adj_mat_list = [] print("Begin to build sparse relation matrix ...") for r_id in tqdm(relation_dict.keys()): np_mat = np.array(relation_dict[r_id]) if r_id == 0: cf = np_mat.copy() cf[:, 1] = cf[:, 1] + n_users # [0, n_items) -&gt; [n_users, n_users+n_items) vals = [1.] * len(cf) adj = sp.coo_matrix((vals, (cf[:, 0], cf[:, 1])), shape=(n_nodes, n_nodes)) else: vals = [1.] * len(np_mat) adj = sp.coo_matrix((vals, (np_mat[:, 0], np_mat[:, 1])), shape=(n_nodes, n_nodes)) adj_mat_list.append(adj) norm_mat_list = [_bi_norm_lap(mat) for mat in adj_mat_list] mean_mat_list = [_si_norm_lap(mat) for mat in adj_mat_list] # interaction: user-&gt;item, [n_users, n_entities] norm_mat_list[0] = norm_mat_list[0].tocsr()[:n_users, n_users:].tocoo() mean_mat_list[0] = mean_mat_list[0].tocsr()[:n_users, n_users:].tocoo() return adj_mat_list, norm_mat_list, mean_mat_listdef load_data(model_args): global args args = model_args directory = args.data_path + args.dataset + '/' print('reading train and test user-item set ...') train_cf = read_cf(directory + 'train.txt') test_cf = read_cf(directory + 'test.txt') remap_item(train_cf, test_cf) print('combinating train_cf and kg data ...') triplets = read_triplets(directory + 'kg_final.txt') print('building the graph ...') graph, relation_dict = build_graph(train_cf, triplets) print('building the adj mat ...') adj_mat_list, norm_mat_list, mean_mat_list = build_sparse_relational_graph(relation_dict) n_params = &#123; 'n_users': int(n_users), 'n_items': int(n_items), 'n_entities': int(n_entities), 'n_nodes': int(n_nodes), 'n_relations': int(n_relations) &#125; user_dict = &#123; 'train_user_set': train_user_set, 'test_user_set': test_user_set &#125; return train_cf, test_cf, user_dict, n_params, graph, \ [adj_mat_list, norm_mat_list, mean_mat_list] evaluate.py模型测试代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180from .metrics import *from .parser import parse_argsimport torchimport numpy as npimport multiprocessingimport heapqfrom time import timecores = multiprocessing.cpu_count() // 2args = parse_args()Ks = eval(args.Ks)device = torch.device("cuda:" + str(args.gpu_id)) if args.cuda else torch.device("cpu")BATCH_SIZE = args.test_batch_sizebatch_test_flag = args.batch_test_flagdef ranklist_by_heapq(user_pos_test, test_items, rating, Ks): item_score = &#123;&#125; for i in test_items: item_score[i] = rating[i] K_max = max(Ks) K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get) r = [] for i in K_max_item_score: if i in user_pos_test: r.append(1) else: r.append(0) auc = 0. return r, aucdef get_auc(item_score, user_pos_test): item_score = sorted(item_score.items(), key=lambda kv: kv[1]) item_score.reverse() item_sort = [x[0] for x in item_score] posterior = [x[1] for x in item_score] r = [] for i in item_sort: if i in user_pos_test: r.append(1) else: r.append(0) auc = AUC(ground_truth=r, prediction=posterior) return aucdef ranklist_by_sorted(user_pos_test, test_items, rating, Ks): item_score = &#123;&#125; for i in test_items: item_score[i] = rating[i] K_max = max(Ks) K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get) r = [] for i in K_max_item_score: if i in user_pos_test: r.append(1) else: r.append(0) auc = get_auc(item_score, user_pos_test) return r, aucdef get_performance(user_pos_test, r, auc, Ks): precision, recall, ndcg, hit_ratio = [], [], [], [] for K in Ks: precision.append(precision_at_k(r, K)) recall.append(recall_at_k(r, K, len(user_pos_test))) ndcg.append(ndcg_at_k(r, K, user_pos_test)) hit_ratio.append(hit_at_k(r, K)) return &#123;'recall': np.array(recall), 'precision': np.array(precision), 'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc&#125;def test_one_user(x): # user u's ratings for user u rating = x[0] # uid u = x[1] # user u's items in the training set try: training_items = train_user_set[u] except Exception: training_items = [] # user u's items in the test set user_pos_test = test_user_set[u] all_items = set(range(0, n_items)) test_items = list(all_items - set(training_items)) if args.test_flag == 'part': r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks) else: r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks) return get_performance(user_pos_test, r, auc, Ks)def test(model, user_dict, n_params): result = &#123;'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)), 'hit_ratio': np.zeros(len(Ks)), 'auc': 0.&#125; global n_users, n_items n_items = n_params['n_items'] n_users = n_params['n_users'] global train_user_set, test_user_set train_user_set = user_dict['train_user_set'] test_user_set = user_dict['test_user_set'] pool = multiprocessing.Pool(cores) u_batch_size = BATCH_SIZE i_batch_size = BATCH_SIZE test_users = list(test_user_set.keys()) n_test_users = len(test_users) n_user_batchs = n_test_users // u_batch_size + 1 count = 0 entity_gcn_emb, user_gcn_emb = model.generate() for u_batch_id in range(n_user_batchs): start = u_batch_id * u_batch_size end = (u_batch_id + 1) * u_batch_size user_list_batch = test_users[start: end] user_batch = torch.LongTensor(np.array(user_list_batch)).to(device) u_g_embeddings = user_gcn_emb[user_batch] if batch_test_flag: # batch-item test n_item_batchs = n_items // i_batch_size + 1 rate_batch = np.zeros(shape=(len(user_batch), n_items)) i_count = 0 for i_batch_id in range(n_item_batchs): i_start = i_batch_id * i_batch_size i_end = min((i_batch_id + 1) * i_batch_size, n_items) item_batch = torch.LongTensor(np.array(range(i_start, i_end))).view(i_end-i_start).to(device) i_g_embddings = entity_gcn_emb[item_batch] i_rate_batch = model.rating(u_g_embeddings, i_g_embddings).detach().cpu() rate_batch[:, i_start: i_end] = i_rate_batch i_count += i_rate_batch.shape[1] assert i_count == n_items else: # all-item test item_batch = torch.LongTensor(np.array(range(0, n_items))).view(n_items, -1).to(device) i_g_embddings = entity_gcn_emb[item_batch] rate_batch = model.rating(u_g_embeddings, i_g_embddings).detach().cpu() user_batch_rating_uid = zip(rate_batch, user_list_batch) batch_result = pool.map(test_one_user, user_batch_rating_uid) count += len(batch_result) for re in batch_result: result['precision'] += re['precision']/n_test_users result['recall'] += re['recall']/n_test_users result['ndcg'] += re['ndcg']/n_test_users result['hit_ratio'] += re['hit_ratio']/n_test_users result['auc'] += re['auc']/n_test_users assert count == n_test_users pool.close() return result helper.py一些辅助接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253'''Created on Aug 19, 2016@author: Xiang Wang (xiangwang@u.nus.edu)'''__author__ = "xiangwang"import osimport redef txt2list(file_src): orig_file = open(file_src, "r") lines = orig_file.readlines() return linesdef ensureDir(dir_path): d = os.path.dirname(dir_path) if not os.path.exists(d): os.makedirs(d)def uni2str(unicode_str): return str(unicode_str.encode('ascii', 'ignore')).replace('\n', '').strip()def hasNumbers(inputString): return bool(re.search(r'\d', inputString))def delMultiChar(inputString, chars): for ch in chars: inputString = inputString.replace(ch, '') return inputStringdef merge_two_dicts(x, y): z = x.copy() # start with x's keys and values z.update(y) # modifies z with y's keys and values &amp; returns None return zdef early_stopping(log_value, best_value, stopping_step, expected_order='acc', flag_step=100): # early stopping strategy: assert expected_order in ['acc', 'dec'] if (expected_order == 'acc' and log_value &gt;= best_value) or (expected_order == 'dec' and log_value &lt;= best_value): stopping_step = 0 best_value = log_value else: stopping_step += 1 if stopping_step &gt;= flag_step: print("Early stopping is trigger at step: &#123;&#125; log:&#123;&#125;".format(flag_step, log_value)) should_stop = True else: should_stop = False return best_value, stopping_step, should_stop metrics.py实验用的评测指标，主要有： recall 召回率 precision_at_k TopK精确率 average_precision area under PR curve ndcg@k …… AUC 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import numpy as npfrom sklearn.metrics import roc_auc_scoredef recall(rank, ground_truth, N): return len(set(rank[:N]) &amp; set(ground_truth)) / float(len(set(ground_truth)))def precision_at_k(r, k): """Score is precision @ k Relevance is binary (nonzero is relevant). Returns: Precision @ k Raises: ValueError: len(r) must be &gt;= k """ assert k &gt;= 1 r = np.asarray(r)[:k] return np.mean(r)def average_precision(r,cut): """Score is average precision (area under PR curve) Relevance is binary (nonzero is relevant). Returns: Average precision """ r = np.asarray(r) out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]] if not out: return 0. return np.sum(out)/float(min(cut, np.sum(r)))def mean_average_precision(rs): """Score is mean average precision Relevance is binary (nonzero is relevant). Returns: Mean average precision """ return np.mean([average_precision(r) for r in rs])def dcg_at_k(r, k, method=1): """Score is discounted cumulative gain (dcg) Relevance is positive real values. Can use binary as the previous methods. Returns: Discounted cumulative gain """ r = np.asfarray(r)[:k] if r.size: if method == 0: return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1))) elif method == 1: return np.sum(r / np.log2(np.arange(2, r.size + 2))) else: raise ValueError('method must be 0 or 1.') return 0.def ndcg_at_k(r, k, ground_truth, method=1): """Score is normalized discounted cumulative gain (ndcg) Relevance is positive real values. Can use binary as the previous methods. Returns: Normalized discounted cumulative gain Low but correct defination """ GT = set(ground_truth) if len(GT) &gt; k : sent_list = [1.0] * k else: sent_list = [1.0]*len(GT) + [0.0]*(k-len(GT)) dcg_max = dcg_at_k(sent_list, k, method) if not dcg_max: return 0. return dcg_at_k(r, k, method) / dcg_maxdef recall_at_k(r, k, all_pos_num): r = np.asfarray(r)[:k] return np.sum(r) / all_pos_numdef hit_at_k(r, k): r = np.array(r)[:k] if np.sum(r) &gt; 0: return 1. else: return 0.def F1(pre, rec): if pre + rec &gt; 0: return (2.0 * pre * rec) / (pre + rec) else: return 0.def AUC(ground_truth, prediction): try: res = roc_auc_score(y_true=ground_truth, y_score=prediction) except Exception: res = 0. return res parser.py该文件比较简单，负责读取命令行输入的训练参数、默认训练参数设置以及参数注释。调用的核心接口是argparse.ArgumentParser.add_argument 12345678910111213141516171819202122232425262728293031323334353637383940414243import argparsedef parse_args(): parser = argparse.ArgumentParser(description="KGIN") # ===== dataset ===== # parser.add_argument("--dataset", nargs="?", default="last-fm", help="Choose a dataset:[last-fm,amazon-book,alibaba]") parser.add_argument( "--data_path", nargs="?", default="data/", help="Input data path." ) # ===== train ===== # parser.add_argument('--epoch', type=int, default=1000, help='number of epochs') parser.add_argument('--batch_size', type=int, default=1024, help='batch size') parser.add_argument('--test_batch_size', type=int, default=1024, help='batch size') parser.add_argument('--dim', type=int, default=64, help='embedding size') parser.add_argument('--l2', type=float, default=1e-5, help='l2 regularization weight') parser.add_argument('--lr', type=float, default=1e-4, help='learning rate') parser.add_argument('--sim_regularity', type=float, default=1e-4, help='regularization weight for latent factor') parser.add_argument("--inverse_r", type=bool, default=True, help="consider inverse relation or not") parser.add_argument("--node_dropout", type=bool, default=True, help="consider node dropout or not") parser.add_argument("--node_dropout_rate", type=float, default=0.5, help="ratio of node dropout") parser.add_argument("--mess_dropout", type=bool, default=True, help="consider message dropout or not") parser.add_argument("--mess_dropout_rate", type=float, default=0.1, help="ratio of node dropout") parser.add_argument("--batch_test_flag", type=bool, default=True, help="use gpu or not") parser.add_argument("--channel", type=int, default=64, help="hidden channels for model") parser.add_argument("--cuda", type=bool, default=True, help="use gpu or not") parser.add_argument("--gpu_id", type=int, default=0, help="gpu id") parser.add_argument('--Ks', nargs='?', default='[20, 40, 60, 80, 100]', help='Output sizes of every layer') parser.add_argument('--test_flag', nargs='?', default='part', help='Specify the test type from &#123;part, full&#125;, indicating whether the reference is done in mini-batch') parser.add_argument("--n_factors", type=int, default= 4, help="number of latent factor for user favour") parser.add_argument("--ind", type=str, default='distance', help="Independence modeling: mi, distance, cosine") # ===== relation context ===== # parser.add_argument('--context_hops', type=int, default=3, help='number of context hops') # ===== save model ===== # parser.add_argument("--save", type=bool, default=False, help="save model or not") parser.add_argument("--out_dir", type=str, default="./weights/", help="output directory for model") return parser.parse_args() KGIN.pyRelational Path-aware Aggregation123456789101112131415161718192021222324252627282930313233'''Created on July 1, 2020PyTorch Implementation of KGIN@author: Tinglin Huang (tinglin.huang@zju.edu.cn)'''__author__ = "huangtinglin"import randomimport numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch_scatter import scatter_meanclass Aggregator(nn.Module): """ Relational Path-aware Convolution Network """ def __init__(self, n_users, n_factors): super(Aggregator, self).__init__() self.n_users = n_users self.n_factors = n_factors def forward(self, entity_emb, user_emb, latent_emb, edge_index, edge_type, interact_mat, weight, disen_weight_att): n_entities = entity_emb.shape[0] channel = entity_emb.shape[1]#embedding size,dim n_users = self.n_users n_factors = self.n_factors# 用户偏好的隐因子 默认值为4 3.2.2 Aggregation Layer over Knowledge Graph12345678910"""KG aggregate"""head, tail = edge_indexedge_relation_emb = weight[edge_type - 1] # exclude interact, remap [1, n_relations) to [0, n_relations-1)#weight形状是[n_relations - 1, in_channel=arg.dim]，表示各个关系的嵌入表示#edge_relation_emb 通过 edge_type - 1 取出 edge_type对应relation的嵌入表示# tail=edge_index[1],shape=[tripletsNum,]# entity_emb[tail].shape=[tripletsNum,embSize]# edge_relation_emb.shape=[tripletsNum,embSize]neigh_relation_emb = entity_emb[tail] * edge_relation_emb # [-1, channel] 对于KG中的实体结点$i$，嵌入表示为$e_{i}$，由其邻居结点聚合得到，公式如(1)所示 e_{i}^{(1)}=\frac{1}{|N_{i}|}\sum_{(r,v)\in N_{i}}e_{r}\odot e_{v}^{(0)}其中\odot是element-wise product，即逐元素乘法 给出该公式的示意图辅助理解： 1entity_agg = scatter_mean(src=neigh_relation_emb, index=head, dim_size=n_entities, dim=0) scatter_mean操作示意图如下： 具体到此处，scatter_mean的作用是将形状为[tripletsNum,embSize]的neigh_relation_emb的行向量，根据传入的head，分发到对应head的位置上求均值。可以理解为neigh_relation_emb是$e_{r}\odot e_{v}^{(0)}$的结算结果，而求均值由scatter_mean完成。示意图如下： 3.2.1 Aggregation Layer over Intent Graph.注意力机制，计算隐因子（用户意图向量）和用户嵌入表示的注意力分数 \beta(u,p)=\frac{exp(e_{p}^{T}e_{u}^{(0)})}{\sum_{p^{\prime}\in P}exp(e_{p^{\prime}}^{T}e_{u}^{(0)})}123"""cul user-&gt;latent factor attention"""score_ = torch.mm(user_emb, latent_emb.t())score = nn.Softmax(dim=1)(score_).unsqueeze(-1) # [n_users, n_factors, 1] 用户向量计算： 123"""user aggregate"""user_agg = torch.sparse.mm(interact_mat, entity_emb) # [n_users, channel]#[n_users,n_entity] \cdot [n_entity,channel=embd_size]=&gt;[n_users, channel] 交互矩阵interact_mat是拉普拉斯矩阵，则此处计算的是 user\_agg[u_{i}]=\sum_{i \in interaction(u,i)}{L(u,i)*e_{i}}\\ \large e_{u_{}}^{(1)}=\sum_{i \in interaction(u)}{L(u,i)*e_{i}^{(0)}}\\ usrEmbMat=LaplacianMat \cdot itemEmbedMat相当于用用户的行为历史（交互过的物品/实体）向量做一个聚合表示。 disen_weight_att被定义在GraphConv的__init__方法中，disen_weight_att = initializer(torch.empty(n_factors, n_relations - 1)),表示隐因子（用户意图）与KG中各个relation的关系（从shape和注释来看不包含user-item-interaction） 12disen_weight = torch.mm(nn.Softmax(dim=-1)(disen_weight_att),weight) .expand(n_users, n_factors, channel) nn.Softmax(dim=-1)(disen_weight_att)计算用户意图$p_{i}$与各个relation的分数，shape为[n_factors, n_relations - 1] weight是relation的嵌入矩阵，形状为[n_relations - 1, in_channel=arg.dim] torch.mm()计算后形状为[n_factors, channel=arg.dim] expand拷贝[n_factors, channel=arg.dim]向量广播成[n_users, n_factors, channel]形状。这个操作可行的原因是论文中提到过意图向量集合P被所有用户共享，视为所有用户都有的意图。但是每个意图与用户的契合度/信息量是通过注意力机制控制的。 上述计算过程等价于使用KG中的relation聚合表示意图向量 \large e_{p_{i}}=\sum att\_score_{p_{i},r_{j}}*e_{r_{j}}123user_agg = user_agg * (disen_weight * score).sum(dim=1) + user_agg # [n_users, channel]return entity_agg, user_agg user_agg = user_agg * (disen_weight * score).sum(dim=1) + user_agg (disen_weight * score)：[n_users, n_factors, channel] * [n_users, n_factors, 1]=&gt;[n_users, n_factors, channel] 求解的是$\beta(u,p)e_{p}$，每个用户的每个意图向量乘上对应的注意力分数$\beta(u,p)$ (disen_weight * score).sum(dim=1)：$\sum_{p}\beta(u,p)e_{p}$ user_agg * (disen_weight * score).sum(dim=1)： \sum_{i \in interaction(u,i)}{L(u,i)*e_{i}}*\sum_{p}\beta(u,p)e_{p}对应于原论文中的公式(7)的一部分 e_{u}^{(1)}=\frac{1}{|N_{u}|}\sum_{(p,i)\in N_{u}}\beta(u,p)e_{p}\odot e_{i}^{(0)}图卷积网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class GraphConv(nn.Module): """ Graph Convolutional Network """ def __init__(self, channel, n_hops, n_users, n_factors, n_relations, interact_mat, ind, node_dropout_rate=0.5, mess_dropout_rate=0.1): super(GraphConv, self).__init__() self.convs = nn.ModuleList() self.interact_mat = interact_mat self.n_relations = n_relations self.n_users = n_users self.n_factors = n_factors self.node_dropout_rate = node_dropout_rate self.mess_dropout_rate = mess_dropout_rate self.ind = ind# 衡量向量信息量相同程度的策略：互信息，距离，相关性 self.temperature = 0.2 initializer = nn.init.xavier_uniform_ weight = initializer(torch.empty(n_relations - 1, channel)) # not include interact #channel：hidden channels for model，channel的含义需要回顾论文,但其实际值为输入参数中的embedding size 即 dim参数 self.weight = nn.Parameter(weight) # [n_relations - 1, in_channel] # weight矩阵是 KG中的relation的嵌入表示 disen_weight_att = initializer(torch.empty(n_factors, n_relations - 1)) self.disen_weight_att = nn.Parameter(disen_weight_att) #disen_weight_att [n_factors, n_relations - 1] for i in range(n_hops): self.convs.append(Aggregator(n_users=n_users, n_factors=n_factors)) self.dropout = nn.Dropout(p=mess_dropout_rate) # mess dropout def _edge_sampling(self, edge_index, edge_type, rate=0.5): # edge_index: [2, -1] # edge_type: [-1] n_edges = edge_index.shape[1] random_indices = np.random.choice(n_edges, size=int(n_edges * rate), replace=False) return edge_index[:, random_indices], edge_type[random_indices] def _sparse_dropout(self, x, rate=0.5): noise_shape = x._nnz() random_tensor = rate random_tensor += torch.rand(noise_shape).to(x.device) dropout_mask = torch.floor(random_tensor).type(torch.bool) i = x._indices() v = x._values() i = i[:, dropout_mask] v = v[dropout_mask] out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device) return out * (1. / (1 - rate)) # def _cul_cor_pro(self): # # disen_T: [num_factor, dimension] # disen_T = self.disen_weight_att.t() # # # normalized_disen_T: [num_factor, dimension] # normalized_disen_T = disen_T / disen_T.norm(dim=1, keepdim=True) # # pos_scores = torch.sum(normalized_disen_T * normalized_disen_T, dim=1) # ttl_scores = torch.sum(torch.mm(disen_T, self.disen_weight_att), dim=1) # # pos_scores = torch.exp(pos_scores / self.temperature) # ttl_scores = torch.exp(ttl_scores / self.temperature) # # mi_score = - torch.sum(torch.log(pos_scores / ttl_scores)) # return mi_score 衡量向量间信息量相同程度123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def _cul_cor(self): def CosineSimilarity(tensor_1, tensor_2): # tensor_1, tensor_2: [channel] normalized_tensor_1 = tensor_1 / tensor_1.norm(dim=0, keepdim=True) normalized_tensor_2 = tensor_2 / tensor_2.norm(dim=0, keepdim=True) return (normalized_tensor_1 * normalized_tensor_2).sum(dim=0) ** 2 # no negative def DistanceCorrelation(tensor_1, tensor_2): # tensor_1, tensor_2: [channel] # ref: https://en.wikipedia.org/wiki/Distance_correlation channel = tensor_1.shape[0] zeros = torch.zeros(channel, channel).to(tensor_1.device) zero = torch.zeros(1).to(tensor_1.device) tensor_1, tensor_2 = tensor_1.unsqueeze(-1), tensor_2.unsqueeze(-1) """cul distance matrix""" a_, b_ = torch.matmul(tensor_1, tensor_1.t()) * 2, \ torch.matmul(tensor_2, tensor_2.t()) * 2 # [channel, channel] tensor_1_square, tensor_2_square = tensor_1 ** 2, tensor_2 ** 2 a, b = torch.sqrt(torch.max(tensor_1_square - a_ + tensor_1_square.t(), zeros) + 1e-8), \ torch.sqrt(torch.max(tensor_2_square - b_ + tensor_2_square.t(), zeros) + 1e-8) # [channel, channel] """cul distance correlation""" A = a - a.mean(dim=0, keepdim=True) - a.mean(dim=1, keepdim=True) + a.mean() B = b - b.mean(dim=0, keepdim=True) - b.mean(dim=1, keepdim=True) + b.mean() dcov_AB = torch.sqrt(torch.max((A * B).sum() / channel ** 2, zero) + 1e-8) dcov_AA = torch.sqrt(torch.max((A * A).sum() / channel ** 2, zero) + 1e-8) dcov_BB = torch.sqrt(torch.max((B * B).sum() / channel ** 2, zero) + 1e-8) return dcov_AB / torch.sqrt(dcov_AA * dcov_BB + 1e-8) def MutualInformation(): # disen_T: [num_factor, dimension] disen_T = self.disen_weight_att.t() # normalized_disen_T: [num_factor, dimension] normalized_disen_T = disen_T / disen_T.norm(dim=1, keepdim=True) pos_scores = torch.sum(normalized_disen_T * normalized_disen_T, dim=1) ttl_scores = torch.sum(torch.mm(disen_T, self.disen_weight_att), dim=1) pos_scores = torch.exp(pos_scores / self.temperature) ttl_scores = torch.exp(ttl_scores / self.temperature) mi_score = - torch.sum(torch.log(pos_scores / ttl_scores)) return mi_score """cul similarity for each latent factor weight pairs""" if self.ind == 'mi': return MutualInformation() else: cor = 0 for i in range(self.n_factors): for j in range(i + 1, self.n_factors): if self.ind == 'distance': cor += DistanceCorrelation(self.disen_weight_att[i], self.disen_weight_att[j]) else: cor += CosineSimilarity(self.disen_weight_att[i], self.disen_weight_att[j]) return cor 图卷积网络前向计算1234567891011def forward(self, user_emb, entity_emb, latent_emb, edge_index, edge_type, interact_mat, mess_dropout=True, node_dropout=False): """node dropout""" if node_dropout: edge_index, edge_type = self._edge_sampling(edge_index, edge_type, self.node_dropout_rate) interact_mat = self._sparse_dropout(interact_mat, self.node_dropout_rate) entity_res_emb = entity_emb # [n_entity, channel] user_res_emb = user_emb # [n_users, channel] cor = self._cul_cor() Capturing Relational Paths两公式等价证明 \Large e_{i}^{(l)}=\frac{1}{|N_{i}|}\sum_{(r,v)\in N_{i}}{e_{r} \odot e_{v}^{(l-1)}}\\ \Large=\sum_{(r,v)\in N_{i}}{\frac{e_{r}}{|N_{i}|} \odot e_{v}^{(l-1)}}为了方便，改变一些符号的记法 \Large e_{s_{0}}^{(l)}=\sum_{(r_{1},s_{1})\in N_{s_{0}}}{\frac{e_{r_{1}}}{|N_{s_{0}}|} \odot e_{s_{1}}^{(l-1)}}\\ \Large e_{s_{1}}^{(l-1)}=\sum_{(r_{2},s_{2})\in N_{s_{1}}}{\frac{e_{r_{2}}}{|N_{s_{1}}|} \odot e_{s_{2}}^{(l-2)}}\\ ...\\ ...\\ ...\\ \Large e_{s_{l-1}}^{(1)}=\sum_{(r_{l},s_{l})\in N_{s_{l-1}}}{\frac{e_{r_{l}}}{|N_{s_{l-1}}|} \odot e_{s_{l}}^{(0)}}\\综合可得： \Large e_{s_{0}}^{(l)}=\sum_{(r_{1},s_{1})\in N_{s_{0}}}{\frac{e_{r_{1}}}{|N_{s_{0}}|} \odot (\sum_{(r_{2},s_{2})\in N_{s_{1}}}{\frac{e_{r_{2}}}{|N_{s_{1}}|} \odot e_{s_{2}}^{(l-2)}})}\\ \Large e_{s_{0}}^{(l)}=\sum_{(r_{1},s_{1})\in N_{s_{0}}}{\frac{e_{r_{1}}}{|N_{s_{0}}|} \odot (\sum_{(r_{2},s_{2})\in N_{s_{1}}}{\frac{e_{r_{2}}}{|N_{s_{1}}|} \odot (...(\sum_{(r_{l},s_{l})\in N_{s_{l-1}}}{\frac{e_{r_{l}}}{|N_{s_{l-1}}|} \odot e_{s_{l}}^{(0)}})...)})}\\ \Large e_{s_{0}}^{(l)}=\sum_{(r_{1},s_{1})\in N_{s_{0}}}{\sum_{(r_{2},s_{2})\in N_{s_{1}}}...\sum_{(r_{l},s_{l})\in N_{s_{l-1}}}\frac{e_{r_{1}}}{|N_{s_{0}}|} \odot {\frac{e_{r_{2}}}{|N_{s_{1}}|} \odot ... \odot \frac{e_{r_{l}}}{|N_{s_{l-1}}|} \odot e_{s_{l}}^{(0)}}}\\1234567891011121314#KG上基于结点的多跳传播for i in range(len(self.convs)): #entity_emb，user_emb更新，具体更新过程公式在论文中的Aggregation Layer over Intent Graph和Aggregation Layer over Knowledge Graph， Capturing Relational Paths.即上面锁分析的Aggregation层的前向计算过程 entity_emb, user_emb = self.convs[i](entity_emb, user_emb, latent_emb, edge_index, edge_type, interact_mat, self.weight, self.disen_weight_att) """message dropout""" if mess_dropout: entity_emb = self.dropout(entity_emb) user_emb = self.dropout(user_emb) entity_emb = F.normalize(entity_emb) user_emb = F.normalize(user_emb) 123"""result emb"""entity_res_emb = torch.add(entity_res_emb, entity_emb)user_res_emb = torch.add(user_res_emb, user_emb) 这里对应论文中的公式13 e_{u}^{*}=e_{u}^{(0)}+...+e_{u}^{(L)}\\ e_{i}^{*}=e_{i}^{(0)}+...+e_{i}^{(L)}\\1return entity_res_emb, user_res_emb, cor 推荐模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class Recommender(nn.Module): def __init__(self, data_config, args_config, graph, adj_mat): super(Recommender, self).__init__() self.n_users = data_config['n_users']#用户数 self.n_items = data_config['n_items']#物品数 self.n_relations = data_config['n_relations']#关系总数=usritm_interaction+KG rel self.n_entities = data_config['n_entities'] # include items self.n_nodes = data_config['n_nodes'] # n_users + n_entities self.decay = args_config.l2#L2 正则权重 self.sim_decay = args_config.sim_regularity# 隐因子的正则化权重 self.emb_size = args_config.dim#嵌入向量维度 self.context_hops = args_config.context_hops#关系上下文的最大跳数 self.n_factors = args_config.n_factors#用户偏好的隐因子数目 self.node_dropout = args_config.node_dropout#bool：结点裁剪控制 self.node_dropout_rate = args_config.node_dropout_rate#结点裁剪率 self.mess_dropout = args_config.mess_dropout#消息裁剪控制 self.mess_dropout_rate = args_config.mess_dropout_rate#消息裁剪率 self.ind = args_config.ind#独立模型选择，论文中提到过三种区分意图向量包含信息的算法：向量距离，余弦相似度，互信息 self.device = torch.device("cuda:" + str(args_config.gpu_id)) if args_config.cuda \ else torch.device("cpu") self.adj_mat = adj_mat# 传入的是 mean_adi_list[0]，均值化后的拉普拉斯矩阵，且矩阵裁剪后的形状为[userNum,itemNum]， self.graph = graph# 代表知识图谱，是由kg_final的三元组中提取的&lt;头结点，尾结点&gt;组成的有向图 self.edge_index, self.edge_type = self._get_edges(graph)#[-1,2]，[-1,1] self._init_weight() self.all_embed = nn.Parameter(self.all_embed) self.latent_emb = nn.Parameter(self.latent_emb) self.gcn = self._init_model() def _init_weight(self): initializer = nn.init.xavier_uniform_ self.all_embed = initializer(torch.empty(self.n_nodes, self.emb_size)) #IG 和 KG 结点的嵌入向量初始化 self.latent_emb = initializer(torch.empty(self.n_factors, self.emb_size)) #用户偏好的嵌入表示 # [n_users, n_entities]？ 这里的形状应该是[n_users,n_items] self.interact_mat = self._convert_sp_mat_to_sp_tensor(self.adj_mat).to(self.device) def _init_model(self): return GraphConv(channel=self.emb_size, n_hops=self.context_hops, n_users=self.n_users, n_relations=self.n_relations, n_factors=self.n_factors, interact_mat=self.interact_mat, ind=self.ind, node_dropout_rate=self.node_dropout_rate, mess_dropout_rate=self.mess_dropout_rate) def _convert_sp_mat_to_sp_tensor(self, X): coo = X.tocoo() i = torch.LongTensor([coo.row, coo.col]) v = torch.from_numpy(coo.data).float() return torch.sparse.FloatTensor(i, v, coo.shape) def _get_indices(self, X): coo = X.tocoo() return torch.LongTensor([coo.row, coo.col]).t() # [-1, 2] def _get_edges(self, graph): graph_tensor = torch.tensor(list(graph.edges)) # [-1, 3] index = graph_tensor[:, :-1] # [-1, 2] type = graph_tensor[:, -1] # [-1, 1] #.t() 转置Tensor long()将张量中各元素转为torch.int64类型数据 .to指定运算张量的设备 return index.t().long().to(self.device), type.long().to(self.device) 对_get_edges中的一些变量打印查看， 123456789print(list(graph.edges)[0:5])print(graph_tensor.shape)print(index.shape)print(graph_tensor[:, -1].shape)==&gt;[(0, 48123, 1), (0, 73455, 2), (0, 80230, 2), (0, 91668, 2), (0, 91810, 2)]torch.Size([929134, 3])torch.Size([929134, 2])torch.Size([929134]) 根据之前的分析，结合这里的打印信息可以知道，graph_tensor存储的数据格式为[entityId，entityId，relationId]。为了进一步验证分析，查看Networkx文档，搜索networkx.MultiGraph.edges可知graph.edges返回[u, v, k]格式的数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def forward(self, batch=None): user = batch['users'] pos_item = batch['pos_items'] neg_item = batch['neg_items'] user_emb = self.all_embed[:self.n_users, :] item_emb = self.all_embed[self.n_users:, :] #channel 的 值等于emb_size等于输入的dim , 即'embedding size' # entity_gcn_emb: [n_entity, channel] # user_gcn_emb: [n_users, channel] entity_gcn_emb, user_gcn_emb, cor = self.gcn(user_emb, item_emb, self.latent_emb, self.edge_index, self.edge_type, self.interact_mat, mess_dropout=self.mess_dropout, node_dropout=self.node_dropout) u_e = user_gcn_emb[user] pos_e, neg_e = entity_gcn_emb[pos_item], entity_gcn_emb[neg_item] return self.create_bpr_loss(u_e, pos_e, neg_e, cor)def generate(self): user_emb = self.all_embed[:self.n_users, :] item_emb = self.all_embed[self.n_users:, :] return self.gcn(user_emb, item_emb, self.latent_emb, self.edge_index, self.edge_type, self.interact_mat, mess_dropout=False, node_dropout=False)[:-1]def rating(self, u_g_embeddings, i_g_embeddings): return torch.matmul(u_g_embeddings, i_g_embeddings.t())def create_bpr_loss(self, users, pos_items, neg_items, cor): batch_size = users.shape[0] pos_scores = torch.sum(torch.mul(users, pos_items), axis=1) neg_scores = torch.sum(torch.mul(users, neg_items), axis=1) mf_loss = -1 * torch.mean(nn.LogSigmoid()(pos_scores - neg_scores)) # cul regularizer regularizer = (torch.norm(users) ** 2 + torch.norm(pos_items) ** 2 + torch.norm(neg_items) ** 2) / 2 emb_loss = self.decay * regularizer / batch_size cor_loss = self.sim_decay * cor return mf_loss + emb_loss + cor_loss, mf_loss, emb_loss, cor]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RecSearchAuditionReady]]></title>
    <url>%2F2021%2F09%2F06%2FRecSearchAuditionReady%2F</url>
    <content type="text"><![CDATA[推荐系统方向面试准备 参考路线 2.1推荐系统相关面试题2.1.1 树算法 决策树和CART ID3, 信息增益 C4.5, 信息增益率 CART GBDT XGboost 2.1.2 协同过滤：ItemCF与UserCF 基于用户的协同过滤算法(userCF)： 思想：目标用户与其相似用户的兴趣类似 步骤一：找到和目标用户相似的Top-K用户集合 $N(u)$、$N(v)$分别为用户u、v有过正反馈的物品集合 Jaccard公式：计算两个集合A和B的交集元素在A、B的并集中所占的比例 sim_{u,v}=\frac{|N(u)\cap N(v)|}{|N(u)\cup N(v)|} 余弦相似度： 用物品向量（有过正反馈的物品的向量分量置1，否则为0）度量 则u、v两用户的相似度为 sim_{u,v}=\frac{|N(u)\cap N(v)|}{\sqrt{|N(u)|| N(v)|}}等价于原始的向量余弦相似度定义 sim_{u,v}=\frac{\vec A\cdot \vec B}{|\vec A||\vec B|}用户的物品向量为$\vec A=(x_{0},x_{1}…x_{n})$其中$x_{i}$可取0、1 热门物品惩罚： 上述公式的分子粗糙的对两用户的交互物品集合求交集，没有区分物品的热门程度。但实际上，如果物品足够热门，那么它会出现在许多用户交互物品交集中，这样的物品并不能为评估用户相似度提供很多的信息。换言之，两用户对同一冷门物品产生交互，更能说明二者相似。 sim_{u,v}=\frac{\sum_{i\in N(u)\cap N(v)}{\frac{1}{log(1+N(i))}}}{\sqrt{|N(u)\cup N(v)|}}- 步骤二：将该集合中用户喜欢的物品，推荐给目标用户 计算目标用户对待推荐物品的兴趣 $p_{u,i}$ 公式： $$ p_{u,i}=\sum_{v\in {TopkSimUser\cap N(i)}}{sim_{u,v}* rate_{v,i}} $$ ​ 其中,N(i)是与物品$i$交互过的用户，TopkSimUser是与用户$u$最相似的K个用户 基于物品(标的物)的协同过滤算法(itemCF)： 步骤一：计算物品之间相似度 步骤二：分析目标用户的历史行为，为其推荐与喜欢物品相似的其他物品 思想：目标用户曾经对物品A感兴趣，而物品B与A相似，则认为用户也会对B感兴趣 常用的几种相似度计算公式 $N(i)$、$N(j)$分别为喜欢物品i、j的用户集合 公式一 sim_{i,j}=\frac{|N(i) \cap N(j)|}{|N(i)|}直观理解，该相似度表达的含义是喜欢物品i的用户中，有多大比例喜欢物品 j 。该相似度指标的最大的一个问题是热门物品会跟任何一个物品都很相似。（喜欢物品i的用户大都喜欢（有过正反馈）热门物品 j ） 公式二，余弦相似度（惩罚热门物品） 对于物品，使用用户向量度量相似度（有过正反馈的用户的向量分量置1，否则为0） 物品i、j的相似度为 sim_{i,j}=\frac{|N(i) \cap N(j)|}{\sqrt{|N(i)||N(j)|}}以上是通过集合运算得出的余弦相似度结果，等价于两用户向量间的余弦相似度计算。虽然该公式对热门物品做出了一定的惩罚，但当物品j非常热门时，物品i与热门物品j还是会获得较大的相似度。 公式三 sim_{i,j}=\frac{|N(i) \cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}修改分母的指数项大小，进一步惩罚热门物品 itemCF与userCF比较： UserCF利用“小圈子”群体进行推荐，偏社会化；而ItemCF考虑用户个人的历史兴趣，更加个性化。 2.1.3 矩阵分解 SVD Funk-SVD（latent factor model） R\approx U\cdot I = \hat R用户潜在特征矩阵：$U\in R^{userNum\times k}$ 物品潜在特征矩阵：$I\in R^{k \times itemNum}$ 评分预测：$\hat R = U \cdot I$ 训练过程中优化目标: min\sum_{(u,i)\in ratePairSet}{(r_{u,i}-\hat r_{u,i})^{2}} 其中$ratePairSet$为评分矩阵$R$中已有评分记录的对 Regularized Matrix Factorization 加入$L_{2}$正则项的Funk-SVD 优化目标： min_{p^{*},q^{*}}\sum_{(u,i)\in ratePairSet}{(r_{u,i}-\hat r_{u,i})^{2}}+L(p,q)其中$p$为用户的潜在特征向量，$q$为物品的潜在特征向量 2.1.4 FM、wide&amp;deep、DeepFM FM wide&amp;deep DeepFM 2.1.5 基于内容2.1.6 基于关联规则2.1.7 关键词怎么提取的？TF-IDF有改进吗？怎么改进？与TextRank区别？ 关键词提取 TF-IDF TF指terms frequence，为单词$w_{i}$在文档中$dcmt_{j}$的出现频率。公式： TF_{w_{i},dcmt_{j}}=\frac{n_{w_{i},dcmt_{j}}}{\sum_{k} n_{w_{k},dcmt_{j}}} 其中$n_{w_{i},dcmt_{j}}$为单词$w_{i}$在文档中$dcmt_{j}$的出现次数。 IDF指inverse document frequence，计算公式如下： IDF_{w_{i}}=log(\frac{dn}{dn_{i}}) 其中$dn$为文档总数，$dn_{i}$为包含单词$w_{i}$的文档数。 总的分数为二者乘积，表示单词$w_{i}$对于某文档的重要性。直观上理解，$TF_{w_{i},dcmt_{j}}$表示单词$w_{i}$在某一文档中的出现频率，频率越高越重要。但同时还要从全局角度考虑IDF_{w_{i}}，该值越小，说明单词$w_{i}$在该文档中越具有代表性，因为在该文档中出现的同时在其他文档中并不常见。 TF-IDF改进 传统TF-IDF缺点： 基于词袋模型，没有考虑位置信息。如文档的开始和结尾位置的词通常更加重要 偏僻词容易被赋予更大的IDF分数 人名、地名等实体词在文档中的出现次数不多，导致TF分数较低。 IDF分数计算受到文档集合内部类别分布的影响较大。类别占比大的文档中的词汇相对于类别占比小的文档中的词汇IDF分数更低，但也许实际上词汇的重要程度相差不大。 改进 增加位置权重 设置阈值，特别大的IDF分数的词汇认定为错误词。 引入实体识别的模块，给实体词增加权重 TextRank 公式： WS(V_{i})=(1-d)+d* \sum_{j\in In(V_{i})}\frac{w_{ji}}{\sum _{V_{k}\in Out(V_{j})}w_{jk}}WS(V_{j}) 2.1.9 推荐系统架构2.1.10 word2vec 算法描述 负采样 层次softmax 参数设置 滑动窗口设置 负采样个数及比例 Embedding效果评价 训练加速 单词按照出现频率概率丢失，高频单词被采样概率小 负采样 层次softmax 2.1.11 冷启动策略主要有用户冷启动、物品冷启动与系统冷启动三种 用户冷启动 利用用户的社交属性进行粗粒度推荐，如性别、地区、年龄 热门/热度推荐 用户注册时使用问卷调查等形式，让用户选择感兴趣的内容、标签、分类等，使用调查结果推荐对应的物品 从其他社交平台导入用户信息，如微信、QQ账号登入 物品冷启动 基于物品内容相似度的推荐 系统冷启动 专家知识 2.1.12 Embedding deepFM、FM、LFM、SVD word2vec item2vec 假设一个长度为$k$的用户历史记录（交互物品列表）为$w_{1},w_{2}…w_{k}$，则优化目标为 L=\frac{1}{k}\sum_{i}^{k}\sum_{j\neq i}^{k} log \ p(w_{j}|w_{i}) deepwalk node2vec 嵌入效果的评价：可以将嵌入向量降维，计算类内点距和类外点距，并可视化给出直观结果。 2.2 机器学习2.2.1 线性回归2.2.2 逻辑回归2.2.3 决策树 决策树 随机森林 介绍：基础单元是决策树，通过训练多棵决策树得到多个弱分类器，在分类时综合所有分类器的结果得到最终结果，从而得到一个强分类器。 训练方式：如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集。单棵决策树在生长时，选取m个特征作为分裂特征，其中m&lt;&lt;M，M为原始特征维数。每一棵树都尽可能生长，不剪枝。 错误率：一方面取决于森林中任意两棵树的相似性，相似性越大随机森林的错误率越高；另一方面取决于每棵树的分类能力，森林中的各决策树分类能力越强，总体的错误率就越低。此外，选取特征数m会影响决策树的相关性和分类能力，m越大，相关性与分类能力都会增大。 问题： 为什么要随机有放回抽样 不放回抽样会使得各决策树的训练集彼此没有交集，；如果不抽样，所有基础分类器都是用同一训练集，相似性大差异性小，投票结果差，森林整体分类能力弱。随机有放回抽样可以产生袋外样本，用来做袋外估计(out of bag error rate)。对于N个不重复样本，随机有放回抽样k次，采样结果为m种不重复样本，则概率为 $$ P(kind=m)= $$ - GBDT XGboost LightGBM 2.2.4 聚类 K-均值聚类(K-Means) 设定K值，随机选取K个样本点为初始中心点 计算所有样本点与各中心点的距离，将样本点归类到距离最近的中心点同一类别，得到K个簇。 分类完所有样本点后，计算每个簇的新中心点，返回第2步。 均值漂移聚类(Mean-Shift) 设定K值，随机选取K个样本点为初始中心点 计算各个中心点的偏移向量，中心点沿偏移向量向更密集的地方移动 重复第2步，直到偏移向量大小满足设定的阈值条件，算法终止。 基于密度的聚类方法(DBSCAN) 高斯混合模型聚类算法(GMM) 图团体检测(Graph Community Detection) 2.2.5 正则化正则化理解 带约束的最优化 2.2.6 损失函数 交叉熵 二分类 L=\frac{1}{N}\sum_{i}^{N}{y_{i}log(p_{i})+(1-y_{i})\cdot log(1-p_{i})}多分类是二分类的扩展，其实是统一的 L=\frac{1}{N}\sum_{i}^{N}\sum_{c}^{M} y_{ic}\cdot log(p_{ic})多分类公式在M=2时退化为二分类公式： L=\frac{1}{N}\sum_{i}^{N}\sum_{c}^{2} y_{ic}\cdot log(p_{ic})\\=\frac{1}{N}\sum_{i}^{N}(y_{i,c=1}\cdot log(p_{i,c=1})+y_{i,c=0}\cdot log(p_{i,c=0}))\\=\frac{1}{N}\sum_{i}^{N}(y_{i,c=1}\cdot log(p_{i,c=1})+(1-y_{i,c=1})\cdot log(1-p_{i,c=1})) 欧式距离 均方误差 MSE=\frac{1}{N}\cdot \sum_{i}^{N}{(\hat y_{i}-y_{i})^{2}}]]></content>
      <tags>
        <tag>jobHunting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LOFTER_Reptile]]></title>
    <url>%2F2021%2F05%2F29%2FLOFTER-Reptile%2F</url>
    <content type="text"><![CDATA[LOFTER是网易公司于2012年3月推出的一款轻博客产品。LOFTER专注于为用户提供简约、易用、有品质、重原创的博客工具、原创社区，以及有品质的手机博客应用。 其上发布的贴子组成有图片、描述、标签、用户评论，可以通过爬虫算法以收集数据集，用于机器学习。（此文的目的是构建 SIT 项目的原始数据集，以进行系统冷启动和功能测试） 目标数据集格式： &lt;图片，标签列表&gt; &lt;评论用户，图片，时间，类型&gt; 参考的前辈经验： https://www.bilibili.com/video/BV1x54y1W7nQ 阿萨德 基本爬虫知识储备Lofter 请求报文中的参数分析GET请求POST请求xhr中的PostBean.getPostHots.dwr文件的请求负载部分该dwr文件携带的信息是某一帖子的热度列表，具体的就是其他用户对该帖子的反馈行为（喜欢、收藏、转发） 12345678910111213141516callCount=1scriptSessionId=$&#123;scriptSessionId&#125;187httpSessionId=c0-scriptName=TagBeanc0-methodName=searchc0-id=0c0-param0=string:%E8%A1%A8%E6%83%85%E5%8C%85c0-param1=number:0c0-param2=string:c0-param3=string:newc0-param4=boolean:falsec0-param5=number:1282300337c0-param6=number:20c0-param7=number:80c0-param8=number:1622120720606batchId=773988 c0-param0=string:%E8%A1%A8%E6%83%85%E5%8C%85 1234567//在线网站 url编码解码http://tool.chinaz.com/tools/urlencode.aspx//url解码%E8%A1%A8%E6%83%85%E5%8C%85--&gt;表情包 c0-param1=number:0 分析dwr的response数据可知，改参数为帖子的ID c0-param2=string: c0-param3=string:new c0-param4=boolean:false c0-param5=number:1282300337 c0-param6=number:20 观察相邻两个dwr文件可以推知，c0-param6与dwr文件的请求大小有关（具体的可以通过F12的Preview预览请求报文，可以看出dwr文件大概为 c0-param6*100行 左右）。 c0-param7=number:80 第 i 个dwr文件的c0-param7为第 i-1 个dwr文件的c0-param6与c0-param7之和，意义上应该等价于一个大文件分片传输时指示上一次结束的位置。 c0-param8=number:1622120720606 时间戳 1234//时间戳扎转换网站https://tool.lu/timestamp/时间戳 1622120720606 毫秒(ms) 2021-05-27 21:05:20 北京时间 batchId=773988 通过接口名判断是回调接口的第一个参数，具体含义不知道 123Preview的最后一行：dwr.engine._remoteHandleCallback(&apos;773988&apos;,&apos;0&apos;,[s19,s20,s21,s22,s23,s24,s25,s26,s27,s28,s29,s30,s31,s32,s33,s34,s35,s36,s37,s38]); xhr中的PostBean.getPostHots.dwr文件的Response数据 12 html页面的标签属性解析多贴子列表展示的lofter地址 1234567&lt;div class="archiveitm m-post m-post-img"&gt;&lt;!--帖子的div--&gt; &lt;a class="fullnk f-cb" href="#" onclick="loft.m.tagarchive.g.showPostLayer(this,'308f4a6f_1c93a97fd',event);"&gt; &lt;span class="pic"&gt;&lt;img src="https://imglf6.lf127.net/img/WTJGSFVSQkQvU3YxRFhSdDhLV3NKRXNHVlNtbjFQUXZQNWhUdEsrVzJVbWJkK1V6cnhMRTN3PT0.png?imageView&amp;amp;thumbnail=300y300&amp;amp;type=png&amp;amp;quality=96&amp;amp;stripmeta=0" /&gt;&lt;!--帖子的封面图片，一般为帖子内容的第一张图--&gt;&lt;/span&gt; &lt;span class="layer f-trans"&gt;&lt;/span&gt; &lt;/a&gt; &lt;div class="info f-cb" onmouseover="loft.g.dousercard(this,814697071,true);" onmouseout="loft.g.dousercard(this,814697071,false);"&gt; &lt;a target="_blank" class="img" href="https://moxueyibeizidoushijinchuya.lofter.com"&gt;&lt;!--博主的lofter主页地址--&gt;&lt;img src="https://avaimg.lf127.net/img/WTJGSFVSQkQvU3MxaDEvTHhQZ1hUdE5qTHpDYUtjU2I2eGUrTnYzazFqWjVBQTZ4dytXTjJBPT0.jpg?imageView&amp;amp;thumbnail=30x30&amp;amp;type=jpg" /&gt;&lt;!--博主的个人头像--&gt;&lt;/a&gt; &lt;a target="_blank" class="name" href="https://moxueyibeizidoushijinchuya.lofter.com"&gt;此用户不存在&lt;/a&gt; &lt;!--博主的用户名--&gt; &lt;/div&gt;&lt;/div&gt; 但此处存在难点，从多帖子列表进入某单一帖子并不会直接跳转到目标网址，而是调用一个onclick事件loft.m.tagarchive.g.showPostLayer(this,&#39;1fc3c2e3_1cc2b1a44&#39;,event);，推测为ajax全刷网页而不变更地址。 多帖子单列展示 测试DEMO V1]]></content>
      <tags>
        <tag>Reptile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode1035]]></title>
    <url>%2F2021%2F05%2F21%2FLeetcode1035%2F</url>
    <content type="text"><![CDATA[1035. 不相交的线难度中等 在两条独立的水平线上按给定的顺序写下 nums1 和 nums2 中的整数。 现在，可以绘制一些连接两个数字 nums1[i] 和 nums2[j] 的直线，这些直线需要同时满足满足： nums1[i] == nums2[j] 且绘制的直线不与任何其他连线（非水平线）相交。 请注意，连线即使在端点也不能相交：每个数字只能属于一条连线。 以这种方法绘制线条，并返回可以绘制的最大连线数。 示例 1： 输入：nums1 = [1,4,2], nums2 = [1,2,4]输出：2解释：可以画出两条不交叉的线，如上图所示。但无法画出第三条不相交的直线，因为从 nums1[1]=4 到 nums2[2]=4 的直线将与从 nums1[2]=2 到 nums2[1]=2 的直线相交。示例 2： 输入：nums1 = [2,5,1,2,5], nums2 = [10,5,2,1,5,2]输出：3示例 3： 输入：nums1 = [1,3,7,1,7,5], nums2 = [1,9,2,5,1]输出：2 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/uncrossed-lines著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 Code-v1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution &#123;public: static const int MAX_NUM_SIZE=2000+5; //nums元素的最大 大小 static const int MAX_NUMARR_LEN=500+5;//nums数组最大 长度 vector&lt;int&gt;numsIdxTb [MAX_NUM_SIZE]; vector&lt;int&gt; dpTable[MAX_NUMARR_LEN]; vector&lt;int&gt; tailEle[MAX_NUMARR_LEN];//最优解[i] 对应的最长子序列的最后一位元素 int maxUncrossedLines(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; //memset(dpTable,0,MAX_NUMARR_LEN*sizeof(int)); /* O(len)记录 可能配对方案 记录nums2各个元素 i 的位置 */ for(int i=0;i&lt;nums2.size();++i) &#123; numsIdxTb[nums2[i]].push_back(i); &#125; /* len*numSize*log2k 转换成一个特殊的最长递增子序列问题 dp[i]=max(dp[j]+ tailEle[j]&gt;=nums[i]? 0 : 1 ; ) */ int ans=0; dpTable[0].push_back(0); tailEle[0].push_back(-1); for(int i=1;i&lt;=nums1.size();i++) &#123; int currNum=nums1[i-1]; for(int k=0;k&lt;numsIdxTb[currNum].size();k++) &#123;//初始化 dpTable[i].push_back(1); tailEle[i].push_back(numsIdxTb[currNum][k]); &#125; for(int j=0;j&lt;i;j++) &#123; for(int k=0;k&lt;numsIdxTb[currNum].size();k++) &#123; int tailEleTmp=numsIdxTb[currNum][k]; for(int l=0;l&lt;tailEle[j].size();++l) &#123; if(tailEle[j][l]&lt;tailEleTmp) &#123;//满足严格递增 if(dpTable[i][k]&lt;dpTable[j][l]+1) &#123;//有收益 末尾追加当前元素 dpTable[i][k]=dpTable[j][l]+1; &#125; &#125; &#125; if(ans&lt;dpTable[i][k])ans=dpTable[i][k]; &#125; &#125; &#125; return ans; &#125;&#125;; 12执行用时：48 ms, 在所有 C++ 提交中击败了5.28%的用户内存消耗：10.1 MB, 在所有 C++ 提交中击败了92.26%的用户 Code-v2注意到 二分查找 lower_bound(起始地址，结束地址，要查找的数值) 返回的是大于或等于val的第一个元素位置 upper_bound(起始地址，结束地址，要查找的数值) 返回的是返回大于val的第一个元素位置 binary_search(起始地址，结束地址，要查找的数值) 返回的是是否存在这么一个数，是一个bool值。 lower_bound()和upper_bound()两个函数的用法类似，在一个左闭右开的有序区间里进行二分查找，需要查找的值由第三个参数给出。 upper_bound返回的是被查序列中第一个大于查找值的指针，也就是返回指向被查值&gt;查找值的最小指针 lower_bound则是返回的是被查序列中第一个大于等于查找值的指针，也就是返回指向被查值&gt;=查找值的最小指针。 除此之外，这两个函数还分别有一个重载函数，可以接受第四个参数。如果第四个参数传入greater()，其中Type改成对应类型，那么upper_bound则返回指向被查值&lt;查找值的最小指针，lower_bound则返回指向被查值&lt;=查找值的最小指针。 此外，如果你用上述两个函数三个参数的那种形式，记得那个左闭右开的区间要为非递减的顺序，如果你给第四个参数传入greater()，则区间为非递增的顺序。 123456789101112131415#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int main()&#123; vector&lt;int&gt; tailEle[5]; tailEle[0]=&#123;4,5,22,23,23,55,66,66,77,100,105&#125;; int item=1; auto idx=lower_bound(tailEle[0].begin(),tailEle[0].end(),item); //if(idx != tailEle[0].end()&amp;&amp; idx != tailEle[0].begin() )idx--; cout&lt;&lt; *(idx)&lt;&lt;endl; return 0;&#125; 应用到该题中 123456789101112int tailEleTmp=numsIdxTb[currNum][k];for(int l=0;l&lt;tailEle[j].size();++l)&#123; if(tailEle[j][l]&lt;tailEleTmp) &#123;//满足严格递增 if(dpTable[i][k]&lt;dpTable[j][l]+1) &#123;//有收益 末尾追加当前元素 dpTable[i][k]=dpTable[j][l]+1; &#125; &#125;&#125; 将在$tailEle[j]$中寻找最大小于$tailEleTmp$的元素对应的索引的$O(n)$遍历循环改为二分查找 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution &#123;public: static const int MAX_NUM_SIZE=2000+5; //nums元素的最大 大小 static const int MAX_NUMARR_LEN=500+5;//nums数组最大 长度 vector&lt;int&gt;numsIdxTb [MAX_NUM_SIZE]; vector&lt;int&gt; dpTable[MAX_NUMARR_LEN]; vector&lt;int&gt; tailEle[MAX_NUMARR_LEN];//最优解[i] 对应的最长子序列的最后一位元素 int maxUncrossedLines(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; //memset(dpTable,0,MAX_NUMARR_LEN*sizeof(int)); /* O(len)记录 可能配对方案 记录nums2各个元素 i 的位置 */ for(int i=0;i&lt;nums2.size();++i) &#123; numsIdxTb[nums2[i]].push_back(i); &#125; /* len*numSize*log2k 转换成一个特殊的最长递增子序列问题 dp[i]=max(dp[j]+ tailEle[j]&gt;=nums[i]? 0 : 1 ; ) */ int ans=0; dpTable[0].push_back(0); tailEle[0].push_back(-1); for(int i=1;i&lt;=nums1.size();i++) &#123; int currNum=nums1[i-1]; for(int k=0;k&lt;numsIdxTb[currNum].size();k++) &#123;//初始化 dpTable[i].push_back(1); tailEle[i].push_back(numsIdxTb[currNum][k]); &#125; for(int j=0;j&lt;i;j++) &#123; for(int k=0;k&lt;numsIdxTb[currNum].size();k++) &#123; int tailEleTmp=numsIdxTb[currNum][k]; auto idx=lower_bound(tailEle[j].begin(),tailEle[j].end(),tailEleTmp); if(idx != tailEle[j].begin() )idx--; else&#123; continue; &#125; int tailEle_j_l=*idx; int dpTable_j_l=dpTable[j][(int)(idx - tailEle[j].begin())]; if(dpTable[i][k]&lt;dpTable_j_l+1) &#123;//有收益 末尾追加当前元素 dpTable[i][k]=dpTable_j_l+1; &#125; if(ans&lt;dpTable[i][k])ans=dpTable[i][k]; &#125; &#125; &#125; return ans; &#125;&#125;; 微弱的提升 123执行用时：44 ms, 在所有 C++ 提交中击败了5.30%的用户内存消耗：10.1 MB, 在所有 C++ 提交中击败了92.06%的用户 Code-v3不断缩小二分查找区间 性质：$dpTable[i-1][k]&lt;=dpTable[i][k]$以及$tailEle[i-1][k]&lt;tailEle[i][k]$ 12345678910111213141516171819202122vector&lt;int&gt;::iterator idx;vector&lt;int&gt;::iterator newBegin=tailEle[j].begin();for(int k=0;k&lt;numsIdxTb[currNum].size();k++)&#123; int tailEleTmp=numsIdxTb[currNum][k]; idx=lower_bound(newBegin,tailEle[j].end(),tailEleTmp); if(idx != tailEle[j].begin() )&#123; newBegin=idx; idx--; &#125; else&#123; continue; &#125; int tailEle_j_l=*idx; int dpTable_j_l=dpTable[j][(int)(idx - tailEle[j].begin())]; if(dpTable[i][k]&lt;dpTable_j_l+1) &#123;//有收益 末尾追加当前元素 dpTable[i][k]=dpTable_j_l+1; &#125; if(ans&lt;dpTable[i][k])ans=dpTable[i][k];&#125; 又是4ms的微弱提升。。。。 12执行用时：40 ms, 在所有 C++ 提交中击败了5.30%的用户内存消耗：10.1 MB, 在所有 C++ 提交中击败了92.06%的用户]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode692]]></title>
    <url>%2F2021%2F05%2F20%2FLeetcode692%2F</url>
    <content type="text"><![CDATA[给一非空的单词列表，返回前 k 个出现次数最多的单词。 返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率，按字母顺序排序。 示例 1： 输入: [“i”, “love”, “leetcode”, “i”, “love”, “coding”], k = 2输出: [“i”, “love”]解析: “i” 和 “love” 为出现次数最多的两个单词，均为2次。 注意，按字母顺序 “i” 在 “love” 之前。 示例 2： 输入: [“the”, “day”, “is”, “sunny”, “the”, “the”, “the”, “sunny”, “is”, “is”], k = 4输出: [“the”, “is”, “sunny”, “day”]解析: “the”, “is”, “sunny” 和 “day” 是出现次数最多的四个单词， 出现次数依次为 4, 3, 2 和 1 次。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/top-k-frequent-words著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 哈希表对频次计数+优先队列排序12 C++ STL Map+Priority_queue12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution &#123;public: class myPair &#123; public: string wd; int freq; myPair()&#123;&#125; myPair(string wd,int freq)&#123; this-&gt;wd=wd; this-&gt;freq=freq; &#125; bool operator &lt; (const myPair&amp; other)const &#123; if(this-&gt;freq==other.freq)&#123; return this-&gt;wd &gt; other.wd; &#125;else&#123; return this-&gt;freq&lt;other.freq; &#125; &#125; &#125;; map&lt;string,int&gt;word_num; map&lt;string,int&gt;::iterator it; map&lt;string,int&gt;::iterator word_num_iter; priority_queue&lt;struct myPair&gt;topKHeap; vector&lt;string&gt; topKFrequent(vector&lt;string&gt;&amp; words, int k) &#123; for(int i=0;i&lt;words.size();i++) &#123; word_num_iter=word_num.find(words[i]); if(word_num_iter==word_num.end()) &#123; word_num.insert(make_pair(words[i],1)); &#125;else&#123; word_num_iter-&gt;second+=1; &#125; &#125; for(it=word_num.begin();it!=word_num.end();it++) &#123; topKHeap.push(myPair(it-&gt;first,it-&gt;second)); &#125; vector&lt;string&gt;res; for(int i=0;i&lt;k &amp;&amp; !topKHeap.empty();i++) &#123; res.push_back(topKHeap.top().wd); topKHeap.pop(); &#125; return res; &#125;&#125;; 12执行用时：16 ms, 在所有 C++ 提交中击败了56.49%的用户内存消耗：11.3 MB, 在所有 C++ 提交中击败了13.05%的用户 限制优先队列大小为k改最大堆为最小堆，当最小堆大小超过k时，丢弃堆顶元素。最后留下的就是topK的结果，然后用stack反序。 12345678910111213141516171819202122232425262728293031class myPair&#123; public: string wd; int freq; myPair()&#123;&#125; myPair(string wd,int freq)&#123; this-&gt;wd=wd; this-&gt;freq=freq; &#125; bool operator &lt; (const myPair&amp; other)const &#123; if(this-&gt;freq==other.freq)&#123; return this-&gt;wd &lt; other.wd; &#125;else&#123; return this-&gt;freq&gt;other.freq; &#125; &#125;&#125;;stack&lt;string&gt;stk;vector&lt;string&gt;res;for(int i=0;i&lt;k &amp;&amp; !topKHeap.empty();i++)&#123; stk.push(topKHeap.top().wd); topKHeap.pop();&#125;while(!stk.empty())&#123; res.push_back(stk.top()); stk.pop();&#125; 12执行用时：12 ms, 在所有 C++ 提交中击败了86.28%的用户内存消耗：11.2 MB, 在所有 C++ 提交中击败了22.26%的用户]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode1328]]></title>
    <url>%2F2021%2F05%2F19%2FLeetcode1328%2F</url>
    <content type="text"><![CDATA[给你一个回文字符串 palindrome ，请你将其中 一个 字符用任意小写英文字母替换，使得结果字符串的字典序最小，且 不是 回文串。 请你返回结果字符串。如果无法做到，则返回一个空串。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/break-a-palindrome著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 示例 1： 12输入：palindrome = &quot;abccba&quot;输出：&quot;aaccba&quot; 示例 2： 12输入：palindrome = &quot;a&quot;输出：&quot;&quot; 对于大多数回文串，只需要破坏其中一位就可使其不再是回文串。要求破坏后的字符串字典序最小，从左往右置更小的字符（最小为’a’）。大致分为以下几种情况 长度为 1 的回文串，直接返回空串”” 对于回文串 $a_{0}a_{1}a_{2}…a_{i}a_{i}…a_{2}a_{1}a_{0}$：若字符$a_{k}$（$k\in [0,i]$）不为’a’，则将其置为’a’返回结果。否则为全’a’回文串，转第三步 $a_{0}a_{1}a_{2}…a_{i}a_{j}a_{i}…a_{2}a_{1}a_{0}$：若字符$a_{k}$（$k\in [0,i]$）不为’a’，则将其置为’a’返回结果。否则形如$aaa…a_{j}…aaa$，转第三步。需要注意的是$a_{j}$位于回文串的对称中心，修改$a_{j}$后仍为回文串。 对于全’a’回文串或者形如$aaa…a_{j}…aaa$的回文串，将其最后一位字符修改为’b’，返回结果。 123456789101112131415161718class Solution &#123;public: string breakPalindrome(string palindrome) &#123; if (palindrome.size()==1)&#123; return ""; &#125; string resStr=palindrome; int length=resStr.size(); for(int i=0;i&lt;length;++i)&#123; if(resStr[i]!='a' &amp;&amp; i!=length/2)&#123; resStr[i]='a'; return resStr; &#125; &#125; resStr[resStr.size()-1]='b'; return resStr; &#125;&#125;; 123执行用时：4 ms, 在所有 C++ 提交中击败了38.63%的用户内存消耗：6.1 MB, 在所有 C++ 提交中击败了43.32%的用户]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2vec]]></title>
    <url>%2F2021%2F05%2F18%2FWord2vec%2F</url>
    <content type="text"><![CDATA[2013年，Google开源了一款用于词向量计算的工具——Word2vec，可以在百万数量的词典和上亿数量级的数据集上进行高效训练，得到的训练结果——词向量（Word Embedding）——可以很好的度量词与词之间的相似性。 One-Hot向量假设词典中有N个单词，使用独热编码，则每个单词的向量维度为N，每个向量只有一个分量为1，其余为0 “土豆” [1,0,0] “马铃薯” [0,1,0] “番茄” [0,0,1] One-Hot向量简单的表示了一个词语，但是却无法有效表达它们的语义信息。“土豆”和“马铃薯”虽然是同一种食物，但利用常规的向量距离公式，比如欧几里德距离或者余弦距离公式，都无法有效计算它们的相似度，显然这种方式不能很好地表达词之间的相似性。 Dristributed 词向量通过训练，将每一个词映射到较短的向量上。 如神经网络DNN来训练出词向量，一般采用三层神经网络结构，分为输入层，隐藏层，和输出层（softmax层）。 word2vec两个模型 跳字模型（Skip-gram） 输入一个词，给出该词在文本序列周围的词（预测单个词的上下文Context）。 连续词袋模型（CBOW） 输入某一个特征词的上下文相关的词对应的词向量，输出该特定词。 假设我们有一句话，people make progress every day。输入的是4个词向量，’people’ , ‘make‘ , ‘every’ , ‘day’, 输出是词汇表中所有词的softmax概率，我们的目标是期望progress词对应的softmax概率最大。 CBOW One-Word Context模型 该模型输入一个上下文单词，预测一个目标单词。 输入层 输入层是一个用One-Hot编码表示的单词向量，其维度为词典大小 $V$。记为 $x$，形状为 V x 1 输入层-&gt;隐藏层 全连接，权重矩阵为 $W$，形状为 V x N。输出为向量 $h$，形状为 N x 1 前向计算公式为： \Large h=W^{T}\cdot x注意到输入向量 $x$是One-Hot向量，只有一维为1其余为0。等价的有 \Large h=v_{w_{I}}^{T}其中 $v_{wI}$ 为权重矩阵 $W$ 的第 $k$ 行，相应的输入向量的第 $k$ 维为1 隐藏层-&gt;输出层 全连接，权重矩阵为$W’$，形状为N x V。输出向量为$u$，形状为 V x 1 前向计算公式为： \Large u=W'^{\, T}\cdot h对其分量有 \Large u_{j}=v^{'\, T}_{w_{j}} \cdot h其中$\Large v\,’^{\, T}_{w_{j}}$为$W\,’$的第 $j$ 列向量 Softmax层 经过Softmax层后，输出层第 j 个结点输出的值为 \Large p(w_{j}|w_{I})=\frac{e^{u_{j}}}{\sum^{V}_{j\,'=1}e^{u_{j\,'}}}代入$(2)$和$(4)$式，有 \Large p(w_{j}|w_{I})=\frac{e^{ v^{'\, T}_{w_{j}} \cdot v_{w_{I}}^{T} }}{\sum^{V}_{j\,'=1}e^{ v^{'\, T}_{w_{j\,'}} \cdot v_{w_{I}}^{T} }} 损失函数 该模型的任务为输入上下文单词$w_{I}$的One-Hot向量，经过上述前向传播过程后，预测目标单词$w_{O}$（标签值为V x 1的One-Hot向量，对应于目标单词）。实际上是最大化$(7)$式的条件概率$p(w_{O}|w_{I})$ \Large max\ p(w_{O}|w_{I}) =>max\ log\ p(w_{O}|w_{I})\\ \Large =>max\ u_{j^{*}}-log\ \sum_{j\,'=1}^{V}e^{u_{j\,'}}一般的，我们会用梯度下降法求解参数，有损失函数 $E$ 如下 \Large E=-log\ p(w_{O}|w_{I})=log\ \sum_{j\,'=1}^{V}e^{u_{j\,'}}-u_{j^{*}} 隐藏层-&gt;输出层的权重更新($W^{\,’}$) 输入层-&gt;隐藏层的权重更新($W$) CBOW Multi-Word Context 两种高效训练方式]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode1545]]></title>
    <url>%2F2021%2F05%2F18%2FLeetcode1545%2F</url>
    <content type="text"><![CDATA[题目描述 1545. 找出第 N 个二进制字符串中的第 K 位难度中等14收藏分享切换为英文接收动态反馈 给你两个正整数 n 和 k，二进制字符串 Sn 的形成规则如下： S1 = &quot;0&quot; 当 i &gt; 1 时，Si = Si-1 + &quot;1&quot; + reverse(invert(Si-1)) 其中 + 表示串联操作，reverse(x) 返回反转 x 后得到的字符串，而 invert(x) 则会翻转 x 中的每一位（0 变为 1，而 1 变为 0）。 例如，符合上述描述的序列的前 4 个字符串依次是： S1 = “0” S2 = “011” S3 = “0111001” S4 = “011100110110001” 请你返回 Sn 的 第 k 位字符 ，题目数据保证 k 一定在 Sn 长度范围以内。 递归分解，n规模的问题分解到n-1规模子问题。稍稍注意的是规模为1时的基础解，以及右半部分向子问题传递参数时需要重新计算下k，因为有倒置操作。此外还需要对右半部分的解取反。 12345678910111213141516171819202122232425class Solution &#123;public: int index2[25]; void initIndex2()&#123; index2[0]=1; for(int i=1;i&lt;25;++i)index2[i]=index2[i-1]*2; &#125; char mySolution(int n, int k) &#123; if(n==1)return '0'; //中间位置 if(k==index2[n]/2)return '1'; //左半部分 else if(k&lt;index2[n]/2)return mySolution(n-1,k); //右半部分 else&#123; char tmp=mySolution(n-1,index2[n-1]-k+index2[n]/2); return tmp=='0'?'1':'0'; &#125; &#125; char findKthBit(int n, int k)&#123; initIndex2(); return mySolution(n,k); &#125;&#125;; 1234执行用时：0 ms, 在所有 C++ 提交中击败了100.00%的用户内存消耗：5.8 MB, 在所有 C++ 提交中击败了51.22%的用户]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode494]]></title>
    <url>%2F2021%2F05%2F17%2FLeetcode494%2F</url>
    <content type="text"><![CDATA[题目描述 给你一个整数数组 nums 和一个整数 target 。 向数组中的每个整数前添加 ‘+’ 或 ‘-‘ ，然后串联起所有整数，可以构造一个 表达式 ： 例如，nums = [2, 1] ，可以在 2 之前添加 ‘+’ ，在 1 之前添加 ‘-‘ ，然后串联起来得到表达式 “+2-1” 。返回可以通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/target-sum著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 题解： 枚举+剪枝 子集和 dp动规 枚举+剪枝123456789101112131415161718192021222324252627282930313233class Solution &#123;public: int suffixSum[50];//后缀和 int resNum=0; void dfs(vector&lt;int&gt;&amp; nums, int target,int depth,int sum)&#123; //剪枝 if( (sum+suffixSum[depth]&lt;target) || (sum-suffixSum[depth]&gt;target) )return ; //结束 if(sum==target&amp;&amp;depth==nums.size())&#123; resNum+=1;//succ res+=1 return ; &#125; if(depth&gt;=nums.size()&amp;&amp;sum!=target)return ;//fail res+=0 //搜索 for(int i=0;i&lt;2;i++)&#123; int selection=1+i*(-2); int searchSum=sum+selection*nums[depth]; dfs(nums,target,depth+1,searchSum); &#125; &#125; int findTargetSumWays(vector&lt;int&gt;&amp; nums, int target) &#123; resNum=0; sort(nums.begin(),nums.end());//排序 memset(suffixSum,0,50*sizeof(int)); for(int i=nums.size()-1;i&gt;-1;--i) &#123; suffixSum[i]=suffixSum[i+1]+nums[i]; &#125; dfs(nums,target,0,0); return resNum; &#125;&#125;; 执行用时：1116ms 内存消耗：8.7MB dp动规 状态转移方程： resNum[i][j]$$意为前$i$个数字($nums[0:i]$)构造运算结果等于 $j$的不同表达式的数目resNum[i][j]=resNum[i-1][j-nums[i]]+resNum[i-1][j+nums[i]]$$ 注意点：由于target维度可取负数，因此可以使用Offset将负数域的target偏移映射到非负数域。（或者使用map映射） 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: static const int MAXLEN=25; static const int MAX_TARGET=5000; int numOffset=0;//负数偏移 映射到非负数 int resNum[MAXLEN][MAX_TARGET]; void init(vector&lt;int&gt;&amp; nums)&#123; //target=true target -numOffset int num=nums[0]; memset(resNum,0,MAX_TARGET*sizeof(int)); resNum[0][num+numOffset]+=1; resNum[0][-num+numOffset]+=1; &#125; void dp(int idx,int target,vector&lt;int&gt;&amp; nums)&#123; if(resNum[idx][target +numOffset]!=-1)return; if(resNum[idx-1][target-nums[idx] +numOffset]==-1)dp(idx-1,target-nums[idx],nums); if(resNum[idx-1][target+nums[idx] +numOffset]==-1)dp(idx-1,target+nums[idx],nums); resNum[idx][target +numOffset]= resNum[idx-1][target-nums[idx] +numOffset] +resNum[idx-1][target+nums[idx] +numOffset]; &#125; int findTargetSumWays(vector&lt;int&gt;&amp; nums, int target) &#123; memset(resNum,-1,MAX_TARGET*MAXLEN*sizeof(int)); int absSum=0; for(int i=0;i&lt;nums.size();++i) &#123; absSum+=abs(nums[i]); &#125; numOffset=target-absSum; if(numOffset&lt;0)numOffset=-numOffset; else numOffset=0; if(-nums[0]+numOffset&lt;0)numOffset=nums[0]; init(nums); dp(nums.size()-1,target,nums); return resNum[nums.size()-1][target+ numOffset]; &#125;&#125;; 执行用时：24ms 内存消耗：9.3MB]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TagBasedPR]]></title>
    <url>%2F2021%2F04%2F18%2FTagBasedPR%2F</url>
    <content type="text"><![CDATA[之前实现了基于图的PersonRank算法，采用随机游走的方式求出物品结点与用户结点之间的相关性，完成top-k排序推荐。类似的，基于标签的推荐也可以在图上完成，在原先的user-item图中引入tag结点，构成了新的三分图：user-tag-item图，然后在该图上进行相似的操作，求出PR值用于推荐。 三分图是稀疏图，为了能够容纳更多的结点，使用链表形式建图。（矩阵形式消耗空间过多，应该可以使用稀疏矩阵进行存储和运算） 基于图的标签推荐算法 PR(v)= \left\{ \begin{array}{**lr**} \alpha \sum_{v'\in in(v)}\frac{PR(v')}{|out(v')|}, &(v \neq v_{u}) \\ (1-\alpha)+\alpha \sum_{v'\in in(v)}\frac{PR(v')}{|out(v')|}, &(v = v_{u}) \end{array} \right.user-tag-item Graph： 建图规则：有用户标签行为元组（user,tag,item），分别对应图中结点$v_{user}$、$v_{tag}$、$v_{item}$，则在图中添加两条边($v_{user}$，$v_{tag}$),($v_{tag}$，$v_{item}$），若边已经存在，则边权加一。 从用户u结点出发，采用随机游走，求出收敛时图上各节点的PR值，完成对用户u的topK推荐。 加权随机选择 demo: 1234567import numpy as npimport randomfrom itertools import accumulateimport bisectnode=['A','B','C','D','E','F','G','H']weight=np.random.randint(1,len(node),len(node))print(weight) [6 2 2 5 5 5 7 1] 12345678910111213141516171819202122232425262728293031323334353637def weightedRandSel(node,weight,selNum): ''' node:物品序列 weight:权重序列 selNum:随机选择次数 return:随机选择的结果序列 ''' sumWeight=sum(weight)#权重和 accuSumArr=list(accumulate(weight))#累积和序列 selArr=[] for i in range(selNum): randSel=random.uniform(0,sumWeight)#(0,权重和)的随机数 idx=bisect.bisect_right(accuSumArr, randSel)#二分查找randSel的位置 selectedNode=node[idx] selArr.append(selectedNode) return selArrdef checkRes(arr): ''' 检查随机选择的正确性，统计node及其出现次数 ''' judge=dict() for node in arr: if node not in judge.keys(): judge[node]=0 judge[node]+=1 return judgeselNum=50000#随机选择次数selNodeArr=weightedRandSel(node,weight,selNum)judgeRes= checkRes(selNodeArr)item=judgeRes.items()item=sorted(item,key=lambda item:item[0])print(item) [(&#39;A&#39;, 9050), (&#39;B&#39;, 3018), (&#39;C&#39;, 3036), (&#39;D&#39;, 7621), (&#39;E&#39;, 7646), (&#39;F&#39;, 7443), (&#39;G&#39;, 10658), (&#39;H&#39;, 1528)] 实际应用 123456sumPR =sum(list(PR.values()))#求出整个序列的权重和accuPR=list(accumulate(list(PR.values())))#累积和序列#权重随机选择randSelCnt = random.uniform(0,sumPR)#(0,sumPR)的随机值idx=bisect.bisect_right(accuPR, randSelCnt)#二分查找selectNode = nodeList[idx] 结点PR值更新 123456789def renewPR(selectNode): neighbors = list(Graph[selectNode].keys()) sumNeighborPR = 0.0 for neib in neighbors: sumNeighborPR += PR[neib] / Outdegree[neib] sumNeighborPR = alpha * sumNeighborPR PR[selectNode] = sumNeighborPR if selectNode == startPointName: PR[selectNode] = PR[selectNode] + 1 - alpha]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RecSys_TheQuestForQualityTags]]></title>
    <url>%2F2021%2F04%2F18%2FRecSys-TheQuestForQualityTags%2F</url>
    <content type="text"><![CDATA[论文阅读：The quest for quality tags提出的核心问题 标签评分接口(tag rating) 怎样的标签评价接口(rating interfaces)可以获得最多的评分？ 设计者应该实现怎样的标签评价接口，选择更好的标签来展示给用户？ 标签选择方法(tag selection method) 我们能否根据其他用户的行为来确定一个用户想要看到的标签？ 我们可以根据用户自己的评分来确定用户想要看到的标签吗？ 我们可以根据其他用户的评分来确定用户想要看到的标签吗？ 前人的结论 用户看到的标签会影响他们自己创建的标签 若将标签分为事实标签、主观标签和个人标签，用户通常更喜欢事实标签而不是主观标签，强烈不喜欢个人标签 标签评分接口回答问题：怎样的标签评价接口(rating interfaces)可以获得最多的评分 在MovieLens电影推荐系统上引入了标签的评分功能。评分是通过点赞/点踩来实现的，理由是星级评分占用较多的屏幕空间，同时进行了四组对比试验： C组没有显示任何评分部件，即不可对标签评分，无标签评分反馈 U组只可以点赞，只允许正反馈 D组只可以点踩，只允许负反馈 UD可以点赞/点踩 结论 UD组收集到最多的评价 用户倾向于点踩 回答问题：设计者应该实现怎样的标签评价接口，选择更好的标签来展示给用户？ 系统应该同时提供正面/负面评价功能（如：点赞、点踩） 论文中的实验结论： 当被系统同时提供正面/负面评价功能时，用户将产生更多的正面评价数目 增加的评分数量改善了许多标签选择方法的覆盖范围。 使用负面评级以及正面和负面评级的选择方法比仅使用正面评级或不使用评级的选择方法表现得更好。 使用能够标准化用户影响的标签选择策略 论文中的实验结论： 对于某些标签选择方法：如基于每个标签的搜索次数或应用次数top-k算法，其结果会被一小群“积极用户”所扭曲。按用户标准化的标记选择方法(如应用标记的用户数量num-users)比不应用标记的方法(如num-apps和num-searches)性能更好。 结合基于行为和基于评分的标签选择方法 论文中的实验结论： 基于用户自己评分的方法精确度很高，但覆盖率很低 混合方法在精确度和覆盖率上表现均比较良好（topk排名k值较小时，混合方法表现略差） 可以认为用户通常对相同的标签进行一致的评价，不管它被应用到任一物品(item/标的物)上。 预测标签质量三种基于隐式行为标签选择策略评价指标：标签的平均评分 回答问题：我们能否根据其他用户的行为来确定一个用户想要看到的标签? num-apps：标签被应用的数量。 num-users：应用标签的用户数量 num-searches：标签被搜索的数量。可以转化为对标签的搜索事件和点击事件（前端埋点、日志记录） 存在问题：num-apps与num-searches均会受到少量极端用户的影响。对于num-apps，可能会有少数用户大量使用讨人厌的“个人”标签，导致系统展示的标签平均评分降低。而对于num-searches，可能会有一些“狂热”用户对于某些相同的标签进行频繁搜索，从而降低系统展示标签的平均评分 基于显式行为的标签选择策略（评分，点赞/点踩） 个人用户评分 回答问题：我们可以根据用户自己的评分来确定用户想要看到的标签吗？ user-rating： 该策略认为某一用户总是喜欢他”点赞“的标签，讨厌他”点踩“的标签 user-avg：计算同一用户对同一标签多次评分的均值（对同一标签的多次评分行为，可能发生在标签出现在不同标的物上，论文中举例是用户Sally在“28天后”和“活死人黎明”中都给“僵尸”标签打分） 全体用户评分 基于假设：大多数用户对标签有相似的意见 回答问题：我们可以根据其他用户的评分来确定用户想要看到的标签吗？ global-avg 计算所有用户的平均评分，作为标签评分 consec-apps 基于初始相同评分数量进行top-k排序。（背后的思想是根据标签的分歧程度进行排名，例如：一个标签的赞/踩数目相近，则该标签的分歧较大，不应该推荐。论文中在Table 3中列举了10个最具争议的标签） consec-users 基于来自不同用户的初始相同评分数量进行top-k排序，对consec-apps的一个改进，避免某些积极用户对少数标签的过分关注。（论文中举例：“僵尸”标签总共有五个正面标签，但之前对“僵尸”的四个评级可能都来自同一用户） 集成学习，综合预测受到集成学习的启发，综合多个“专家”系统的结果，可以提高整体性能，论文中选择上述的六种策略进行混合：num-users、num-searches、user-avg、global-avg、consec-apps、consec-users]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PersonalRank]]></title>
    <url>%2F2021%2F03%2F25%2FPersonalRank%2F</url>
    <content type="text"><![CDATA[基于图的推荐算法：PersonalRank算法 算法介绍用户数据采用二元组(u,i)表示用户u对物品i产生过行为，这种数据集很容易用一个二分图表示 用户结点与物品结点之间有连线，说明用户对物品产生过行为，例如用户结点A与物品结点a,b,d之间存在连线，说明用户A与物品a,b,d之间产生过行为。 那么现在给用户u推荐物品的任务就转化为，度量用户结点$v_{u}$与物品结点v之间的相关性，其中v是在图中还未与$v_{u}$有连线的物品结点。 基于随机游走的PersonalRank算法从用户结点$v_{u}$出发，在图中进行随机游走，每到达一个结点后，按照概率$\alpha$决定是否继续游走。如果继续游走，则从当前结点指向的next结点集合中按照均匀分布随机选择一个结点作为游走的下一节点；如果停止本次游走，则从$v_{u}$结点重新开始。经过多次随机游走后，每个物品被访问到的概率会收敛到一个数，最后的推荐列表中的物品权重就是物品结点的访问概率。 \begin{equation}PR(j)=\left\{\begin{matrix}\alpha*\sum_{i\in{in(j)}}\frac{PR(i)}{|out(i)|}\ \ \ \ if(j\ne{u})\\(1-\alpha)+\alpha*\sum_{i\in{in(j)}}\frac{PR(i)}{|out(i)|} \ \ \ \ if(j=u)\end{matrix}\right.\label{pr}\end{equation}其中$in(j)$是结点j的入边的起始点的集合，即$i\in in(j)$，则i—&gt;j，$out(i)$是结点i的出边数目，$\alpha$是上面提到的继续游走概率 PageRank算法比较核心思想 如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高 如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高 简单的理解：排名高的网页&lt;=&gt;”追随者“多，”追随者”厉害 算法步骤主要分为两步骤： 初始化：赋予每个网页结点PR初值（通常初始化为$\frac{1}{N}$，其中$N$为网页总数） 迭代：通过PR的转移公式进行全图PR值的更新，直至收敛 PageRank——1.0PR值计算公式： PR(u)=\sum_{v\in B_{u}}\frac{PR(v)}{OutDegree(v)}Demo： 123456789101112131415161718192021222324252627282930313233343536373839404142Graph=&#123;'A':['B','C','D'], 'B':['A','D'], 'C':['D'], 'D':['B'], &#125;import numpy as npnum=len(list(Graph.keys()))nodeList=list(Graph.keys())node2idx=dict()for i in range(len(nodeList)): node2idx[nodeList[i]]=iG=np.zeros((num,num))#建图for node,neighbors in Graph.items(): for neighbor in neighbors: G[node2idx[node]][node2idx[neighbor]]=1print(G)Weight=G.copy()#结点出度统计OutDegree=np.sum(G,axis=1)OutDegree=OutDegree.reshape((len(OutDegree),1))print(OutDegree)#广播机制计算权重矩阵w=1/outDegreeWeight=Weight/OutDegreeprint(Weight)建图：[[0. 1. 1. 1.] [1. 0. 0. 1.] [0. 0. 0. 1.] [0. 1. 0. 0.]] 结点的出度向量：[[3.] [2.] [1.] [1.]] 权重矩阵：[[0. 0.33333333 0.33333333 0.33333333] [0.5 0. 0. 0.5 ] [0. 0. 0. 1. ] [0. 1. 0. 0. ]] 12345678910111213141516171819202122PR=np.array([1/num]*num).reshape((1,num))print(PR)def train(PR,Weight,iterNum): for i in range(iterNum): PR=PR.dot(Weight)# 1*4 4*4 print("step %i PR: "%(i),PR) return PRtrain(PR,Weight,10)#迭代10次，PR值收敛[[0.25 0.25 0.25 0.25]]step 0 PR: [[0.125 0.33333333 0.08333333 0.45833333]]step 1 PR: [[0.16666667 0.5 0.04166667 0.29166667]]step 2 PR: [[0.25 0.34722222 0.05555556 0.34722222]]step 3 PR: [[0.17361111 0.43055556 0.08333333 0.3125 ]]step 4 PR: [[0.21527778 0.37037037 0.05787037 0.35648148]]step 5 PR: [[0.18518519 0.42824074 0.07175926 0.31481481]]step 6 PR: [[0.21412037 0.37654321 0.0617284 0.34760802]]step 7 PR: [[0.1882716 0.41898148 0.07137346 0.32137346]]step 8 PR: [[0.20949074 0.38413066 0.0627572 0.3436214 ]]step 9 PR: [[0.19206533 0.41345165 0.06983025 0.32465278]] v1.0存在的问题——排名泄露、排名下沉、排名上升、排名泄露：如果存在网页A出度为0，经过多次的迭代后，所有网页的PR趋向与0。一方面网页A的PR值由其入结点贡献，但另一方面网页A并不向其他网页共享PR值，整体PR值下降。 1234567891011[[0.25 0.25 0.25 0.25]]step 0 PR: [[0.125 0.08333333 0.08333333 0.45833333]]step 1 PR: [[0.04166667 0.04166667 0.04166667 0.16666667]]step 2 PR: [[0.02083333 0.01388889 0.01388889 0.07638889]]step 3 PR: [[0.00694444 0.00694444 0.00694444 0.02777778]]step 4 PR: [[0.00347222 0.00231481 0.00231481 0.01273148]]step 5 PR: [[0.00115741 0.00115741 0.00115741 0.00462963]]step 6 PR: [[0.0005787 0.0003858 0.0003858 0.00212191]]step 7 PR: [[0.0001929 0.0001929 0.0001929 0.0007716]]step 8 PR: [[9.64506173e-05 6.43004115e-05 6.43004115e-05 3.53652263e-04]]step 9 PR: [[3.21502058e-05 3.21502058e-05 3.21502058e-05 1.28600823e-04]] 排名下沉：若网页A没有入度链接，经过多次迭代后，A的PR值趋向与0 排名上升：互联网中一个网页只有对自己的出链，或者几个网页的出链形成一个循环圈。那么在不断地迭代过程中，这一个或几个网页的PR值将只增不减。 PageRank——2.0修正的PR值计算公式，增加阻尼系数q PR(u)=q \cdot (\sum_{v\in B_{u}}\frac{PR(v)}{OutDegree(v)})+(1-q)至此，PageRank公式与上述的PersonalRank公式一致，含义上也是相似的，此处的q代表着用户在当前网页停留后点击下一网页的概率，即在整个图上做随机游走。 完整PageRank公式 PR(u)=q \cdot (\sum_{v\in B_{u}}\frac{PR(v)}{OutDegree(v)})+\frac{(1-q)}{N}]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode_for_offer47]]></title>
    <url>%2F2021%2F03%2F16%2FLeetcode-for-offer47%2F</url>
    <content type="text"><![CDATA[剑指Offer47.礼物的最大值在一个 m*n 的棋盘的每一格都放有一个礼物，每个礼物都有一定的价值（价值大于 0）。你可以从棋盘的左上角开始拿格子里的礼物，并每次向右或者向下移动一格、直到到达棋盘的右下角。给定一个棋盘及其上面的礼物的价值，请计算你最多能拿到多少价值的礼物？ 来源：力扣（LeetCode）链接：礼物的最大值.著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 dfs暴力搜索，$O(2^{n})$ dp动态规划，$O(N*M)$ dfs 1234567891011121314151617int dfs(vector&lt;vector&lt;int&gt;&gt;&amp; grid,int M,int N,int r,int c,int sum)&#123; //edge test if(r==M-1&amp;&amp;c==N-1)&#123; return sum+grid[r][c]; &#125; if(c==N-1)return dfs(grid,M,N,r+1,c,sum+grid[r][c]); else if(r==M-1)return dfs(grid,M,N,r,c+1,sum+grid[r][c]); else&#123; return max(dfs(grid,M,N,r+1,c,sum+grid[r][c]) ,dfs(grid,M,N,r,c+1,sum+grid[r][c]) ); &#125; &#125; int maxValue(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int M=grid.size(); int N=grid[0].size(); return dfs(grid,M,N,0,0,0); &#125; dp 状态转移方程：$dpTable[i][j]=max{dpTable[i-1][j],dpTable[i][j-1]}+mat[i][j]$ 分析：dpTable[i][j]的最优解总是从两个子问题最优解的较大者转移而来12345678910111213141516171819202122const static int MAX_M=200; const static int MAX_N=200; int maxValue(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int M=grid.size(); int N=grid[0].size(); int dpTable[MAX_M][MAX_N]; //dpTable[i][j]=max&#123;dpTable[i-1][j],dpTable[i][j-1]&#125;+mat[i][j] memset(dpTable,0,MAX_M*MAX_N*sizeof(int)); //init the edge of table dpTable[0][0]=grid[0][0]; for(int i=1;i&lt;M;i++)dpTable[i][0]=grid[i][0]+dpTable[i-1][0]; for(int i=1;i&lt;N;i++)dpTable[0][i]=grid[0][i]+dpTable[0][i-1]; for(int i=1;i&lt;M;i++)&#123; for(int j=1;j&lt;N;j++)&#123; dpTable[i][j]=grid[i][j]+max(dpTable[i-1][j] ,dpTable[i][j-1]); &#125; &#125; return dpTable[M-1][N-1]; &#125;]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode565]]></title>
    <url>%2F2021%2F03%2F15%2FLeetcode565%2F</url>
    <content type="text"><![CDATA[667.数组嵌套数组嵌套.题目描述：索引从0开始长度为N的数组A，包含0到N - 1的所有整数。找到最大的集合S并返回其大小，其中 S[i] = {A[i], A[A[i]], A[A[A[i]]], … }且遵守规则：假设选择索引为i的元素A[i]为S的第一个元素，S的下一个元素应该是A[A[i]]，之后是A[A[A[i]]]… 以此类推，不断添加直到S出现重复的元素。 解法 暴力枚举 复杂度$O(N^{2})$12345678910111213141516171819int arrayNesting(vector&lt;int&gt;&amp; nums) &#123; int N=nums.size(); int res=0; for(int i=0;i&lt;N;i++)&#123; memset(vis,0,MAX_N*sizeof(bool)); //S[i] int leftNum=nums[i];//init first postion: A[i] for(int j=0;j&lt;=N;j++)&#123; if(!vis[leftNum]) vis[leftNum]=true; else&#123;//end, length&lt;-i if(res&lt;j)res=j; break; &#125; leftNum=nums[leftNum]; &#125; &#125; return res; &#125; 图论$O(n)$注意到以下几点： 嵌套关系可以抽象成有向边 基于第一点，以数组元素$A[i]$为一个结点，则图中每个节点出度、入度均为1。(对于$A[i]$，入结点和出结点分别为$A[j]=i,A[k]=A[A[i]]$) 综上可以看出： 集合S由图中一个环上的结点元素值组成 且由于图中每个节点的出度、入度均为1，则每个节点仅处于一个环上1234567891011121314151617181920212223242526int MAX_N=20005; bool vis[20005]; int arrayNesting(vector&lt;int&gt;&amp; nums) &#123; int N=nums.size(); int res=0; memset(vis,0,MAX_N*sizeof(bool)); int pos=0;//the latest start point while(pos&lt;N)&#123; while(pos&lt;N)&#123; if(!vis[nums[pos]])break; ++pos; &#125; if(pos&gt;=N)break; //S[pos]=&#123;A[pos],A[A[pos]]&#125;... int leftNum=nums[pos]; int len=0; while(vis[leftNum]==false)&#123; vis[leftNum]=true; leftNum=nums[leftNum]; ++len; &#125; if(res&lt;len)res=len; &#125; return res; &#125;]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UserCF-itemCF]]></title>
    <url>%2F2021%2F03%2F09%2FUserCF-itemCF%2F</url>
    <content type="text"><![CDATA[基于领域的推荐算法：分为两大类，一类是基于用户的协同过滤算法，一类是基于物品的协同过滤算法。核心：聚类、匹配相似的人/相似的物品 算法介绍 基于用户的协同过滤算法(userCF)： 步骤一：找到和目标用户相似的用户集合 步骤二：将该集合中用户喜欢的物品，推荐给目标用户 思想：目标用户与其相似用户的兴趣类似 常用的几种相似度计算公式 $N(u)$、$N(v)$分别为用户u、v有过正反馈的物品集合 Jaccard公式：计算两个集合A和B的交集元素在A、B的并集中所占的比例 sim_{u,v}=\frac{|N(u)\cap N(v)|}{|N(u)\cup N(v)|} 余弦相似度： 用物品向量（有过正反馈的物品的向量分量置1，否则为0）度量 则u、v两用户的相似度为 sim_{u,v}=\frac{|N(u)\cap N(v)|}{\sqrt{|N(u)|| N(v)|}}等价于原始的向量余弦相似度定义 sim_{u,v}=\frac{\vec A\cdot \vec B}{|\vec A||\vec B|}用户的物品向量为$\vec A=(x_{0},x_{1}…x_{n})$其中$x_{i}$可取0、1​ 基于物品(标的物)的协同过滤算法(itemCF)： 步骤一：计算物品之间相似度 步骤二：分析目标用户的历史行为，为其推荐与喜欢物品相似的其他物品 思想：目标用户曾经对物品A感兴趣，而物品B与A相似，则认为用户也会对B感兴趣 常用的几种相似度计算公式 $N(i)$、$N(j)$分别为喜欢物品i、j的用户集合 公式一 sim_{i,j}=\frac{|N(i) \cap N(j)|}{|N(i)|}直观理解，该相似度表达的含义是喜欢物品i的用户中，有多大比例喜欢物品 j 。该相似度指标的最大的一个问题是热门物品会跟任何一个物品都很相似。（喜欢物品i的用户大都喜欢（有过正反馈）热门物品 j ） 公式二，余弦相似度（惩罚热门物品） 对于物品，使用用户向量度量相似度（有过正反馈的用户的向量分量置1，否则为0） 物品i、j的相似度为 sim_{i,j}=\frac{|N(i) \cap N(j)|}{\sqrt{|N(i)||N(j)|}}以上是通过集合运算得出的余弦相似度结果，等价于两用户向量间的余弦相似度计算。虽然该公式对热门物品做出了一定的惩罚，但当物品j非常热门时，物品i与热门物品j还是会获得较大的相似度。 公式三 sim_{i,j}=\frac{|N(i) \cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^{\alpha}}修改分母的指数项大小，进一步惩罚热门物品 实验DEMO 数据集：MovieLens 1M Dataset 概述：包含6000多用户对4000多部电影的100万条评分 数据格式： User：UserID::Gender::Age::Occupation::Zip-code MOVIES：MovieID::Title::Genres RATINGS：UserID::MovieID::Rating::Timestamp 数据读取： 123456userfile=open("D:/SIT/推荐系统/ml-1m/ml-1m/users.dat")lines=userfile.readlines()for line in lines: line=line.strip("\n") print(line.split("::"))userfile.close(); 1234[&apos;1&apos;, &apos;F&apos;, &apos;1&apos;, &apos;10&apos;, &apos;48067&apos;][&apos;2&apos;, &apos;M&apos;, &apos;56&apos;, &apos;16&apos;, &apos;70072&apos;][&apos;3&apos;, &apos;M&apos;, &apos;25&apos;, &apos;15&apos;, &apos;55117&apos;].... 预处理： 数据集切分：将数据集随机切分为M份，取其中一份为测试集，剩下的M-1份为训练集。 代码： 123456789101112import randomfrom sklearn.model_selection import KFoldimport numpy as npM=8kf=KFold(n_splits=M,shuffle=True)trainIndexList=[]testIndexList=[]idxArr=[0,1,2,3,-3,-2,-1]for train_index,test_index in kf.split(lines): trainIndexList.append(train_index) testIndexList.append(test_index) print('train_index %s, test_index %s'%(train_index,test_index[idxArr])) 结果： 12345678train_index [ 0 1 2 ... 6037 6038 6039], test_index [ 3 12 13 22 6020 6028 6036]train_index [ 0 1 2 ... 6037 6038 6039], test_index [ 17 18 19 30 6011 6029 6032] train_index [ 1 2 3 ... 6037 6038 6039], test_index [ 0 4 34 35 6024 6025 6031]train_index [ 0 1 2 ... 6036 6038 6039], test_index [ 10 14 21 32 6027 6034 6037]train_index [ 0 1 2 ... 6036 6037 6038], test_index [ 5 7 11 15 6026 6033 6039]train_index [ 0 1 3 ... 6037 6038 6039], test_index [ 2 8 9 24 6006 6014 6035]train_index [ 0 2 3 ... 6037 6038 6039], test_index [ 1 6 28 31 6002 6017 6030]train_index [ 0 1 2 ... 6036 6037 6039], test_index [ 16 52 70 71 5991 6023 6038] 算法评测 对用户$u$推荐N个物品，(记为$R(u)$)，令用户$u$在测试集上喜欢的物品集合为$T(u)$ 召回率： Recall=\frac{\sum_{u\in U}|R(u)\cap T(u)|}{\sum_{u\in U} |T(u)|} 准确率： Precision=\frac{\sum_{u\in U}|R(u)\cap T(u)|}{\sum_{u\in U} |R(u)|} 覆盖率：所有用户的推荐结果并集的元素个数除以总物品数，用以说明推荐结果列表可以包含多大比例的物品（一个推荐系统不能总是推荐一小部分商品。对于该指标随机推荐可以达到100%，因为每个物品有均等机会被推荐） Coverage=\frac{|\cup_{u\in U}R(u)|}{|I|} 新颖度：推荐结果列表的平均流行度]]></content>
      <tags>
        <tag>RecSys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode667]]></title>
    <url>%2F2021%2F03%2F09%2FLeetcode%20667%2F</url>
    <content type="text"><![CDATA[667.优美的排列II被围绕的区域.题目描述：给定整数n和k，需要返回一个长度为n的数组，该数组的元素为1-n的一个排列。对于该数组需要满足条件：对于数组$[a_{1},a_{2}…a_{n}]$，那么有集合$S={|a_{k}-a_{k+1}| |k\in [1,n-1]}$且 |S|=k 。 举例：输入：n=15,k=3输出：[1,4,2,3,5,6,7,8,9,10,11,12,13,14,15] 分析：返回的答案序列由两部分组成 第一部分：偶数下标对应的序列$[1,2,…\frac{k}{2}]$，奇数下标对应的序列$[k+1,k,…\frac{k}{2}]$,例子中：偶数下标的数字序列为[1,2,3],奇数下标的数字序列为[4] 第二部分：[k+2,k+3,…,n] 边界计算：偶数下标对应的序列为递增序列，奇数下标序列为递减序列，求出二者的交点数值(在$\frac{k}{2}$附近)。 如果k%2==0,交点处附近的数字为$…,\frac{k}{2},\frac{k}{2} +2,\frac{k}{2} +1,k+2,k+3…$ 如果k%2==1,交点处附近的数字为$…,\frac{k+1}{2},\frac{k+1}{2} +1,k+2,k+3…$123456789101112131415161718192021222324int* constructArray(int n, int k, int* returnSize)&#123; *returnSize=n;//返回数组大小 int *res=(int *)malloc(n * sizeof(int));//分配空间 for(int i=0;i&lt;n;i++)res[i]=i+1; if(k==1)return res;//k为1直接返回 int endIdx=0;//Res序列前一段的末尾下标 int beginIdx=0;//Res序列后一段的开始下标 if(k%2==0)&#123; endIdx=2*(k/2-1); res[endIdx+2]=k/2+1; beginIdx=endIdx+3; &#125;else&#123; endIdx=2*((k+1)/2-1); beginIdx=endIdx+2; &#125; for(int i=0;i&lt;=endIdx;i+=2)&#123; res[i]=i/2+1; res[i+1]=k+1-(res[i]-1); &#125; for(int i=beginIdx;i&lt;n;i++)&#123; res[i]=k+2+(i-beginIdx); &#125; return res;&#125;]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TalkingPiano]]></title>
    <url>%2F2020%2F02%2F07%2FTalkingPiano%2F</url>
    <content type="text"><![CDATA[新型肺炎疫情,武汉加油 二.钢琴瀑布流 弹出汉字在线钢琴模拟莫得钢琴的程序员：声明一下 我没系统学习过Python,所以有比较浓的C/C++风格（语法全靠查一个记一个） 先介绍下核心思路（其实十分简单，从构思到实现1.0版本，可能也就一个小时不到的亚子）： 得到汉字点阵，转化成二维数组，得到图片像素点的信息，方便做按键映射（知道什么时候敲下哪个按键） 完成键盘的键值对（类似ASCII码和字母）数组的构建，模拟敲击键盘（要不然人的手可敲不来）键盘各键对应键值 上面网站钢琴按键的瀑布流，可以用按下的时间长度来控制其形状，若比较短促，则可以形成矩形像素点（也是实现思路的来源，1.0版本的小Bug也是因为此） 1.0版本：准备工作： 生成汉字图像，当然你下载一个汉字图片也是可以的，只是尺寸可能就不那么统一了，得做一些规范化操作之类的 #encoding: utf-8 这个轮子来自博客https://blog.csdn.net/johinieli/article/details/76151247 import os import pygame chinese_dir = &#39;chinese&#39;#存储图片的文件夹名字 if not os.path.exists(chinese_dir): os.mkdir(chinese_dir) pygame.init() def produceCharacter(codepoint): word = chr(codepoint) # 将Unicode的值转换成对应字符 unichr()用不了 了？？ font = pygame.font.Font(&quot;msyh.ttc&quot;, 64)#复制找到的微软雅黑字体文件到工程文件夹即可 rtext = font.render(word, True, (0, 0, 0), (255, 255, 255)) pygame.image.save(rtext, os.path.join(chinese_dir, word + &quot;.png&quot;)) #湖，北，加，油的Unicode 16进制码 codepoint=[0x6E56,0x5317,0x52A0,0x6CB9] for code in codepoint: produceCharacter(code) 转化成二维数组，这里用到了一个图片二值化的操作即：将图片转化为黑白图片，且是非黑即白，所以称为二值。黑：0 白：255 光亮度，这样可以丢弃颜色细节，方便映射到钢琴按键的按与不按两种状态这样就可以完成对应，判断某一个像素点的位置，是否有黑色像素点，有的话就使用瀑布流做拟合先来看一下所谓的二值化操作效果图： threshold = 200 #用于对汉字图片进行简单二值化的阈值设置，小于则认为是黑色，大于则认为是白色 imgfile=[&quot;湖.png&quot;,&quot;北.png&quot;,&quot;加.png&quot;,&quot;油.png&quot;] for k in range(0,len(imgfile)): img = Image.open(imgfile[k]).convert(&#39;L&#39;)#调用函数，汉字图片转成灰度图 a_img = np.asarray(img,dtype=&quot;int64&quot;).copy()#调用函数，得到灰度图的像素矩阵 for i in range(0, a_img.shape[0]):#得到灰度图的像素矩阵，实际上取值为0-255 for j in range(0, a_img.shape[1]): print(0 if a_img[i][j] &lt; threshold else 1, end=&quot;&quot;)#二值化打印，未作实际的二值化 print() plt.imshow(img, cmap=&#39;gray&#39;)#展示汉字灰度图 plt.axis(&#39;off&#39;)#去除plt展示图像时所画的坐标轴 plt.show() 键盘的键值对（类似ASCII码和字母）数组的构建，方便代码控制，模拟敲击网站的钢琴按键映射的方案，就随意了，反正最后可以一一对应到一个个像素点就好，核心还是用短促的瀑布流图像，来模拟像素点关键字搜索：键盘键值对，或者上面有个百度文库的网址另外一点：Python有自己的键盘事件库，但是我调用的时候出现了一些问题，未能解决，就没采用了 tune = np.array([ 192, 49, 50, 51, 52, 53, 54, 55, 56, 57, 48, 189, 187, 8, 81, 87, 69, 82, 84, 89, 85, 73, 79, 80, 219, 221, 220, 65, 83, 68, 70, 71, 72, 74, 75, 76, 186, 222, 90, 88, 67, 86, 66, 78, 77, 188, 190, 191, 32, 38, 37, 40, 39, 111, 106, 109, 103, 104, 105, 107, 100, 101, 102, 97, ], dtype=&quot;int64&quot;) 1.0版本代码遍历a_img二维数组，由于我们采取的是从下往上滚动汉字。所以进行自上而下，从左到右的遍历方式，外层循环按行遍历，内层按照列遍历（将屏幕倾斜90度，采用从左往右滚动汉字的话，类似） for i in range(0,a_img.shape[0]): for j in range(0,a_img.shape[1]): if a_img[i][j] &lt; threshold:#如果像素点为黑色，按下按键，由瀑布流产生‘像素点’ press(tune[j]) #end if #end for time.sleep(0.08)#按住一排需要按下的位置，持续一个较短时间 for j in range(0, a_img.shape[1]): if a_img[i][j] &lt; threshold:#松开刚才按下的所有按键 up(tune[j]) # end if # end for # end for time.sleep(1.5) 这里看一下运行的效果图： 可以看到，有一些缝隙，这就是我所说的1.0 的Bug 1.0bug修复-&gt;2.0版本 for i in range(0,a_img.shape[0]): for j in range(0,a_img.shape[1]): if a_img[i][j] &lt; threshold: press(tune[j]) #end if #end for time.sleep(0.08) if(i+1&lt;a_img.shape[0]):#判断下一行 for j in range(0, a_img.shape[1]): if a_img[i+1][j] &gt; threshold:#如果下一行该位置的值对应白色 up(tune[j])#则不需要像素点拟合，松开按键 # end if 否则该处的按键不松开，则会形成长条状的瀑布流图像 # end for # end if # end for bug出现的原因，猜测为，key_press和up函数需要较短时间来执行，当执行多次时，会有一定时间差，导致缝隙 完整代码 import matplotlib.pyplot as plt import numpy as np from PIL import Image import win32api import win32con import win32gui from ctypes import * import time def up(x): win32api.keybd_event(x, 0, win32con.KEYEVENTF_KEYUP, 0) def press(x): win32api.keybd_event(x, 0, 0, 0) tune = np.array([ 192, 49, 50, 51, 52, 53, 54, 55, 56, 57, 48, 189, 187, 8, 81, 87, 69, 82, 84, 89, 85, 73, 79, 80, 219, 221, 220, 65, 83, 68, 70, 71, 72, 74, 75, 76, 186, 222, 90, 88, 67, 86, 66, 78, 77, 188, 190, 191, 32, 38, 37, 40, 39, 111, 106, 109, 103, 104, 105, 107, 100, 101, 102, 97, ], dtype=&quot;int64&quot;) time.sleep(2) threshold = 200 imgfile=[&quot;湖.png&quot;,&quot;北.png&quot;,&quot;加.png&quot;,&quot;油.png&quot;] for k in range(0,len(imgfile)): img = Image.open(imgfile[k]).convert(&#39;L&#39;) a_img = np.asarray(img,dtype=&quot;int64&quot;).copy() for i in range(0, a_img.shape[0]): for j in range(0, a_img.shape[1]): print(0 if a_img[i][j] &lt; threshold else 1, end=&quot;&quot;) print() plt.imshow(img, cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) plt.show() for i in range(0,a_img.shape[0]): for j in range(0,a_img.shape[1]): if a_img[i][j] &lt; threshold: press(tune[j]) #end if #end for time.sleep(0.08) if(i+1&lt;a_img.shape[0]):#判断下一行 for j in range(0, a_img.shape[1]): if a_img[i+1][j] &gt; threshold: up(tune[j]) # end if # end for # end if # end for time.sleep(150)#这里150s只是为了方便看一个汉字的运行情况,实际上两个汉字间隔1.5s左右即可 #end for &#39;&#39;&#39; 图像打印操作 和 键盘键值对 for i in range(0, a_img.shape[0]): for j in range(0, a_img.shape[1]): print(a_img\[i][j],end=&quot;&quot;) print() plt.imshow(img, cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) plt.show() VK_CODE = { &#39;backspace&#39;: 0x08, &#39;tab&#39;: 0x09, &#39;clear&#39;: 0x0C, &#39;enter&#39;: 0x0D, &#39;shift&#39;: 0x10, &#39;ctrl&#39;: 0x11, &#39;alt&#39;: 0x12, &#39;pause&#39;: 0x13, &#39;caps_lock&#39;: 0x14, &#39;esc&#39;: 0x1B, &#39;spacebar&#39;: 0x20, &#39;page_up&#39;: 0x21, &#39;page_down&#39;: 0x22, &#39;end&#39;: 0x23, &#39;home&#39;: 0x24, &#39;left_arrow&#39;: 0x25, &#39;up_arrow&#39;: 0x26, &#39;right_arrow&#39;: 0x27, &#39;down_arrow&#39;: 0x28, &#39;select&#39;: 0x29, &#39;print&#39;: 0x2A, &#39;execute&#39;: 0x2B, &#39;print_screen&#39;: 0x2C, &#39;ins&#39;: 0x2D, &#39;del&#39;: 0x2E, &#39;help&#39;: 0x2F, &#39;0&#39;: 0x30, &#39;1&#39;: 0x31, &#39;2&#39;: 0x32, &#39;3&#39;: 0x33, &#39;4&#39;: 0x34, &#39;5&#39;: 0x35, &#39;6&#39;: 0x36, &#39;7&#39;: 0x37, &#39;8&#39;: 0x38, &#39;9&#39;: 0x39, &#39;a&#39;: 0x41, &#39;b&#39;: 0x42, &#39;c&#39;: 0x43, &#39;d&#39;: 0x44, &#39;e&#39;: 0x45, &#39;f&#39;: 0x46, &#39;g&#39;: 0x47, &#39;h&#39;: 0x48, &#39;i&#39;: 0x49, &#39;j&#39;: 0x4A, &#39;k&#39;: 0x4B, &#39;l&#39;: 0x4C, &#39;m&#39;: 0x4D, &#39;n&#39;: 0x4E, &#39;o&#39;: 0x4F, &#39;p&#39;: 0x50, &#39;q&#39;: 0x51, &#39;r&#39;: 0x52, &#39;s&#39;: 0x53, &#39;t&#39;: 0x54, &#39;u&#39;: 0x55, &#39;v&#39;: 0x56, &#39;w&#39;: 0x57, &#39;x&#39;: 0x58, &#39;y&#39;: 0x59, &#39;z&#39;: 0x5A, &#39;numpad_0&#39;: 0x60, &#39;numpad_1&#39;: 0x61, &#39;numpad_2&#39;: 0x62, &#39;numpad_3&#39;: 0x63, &#39;numpad_4&#39;: 0x64, &#39;numpad_5&#39;: 0x65, &#39;numpad_6&#39;: 0x66, &#39;numpad_7&#39;: 0x67, &#39;numpad_8&#39;: 0x68, &#39;numpad_9&#39;: 0x69, &#39;multiply_key&#39;: 0x6A, &#39;add_key&#39;: 0x6B, &#39;separator_key&#39;: 0x6C, &#39;subtract_key&#39;: 0x6D, &#39;decimal_key&#39;: 0x6E, &#39;divide_key&#39;: 0x6F, &#39;F1&#39;: 0x70, &#39;F2&#39;: 0x71, &#39;F3&#39;: 0x72, &#39;F4&#39;: 0x73, &#39;F5&#39;: 0x74, &#39;F6&#39;: 0x75, &#39;F7&#39;: 0x76, &#39;F8&#39;: 0x77, &#39;F9&#39;: 0x78, &#39;F10&#39;: 0x79, &#39;F11&#39;: 0x7A, &#39;F12&#39;: 0x7B, &#39;F13&#39;: 0x7C, &#39;F14&#39;: 0x7D, &#39;F15&#39;: 0x7E, &#39;F16&#39;: 0x7F, &#39;F17&#39;: 0x80, &#39;F18&#39;: 0x81, &#39;F19&#39;: 0x82, &#39;F20&#39;: 0x83, &#39;F21&#39;: 0x84, &#39;F22&#39;: 0x85, &#39;F23&#39;: 0x86, &#39;F24&#39;: 0x87, &#39;num_lock&#39;: 0x90, &#39;scroll_lock&#39;: 0x91, &#39;left_shift&#39;: 0xA0, &#39;right_shift &#39;: 0xA1, &#39;left_control&#39;: 0xA2, &#39;right_control&#39;: 0xA3, &#39;left_menu&#39;: 0xA4, &#39;right_menu&#39;: 0xA5, &#39;browser_back&#39;: 0xA6, &#39;browser_forward&#39;: 0xA7, &#39;browser_refresh&#39;: 0xA8, &#39;browser_stop&#39;: 0xA9, &#39;browser_search&#39;: 0xAA, &#39;browser_favorites&#39;: 0xAB, &#39;browser_start_and_home&#39;: 0xAC, &#39;volume_mute&#39;: 0xAD, &#39;volume_Down&#39;: 0xAE, &#39;volume_up&#39;: 0xAF, &#39;next_track&#39;: 0xB0, &#39;previous_track&#39;: 0xB1, &#39;stop_media&#39;: 0xB2, &#39;play/pause_media&#39;: 0xB3, &#39;start_mail&#39;: 0xB4, &#39;select_media&#39;: 0xB5, &#39;start_application_1&#39;: 0xB6, &#39;start_application_2&#39;: 0xB7, &#39;attn_key&#39;: 0xF6, &#39;crsel_key&#39;: 0xF7, &#39;exsel_key&#39;: 0xF8, &#39;play_key&#39;: 0xFA, &#39;zoom_key&#39;: 0xFB, &#39;clear_key&#39;: 0xFE, &#39;+&#39;: 0xBB, &#39;,&#39;: 0xBC, &#39;-&#39;: 0xBD, &#39;.&#39;: 0xBE, &#39;/&#39;: 0xBF, &#39;`&#39;: 0xC0, &#39;;&#39;: 0xBA, &#39;[&#39;: 0xDB, &#39;\\&#39;: 0xDC, &#39;]&#39;: 0xDD, &quot;&#39;&quot;: 0xDE, &#39;`&#39;: 0xC0} &#39;&#39;&#39;]]></content>
      <tags>
        <tag>WHIMSY</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recursion_Divide_and_conquer]]></title>
    <url>%2F2020%2F01%2F08%2FRecursion-Divide-and-conquer%2F</url>
    <content type="text"><![CDATA[递归与分治策略分割问题，缩小规模，原问题与子问题的解法的 二分搜索技术 大整数乘法 Strassen矩阵乘法 棋盘覆盖 合并线性和快速排序 线性时间选择 最接近点对问题 循环赛日程问题 入门：阶乘函数 斐波那契数列]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SortingAlgorithms]]></title>
    <url>%2F2019%2F12%2F29%2FSortingAlgorithms%2F</url>
    <content type="text"><![CDATA[比较类排序：冒泡排序、选择排序、快速排序、插入排序、希尔排序、归并排序、堆排序分配式排序：基数排序，计 是否基于比较、稳定性、是否需要额外空间数排序，桶排序 算法的基本思想（basic idea) 对排序算法执行的每一步追踪（trace）， 复杂度分析（time/space complexity） 伪代码（pseudocode） 一、冒泡排序从后向前扫描，遇到逆序的就做交换，例如42 20 17 13 28 14 23 15从后向前扫描：（假设升序排序）第一趟：1234523 与 15 逆序 swap-&gt;42 20 17 13 28 14 15 2328 与 14 逆序 swap-&gt;42 20 17 13 14 28 15 2317 与 13 逆序 swap-&gt;42 20 13 17 14 28 15 2320 与 13 逆序 swap-&gt;42 13 20 17 14 28 15 2342 与 13 逆序 swap-&gt;13 42 20 17 14 28 15 23 可以发现这样最小值被放到了序列首部 int n; void bubSort(int *A) { for(int i=0;i&lt;n-1;i++) { for(int j=n-1;j&gt;i;j--) { if(A[j-1]&gt;A[j]) swap(A[j-1],A[j]); } } } 二、选择排序直接选择排序还是将待排序列分为无序区和有序区两块区域，一开始，待排序列为无序区，则从序列尾部向前扫描，找到本次扫描中的遇见的最小值，将其与首个元素交换位置，则无序序列被划分为有序区（最小值）+无序区 这时，就得到了一个新的size为n-1的无序序列（无序区），再对这n-1个元素做上述操作，找到这n-1个元素里的最小值，交换至新无序区(size=n-1的新无序序列)的首部，这样从整体来看，原size=n的待排序列被划分为， size=2：有序区 size=n-2：无序区 int n; void SelSort(int *A) { for(int i=0;i&lt;=n-2;i++)//无序区间的最前端位置 { int minIndex=0; int minVal=2147483647; for(int j=n-1;j&gt;=i;j--)//扫描无序区间元素 { if(minVal&gt;A[j]) { minIndex=j; minVal=A[j]; } } swap(A[minIndex],A[i]); } } 优化选择排序也就是在找寻最小值的过程中顺便也把最大值找出来，这样每一趟排序就可以把当前待排序列中的最大值和最小值放到合适的位置（表尾和表头），代码就不写了，就在上面找最小值的部分下面再加一个做比较找最大值的代码块就行 三、快速排序在介绍快速排序之前，先简单介绍一下BST(二叉搜索树)的一些特性和实现思路作类比BST：在Shaffer的数据结构与算法中，二叉搜索树的左子树中的所有节点的关键码值（理解为Key值，就是一个用于比较的值，例如学生的成绩）小于根节点的关键码值，而右子树的所有节点的关键码值都大于或者等于其根节点的关键码值，这就有个好处了，当你寻找某个值的时候每次都可以把范围缩小一半去找，好比有序序列的区间二分查找 那设想有一个算法，能根据指定的某一位置（枢轴）把一无序序列分为左右两边，其中，左边的区域比枢轴值都要小右边区域的值比枢轴都要大（大于等于…），那么枢轴就处在了他该待在的位置，也就完成了一个元素的位置确定，然后再递归的对左区域和右区域使用这个算法，最后就可以得到有序序列了。 快速排序代码实现：按照自己的理解写的代码有些问题，目前还没想出错误，先放上来留待以后修改，qsort2是对的 # 代码分解讲解：template&lt;typename E&gt; inline int findpivot(E A[],int i,int j){return (i+j)/2;} //产生区间的中值作为枢轴 template&lt;typename E&gt; inline int partition_2(E A[],int l,int r,E pivot) { do{ while(A[++l]&lt;pivot); while((l&lt;r)&amp;&amp;pivot&lt;A[--r]); swap(A[l],A[r]); }while(l&lt;r); return l; } 对于给定的 [l,r] 下标区间的元素，根据上面的描述，枢轴左边的元素应当都比枢轴小，右边的都要比枢轴元素大，那么使用left指针从左边扫描，遇见比枢轴大的元素就停在这，使用right指针从右向左扫描，遇见比枢轴小的元素就停在这，这样left和right分别找到了两个不符合要求的元素（左边的元素都比枢轴元素小，右边的都要比枢轴元素大），那么交换left和right指针所指向的元素，就可以使之满足要求，像这样扫描下下去，直到left与right指针相遇，或者left指针跑到了right的右边的时候，就说明已经完成了任务：左边的区域比枢轴值都要小，右边区域的值比枢轴都要大 #include&lt;iostream&gt; using namespace std; int n; template&lt;typename E&gt; inline int findpivot(E A[],int i,int j){return (i+j)/2;} //产生区间的中值作为枢轴 template&lt;typename E&gt; int partition_(E A[],int l,int r,E PIVOT) { do{ while(1) { if((l&lt;r)&amp;&amp;A[l]&lt;A[PIVOT]) { l++; } else break; } while(1) { if((l&lt;r)&amp;&amp;A[PIVOT]&lt;A[r]) r--; else break; } //while((l&lt;r)&amp;&amp;A[l]&lt;pivot) l++; //while((l&lt;r)&amp;&amp;pivot&lt;A[r])r--; swap(A[l],A[r]); }while(l&lt;r); return l; } template&lt;typename E&gt; inline int partition_2(E A[],int l,int r,E pivot) { do{ while(A[++l]&lt;pivot); while((l&lt;r)&amp;&amp;pivot&lt;A[--r]); swap(A[l],A[r]); }while(l&lt;r); return l; } /* 10 72 6 57 88 60 42 83 73 48 85 */ template&lt;typename E&gt; void quickSort(E A[],int i,int j) { if(i&gt;=j)return ;//0或者1个元素不需要排序 int pivot=findpivot(A,i,j); swap(A[pivot],A[j]);//轴值置于序列末尾 int k=partition_(A,i,j-1,j); swap(A[k],A[j]);//轴值归位 for(int i=0;i&lt;n;i++) { cout&lt;&lt;A[i]&lt;&lt;&quot; &quot;; } cout&lt;&lt;endl; quickSort(A,i,k-1); quickSort(A,k+1,j); } template&lt;typename E&gt; void quickSort2(E A[],int i,int j) { if(i&gt;=j)return ;//0或者1个元素不需要排序 int pivot=findpivot(A,i,j); swap(A[pivot],A[j]);//轴值置于序列末尾 int k=partition_2(A,i-1,j,A[j]); swap(A[k],A[j]);//轴值归位 quickSort2(A,i,k-1); quickSort2(A,k+1,j); } int main() { cin&gt;&gt;n; int *A=new int[n]; for(int i=0;i&lt;n;i++) { cin&gt;&gt;A[i]; } quickSort2&lt;int&gt;(A,0,n-1); for(int i=0;i&lt;n;i++) { cout&lt;&lt;A[i]&lt;&lt;&quot; &quot;; } cout&lt;&lt;endl; return 0; } 四、插入排序想象一下，一堆打乱的牌放在桌子上（写有0~26的总共27张卡片），怎么才能变成有序呢，可以这样，左手拿牌，右手去一张张抽牌，右手拿第一张，将它直接放在左手里（一个元素自身有序）再去拿第二张，如果比第一张大，放在最右边，否则放在左边。拿第三张，这个时候左手里有两张牌了，右手拿着这张新牌，悬空着，从左手的最右边开始向左移动，找到合适位置插入到左手中（比如左手里的是1,5，拿到的是3，那么就插在1,5之间-&gt;1,3,5），这个合适的位置很显然就是，移动找位置过程中遇见的，第一个左相邻元素小于等于待插入元素的位置下面以n比较大的情况说明： 假如左手中的有序牌为：0 5 7 13 20 ，待插入牌为8号牌 则：0 5 7 13 20 8 -&gt;0 5 7 13 8 20 -&gt;0 5 7 8 13 20 找到合适位置，结束123456789101112131415void insort(int *A)&#123; for(int i=1;i&lt;n;i++)//右手取牌循环 &#123;//i=1开始，因为左手里初始就拿了一张A[0]，所以右手从A[1]开始取牌向左手里插 for(int j=i;j&gt;=1;j--)//待插入牌寻找合适位置的循环 &#123; if(A[j-1]&gt;A[j]) &#123; swap(A[j-1],A[j]);//拿着右手的牌在悬在左手牌的上方 //在寻找合适插入位置的过程中，如果当前不合适，就向左交换 //可以自己再想一想右手牌寻找合适位置的情形 &#125;else break; &#125; &#125;&#125; 五、希尔排序改进版的插入排序，采用最小增量进行间断式的分组插入排序，每一组插入排序处理的数据规模减小代码解释以注释形式给出（略显简单^_^）12345678910111213141516171819202122232425void insort2(int *A,int incr)&#123; for(int i=incr;i&lt;n;i+=incr)//如果对插入排序有一个比较深刻的思考，不难知道 &#123;//修改后的插入排序，不过是把i++ i-- 这样的法则改为了i+=incr,i-=incr，从而实现了 //物理空间上的分隔，逻辑上的连续（逻辑上 i,i+incr，i+2*incr应当是连续的，同属于一个子序列） for(int j=i;j&gt;=incr;j-=incr) &#123; if(A[j-incr]&gt;A[j])//类比A[j-1] A[j] 的比较 swap(A[j-incr],A[j]); else break; &#125; &#125;&#125;void ShellSort(int *A)&#123; for(int i=n/3;i&gt;2;i/=3)//用于增量生成的循环 i即为增量 &#123; for(int j=0;j&lt;i;j++) insort2(&amp;A[j],i);//这个函数的传参比较细节，传的是A[j]元素的地址 &#125; insort2(A,1);//做一次收尾的普通插入排序，但此时数据大致有序了&#125; for(int j=0;j&lt;i;j++)insort2(&amp;A[j],i);//这个函数的传参比较细节，传的是A[j]元素的地址 这一个循环单独讲一下，当选定一个增量来划分数据序列的时候，设增量为d，那么序列被划分为这么几组：12345A[0] A[d] A[2*d] ... A[]A[1] A[1+d] A[1+2*d] ... A[]......A[d-1] A[d-1+d] A[d-1+2*d] ... A[] 可以知道，每个子序列的起始端为0,1,2…d-1，且每个子序列的元素不相交（因为A[d]是A[0]开头的那一组的第二个元素，所以A[d-1]是最后一组子序列的开头） 所以，调用insort2函数传的是A[j]元素的地址，也就是每一组子序列的开头元素的地址 六、归并排序算法思想分治法的思想，逐层向下分解（这里做2路归并，向下做二分减小待排数据规模），当分解到规模为1或者2时，就可以很简单的给出当前规模下的有序序列了（2个可以做交换，1个本身就有序） 原序列-&gt;不断2分-&gt;数据量很小的序列直接排序-&gt;向上做合并-&gt;有序序列 代码实现分步骤解释递归出口void MergeSort(int b,int e/*begin and end index of array*/) { if(b==e) return; if(b+1==e){ if(c[b]&gt;c[e]) swap(c[b],c[e]); return ; } //这一块是对最下层的小序列做一个排序，使其有序，是递归的出口 递归分解 int mid = (b-e)/2+e;//防止大数之和超过范围 MergeSort(b,mid); MergeSort(mid+1,e);//二分数据元素序列，减小排序规模 合并两有序子序列分解完序列，并将小规模数据排完序后，向上做合并操作 合并思想对于待合并的两个有序子序列而言，可以开辟（最好全局声明吧）一个tempc[]数组存放中间结果，每一次比较量有序子序列的首部元素，选取两者中最小的（这里默认排序是按照升序方式进行）元素，放进tempc[]数组中，重复操作，得到即得到合并之后有序的数组，再将其拷贝回原数组c[] 合并步骤 int tempi=b,tempj=mid+1; for(int i=b;i&lt;=e;i++) { if(tempi&lt;=mid&amp;&amp;tempj&lt;=e) {//选取两有序子序列各自的最小值，再从二者里面选最小值 if(c[tempi]&lt;c[tempj]) { tempc[i]=c[tempi]; tempi++; }else{ tempc[i]=c[tempj]; tempj++; } }else{//其中一个子序列中的元素已被比较完，只需复制另一序列中的剩余元素即可 if(tempi&gt;mid) { while(tempj&lt;=e) { tempc[i]=c[tempj]; tempj++; i++; } break; }else{ while(tempi&lt;=mid) { tempc[i]=c[tempi]; tempi++; i++; } break; } } } for(int i=b;i&lt;=e;i++) { c[i]=tempc[i]; } 归并排序的循环实现12```# 堆排序 ``` 计数排序假设输入的数据范围是25 ~ 552，则数据的最大值为MaxValue=552那就开一个计数数组Num,大小为553，这样Num数组能利用的下标范围为0-552,且Num数组元素初始化为0然后输入数据input,那么Num[input]++;用数组下标来进行分配，从而避免了排序，因为数组下标本身就是顺序排列的 #include&lt;iostream&gt; #include&lt;cstring&gt; using namespace std; int main() { int n; cin&gt;&gt;n; int maxV; int *A=new int[n]; for(int i=0;i&lt;n;i++) { cin&gt;&gt;A[i]; } maxV=A[0]; for(int i=1;i&lt;n;i++) { if(A[i]&gt;maxV) { maxV=A[i]; } } int *num=new int[maxV+1]; memset(num,0,(maxV+1)*sizeof(int)); for(int i=0;i&lt;n;i++) num[A[i]]++; for(int i=0;i&lt;maxV+1;i++) { if(num[i]&gt;0) { for(int j=0;j&lt;num[i];j++) cout&lt;&lt;i&lt;&lt;&#39; &#39;; } } return 0; } 桶排序使用某些规则划分出不同的桶，根据规则将输入数据分配到相应的桶中，然后每个桶中的数据规模减小，再使用其他排序方法在桶内部进行排序，在收集各桶中的元素即为排序后的结果， 基数排序]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tree]]></title>
    <url>%2F2019%2F12%2F28%2FTree%2F</url>
    <content type="text"><![CDATA[树父指针表示法每个节点只保存一个指针域，指向其父节点 等价类这里直接给出（离散数学中有介绍。。。），等价关系具有自反性、对称性、可传递性。举个简单的例子来理解一下，如果由A、B、C三座城市，给出等价对(A,B)(B,C)，则A、B双向通车，B、C双向通车，则可以认为A、C双向通车，称A、C等价，A、C应当归并为1个集合中（这里的关系是两城市是否连通，当然也可以是A是否是B的朋友这样的表达，这就是关系的两个例子）， 并查集 判断两个结点是否在同一集合中 归并两个集合 两个集合的合并常常被称为“并”UNION，合并的目的是为了之后方便的查询两结点是否在同一集合中。 主要实现Union Find Differ 三个函数来完成并查集 Differ:用于判断给出等价对中两结点是否已经属于同一集合，如果已经处于同一集合，则显然不需要使用合并操作（使用Find操作） Union合并两个集合（分为未使用加权合并和使用加权合并两种） Find:寻找到当前结点所在树的根节点（分为未使用路径压缩和使用路径压缩两个版本） 以下是本人根据对算法思想理解自己写的代码，有错误的话见谅（虽说目前测试没问题，毕竟自己的代码才有灵魂吗O(∩_∩)O~）。。。分解讲解一下思路： 加权合并当两个集合之间要做合并的时候，将元素少的集合向元素多的集合去合并，优点分析：这里的集合其实是一棵棵树，当合并两棵树的时候，如果将节点数少的树向节点数多的树合并，可以使得合并后节点数是至少是节点数少的树的两倍（这是显然的max &gt; min ; max + min&gt;=2*min）,但是高度最多只比合并之前的最大深度多1（试想一下，min和max的树的高度相同皆为h，那合并之后也不过h+1,至于其他的情况也可以自己画一画：高但少的树和矮但多的树合并，高但多的树和矮但少的树合并，合并之后也仍不过h_{MAX}+1） 路径压缩如果D的父节点是C,C的父节点是B，B的父节点是根节点A，那么想一想，既然大家的FIND操作都是为了找到根节点A，为什么不把D,C都像B直接指向A呢，这样以后执行Find操作的时候查找次数将大幅降低，因而在寻找D的根节点的时候，就把这一条路径上的所有节点指向根节点，从而缩短了Find路径 根节点标志ROOT = 2147483647 作为区分，节点是否为根节点的标志 主要物理存储结构uset数组：并查集节点数组，下标代表节点序号，数组元素存储的值代表其父节点序号，从而实现父指针表示法num数组：用于加权合并规则的实现时，方便得知两个待合并集合各集合所拥有的元素个数，以此为依据，进行两集合之间的加权合并 主要操作函数Find1（未使用路径压缩）根据父指针一路向上寻找 Find2（使用路径压缩）根据父指针一路向上寻找，递归寻找，这样当最后一层找到根节点返回的时候，可以将整条路径上的节点的父节点设置为根节点 differ使用Find函数，查找节点的根节点，从而判断是否属于同一集合 Union（使用加权合并规则）根据num数组中存储的集合元素数目进行比较，进行加权合并，至于合并操作非常简单，只需要Find得到两个待合并集合各自的根节点，将元素多的集合的根节点作为新根节点，设置为另一集合根节点的父节点即可。 应用举例曾经写过这样一个题目，森林中有一群猴子，一开始互相不认识，如果见面就会打架，打完架就成了好朋友（不打不相识…），且好朋友这种关系是一个等价关系，上面介绍过了，这里再阐述一下（好朋友关系是可以传递的：A是B的好朋友，B是C的好朋友，那么A是C的好朋友；对称的：A是B的好朋友，那么B也是A的好朋友）这样又给出一个好朋友圈的概念，会打架的两只猴子一定不属于同一个好朋友圈，并且如果他们将要打架，他们会请出自己好朋友圈里最强壮的朋友来帮他们打架，打架之后，两个好朋友圈将会合并。 显然，题目就是一个典型的并查集题目，题解就不放了。 # include&lt;iostream&gt; using namespace std; # define ROOT 2147483647 int *uset,*num; int Find1(int curr) { while(uset[curr]!=ROOT) curr=uset[curr]; return curr; } int Find2(int curr) { if(uset[curr]==ROOT)return curr; uset[curr]=Find2(uset[curr]); return uset[curr]; } bool differ(int a,int b,int(*Find)(int curr)) { if(Find(a)==Find(b)) return false; else return true; } void Union(int a,int b,int(*Find)(int curr)) { int RootOfa=Find(a); int RootOfb=Find(b); if(num[RootOfa]&lt;=num[RootOfb]) { uset[RootOfa]=RootOfb; num[RootOfb]+=num[RootOfa]; }else { uset[RootOfb]=RootOfa; num[RootOfa]+=num[RootOfb]; } } int main() { int n; cin&gt;&gt;n; uset=new int[n]; num=new int[n]; for(int i=0;i&lt;n;i++) { uset[i]=ROOT; num[i]=1; } cout&lt;&lt;&quot;输入等价对数量&quot;; int m; cin&gt;&gt;m; for(int i=0;i&lt;m;i++) { int a,b; cin&gt;&gt;a&gt;&gt;b; if(differ(a,b,Find2)) { Union(a,b,Find2); } } for(int i=0;i&lt;n;i++) cout&lt;&lt;i&lt;&lt;&quot; &quot;&lt;&lt;uset[i]&lt;&lt;endl; return 0; } 以下的表示法在床上敲的，先不放图了，等我以后再回来放图，放图是需要鼠标的。。。。。。 树的表示法子节点表-表示法节点数组+子节点链表什么意思呢？就是说有一个数组，他的元素是树中所有结点，但是呢，同时每一个数组元素也是一个父节点，作为一个叫子节点链表的头节点 头节点组成元素： 自身的value值 指向父节点的索引值 指向子节点链表的指针（指向的其实是最左子节点） 子节点链表中的节点元素 自身的value值 指向右兄弟节点的指针 最左子节点/右兄弟-表示法每一个节点，有这么几个组成元素 自身的value值 指向父节点的索引 指向自己最左子节点的索引 指向自己的右兄弟的索引 动态节点-表示法子节点数目固定 子节点数+相应的子结点指针 子节点数目不固定 子节点指针链表 树的顺序表示法先根序列表示法把树的结点按照先根遍历顺序列出，把所有非空节点看成分支节点，只有空指针 NULL被当作叶子节点AB/D//CEG///FH//I//然后来重构一下这棵树A root节点B A的左子节点 / 说明B无左子节点，B的左子树遍历完毕 紧随其后的是B的右子树，D说明D是B的右子节点，]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LuoguP1908]]></title>
    <url>%2F2019%2F12%2F18%2FLuoguP1908%2F</url>
    <content type="text"><![CDATA[题目描述猫猫TOM和小老鼠JERRY最近又较量上了，但是毕竟都是成年人，他们已经不喜欢再玩那种你追我赶的游戏，现在他们喜欢玩统计。最近，TOM老猫查阅到一个人类称之为“逆序对”的东西，这东西是这样定义的：对于给定的一段正整数序列，逆序对就是序列中ai&gt;aj且i &lt; j的有序对。知道这概念后，他们就比赛谁先算出给定的一段正整数序列中逆序对的数目。 总之就是计算逆序对数目 暴力法 归并排序 树状数组 暴力法不讲了，就是枚举，平方复杂度 归并排序#include&lt;iostream&gt; #include&lt;algorithm&gt; #include&lt;cmath&gt; #include&lt;cstdlib&gt; using namespace std; int c[500000+10]; int tempc[500000+10]; int ans=0; void MergeSort(int b,int e)//begin and end index of array) { if(b==e) return ; if(b+1==e){ if(c[b]&gt;c[e]) { swap(c[b],c[e]); ans+=1; } return; } int mid = (b-e)/2+e; MergeSort(b,mid); MergeSort(mid+1,e); int tempi=b,tempj=mid+1; for(int i=b;i&lt;=e;i++) { if(tempi&lt;=mid&amp;&amp;tempj&lt;=e) { if(c[tempi]&lt;c[tempj]) { tempc[i]=c[tempi]; tempi++; }else{ while(tempi&lt;=mid) { if(c[tempi]==c[tempj]) { tempc[i]=c[tempi]; i++; tempi++; } else{ ans+=mid-tempi+1; break; } } tempc[i]=c[tempj]; tempj++; } }else{ if(tempi&gt;mid) { while(tempj&lt;=e) { tempc[i]=c[tempj]; tempj++; i++; } break; }else{ while(tempi&lt;=mid) { tempc[i]=c[tempi]; tempi++; i++; } break; } } } for(int i=b;i&lt;=e;i++) { c[i]=tempc[i]; } } int main() { ans=0; int n; cin&gt;&gt;n; for(int i=0;i&lt;n;i++) { cin&gt;&gt;c[i]; //c[i]=n-i; } MergeSort(0,n-1); cout&lt;&lt;ans; return 0; } 树状数组]]></content>
      <tags>
        <tag>Luogu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamicProgramming]]></title>
    <url>%2F2019%2F12%2F17%2FDynamicProgramming%2F</url>
    <content type="text"><![CDATA[【动态规划】 01背包问题：有n个物品，它们有各自的体积和价值，现有给定容量的背包，如何让背包里装入的物品具有最大的价值总和 给出一个例子作为讲解：物品数目number＝4，背包容量capacity＝8 i（物品编号） w（体积） v（价值） 1 2 3 2 3 4 3 4 5 4 5 6 这里就有必要引入：决策、最优解、状态、状态转移方程了，但并不线给出抽象的定义，而是用通俗一点的方式理解 假如我们现在的背包是空的，剩余容量c为8，开始从一号物品往后装（顺序去拿），那么对于某一个物品而言，就有拿与不拿两种处理方式（这里就是决策，做出的判断与行动），然后相应的有两种分支， 把第一件物品装进包，那么包的剩余容量会减小，但是包里的总价值会增加 不装第一件物品，那么包的容量和其内部物品的价值保持不变，都是0这里包就有了两种属性，容量和价值（即包的状态）每一次的决策，会带来状态之间的变化，可以找出一定的关系式（状态转移方程，方便计算机进行迭代） 1号决策-&gt;2号决策-&gt;3号决策-&gt;4号决策 1号最优解=全局最优解=最优（1号决策产生的所有分支） 换句话说，1号决策产生了两条路径，答案总会出现在两条路径中的其中一条举个例子理解一下：capacity=8,value=0对于第一个物品，两种决策（背包容量比较大）这里不妨引入一个初态state0:capacity=8,value=0 拿： capacity=6,value=3 state0-&gt;state1 不拿：capacity=8,value=0 state0-&gt;state2那么，你再分别基于这两个状态的背包，进行二号决策，又会产生四个状态（如果背包空间还是比较大的话）但是先到这里，最优解的产生一定是一系列决策的共同作用，也就是有一个最优决策序列。那么再来重复下上面那句话：1号最优解=全局最优解=最优（1号决策产生的所有分支，这里的例子是2个） 那么当我们做2号决策的时候，可以看见的是背包有两个状态，又或者说，要基于这两个状态的背包分别做决策state1:那么其实 state1 就是一个新的state0了state2:同理state2 也是一个新的state0 这样,2号决策就拥有了与1号决策相似的结构了，它的最优解称为子最优解那么来一个不正式的理解：全局最优解=最优(子最优解)子最优解=最优（子子最优解）…其实呢，动态规划，个人是觉得听不太好阐述清楚，有时间对比一下其他的一些思想和方法一起理解 洛谷P1060 开心的金明状态转移方程 opt[N_{i}][i]=\begin{Bmatrix} max(opt[N_{i}-n_{i}][i+1]+v_{i}\cdot p_{i},opt[N_{i}][i+1]),N_{i}\geqslant n_{i}\\\\ opt[N_{i}][i+1],N_{i}< n_{i} \end{Bmatrix} 递归实现1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;int opt[3000][27];int v[27+2],p[27+2];int N,m;int dp(int Ni,int i/*id*/)&#123; if(i==m)return Ni&gt;=v[m]?v[m]*p[m]:0; return Ni&gt;=v[i]?/*背包容量比较充裕，可做两种决策*/ max( dp(Ni-v[i],i+1)+v[i]*p[i] , dp(Ni,i+1)) :/*背包容量不充裕，不能放当前物品*/ dp(Ni,i+1);&#125;int main()&#123; cin&gt;&gt;N&gt;&gt;m; for(int i=0;i&lt;m;i++) &#123; cin&gt;&gt;v[i+1]&gt;&gt;p[i+1]; //id=i+1 &#125; cout&lt;&lt;dp(N,1); return 0;&#125; 记忆化搜索​ 123456789101112131415161718192021222324252627282930313233343536373839 #include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;int opt[3000][27];int DP[3000][27];int v[27+2],p[27+2];int N,m;int dp(int Ni,int i/*id*/)&#123; if(i==m) return Ni&gt;=v[m]?v[m]*p[m]:0; if(DP[Ni-v[i]][i+1]==-1) DP[Ni-v[i]][i+1]=dp(Ni-v[i],i+1); if(DP[Ni][i+1]==-1) DP[Ni][i+1]=dp(Ni,i+1); return Ni&gt;=v[i]?/*背包容量比较充裕，可做两种决策*/ max( DP[Ni-v[i]][i+1]+v[i]*p[i] , DP[Ni][i+1]) :/*背包容量不充裕，不能放当前物品*/ DP[Ni][i+1];&#125;int main()&#123; cin&gt;&gt;N&gt;&gt;m; for(int i=0;i&lt;m;i++) &#123; cin&gt;&gt;v[i+1]&gt;&gt;p[i+1]; //id=i+1 &#125; memset(DP,-1,3000*27*sizeof(int)) ; cout&lt;&lt;dp(N,1); return 0;&#125; 循环实现123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;int opt[3000][27+2];int v[27+2],p[27+2];int main()&#123; int N,m; cin&gt;&gt;N&gt;&gt;m; for(int i=0;i&lt;m;i++) &#123; cin&gt;&gt;v[i+1]&gt;&gt;p[i+1]; //id=i+1 &#125; for(int i=0;i&lt;=N;i++) &#123; opt[i][m]= i&gt;=v[m]?v[m]*p[m]:0;//最后一步的决策是可以直接给出的 //初始化 &#125; for(int i=m-1;i&gt;=1;i--) &#123; for(int j=0;j&lt;=N;j++) &#123; opt[j][i]= j&gt;=v[i]? max(opt[j-v[i]][i+1]+v[i]*p[i],opt[j][i+1]): opt[j][i+1];//把上面的状态转移方程coding一下 &#125; &#125; for(int i=0;i&lt;=m;i++) &#123; for(int j=0;j&lt;=N;j++) &#123; printf("%3d ",opt[j][i]); &#125; cout&lt;&lt;endl; &#125; cout&lt;&lt;endl&lt;&lt;opt[N][1]; return 0;&#125; 空间压缩当然这里可以通过循环控制，将二维数组压缩为一维数组 1234567891011121314151617181920for(int i=0;i&lt;m;i++) &#123; cin&gt;&gt;v[i+1]&gt;&gt;p[i+1]; //id=i+1&#125;for(int i=0;i&lt;=N;i++)&#123; opt[i]= i&gt;=v[m]?v[m]*p[m]:0;//最后一步的决策是可以直接给出的 //初始化 &#125;for(int i=m-1;i&gt;=1;i--)&#123; for(int j=N;j&gt;=0;j--) &#123; opt[j]= j&gt;=v[i]? max(opt[j-v[i]]+v[i]*p[i],opt[j]): opt[j];//把上面的状态转移方程coding一下 &#125;&#125;cout&lt;&lt;endl&lt;&lt;opt[N];]]></content>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ShortestPath]]></title>
    <url>%2F2019%2F12%2F10%2FShortestPath%2F</url>
    <content type="text"><![CDATA[临时路线. 最短路径：什么是最短路径？最短路径是指从某一顶点出发到达另一顶点，可能存在许多路径，则某路径上的各边权值之和最小，则为最短路径 问题解法 边上权值非负情形的单源最短路径问题：Dijksta算法 边上权值为任意值的单源最短路径问题（不可有带负权边的回路）：Bellman-Ford算法 所有顶点之间的最短路径（全源最短路径）：Floyd算法和Johonson算法 值得注意的的是：最短路径问题具备最优子结构性质，也就是两顶点之间的最短路径包括路径上其他顶点的最短路径。 反证法证明：如果不包含，也就是说有办法使得换了路径上的某些顶点，可以使得仍到达目的地，但是总路径可以变短，这显然是和最短路径相违背的。符号化一些的描述呢，有一条最短路径&lt; i,j &gt;,这条路径上会经过许多点，设为i,v_{1},v_{2}...v_{t},j，如果假设成立，那么设当前从v_{1}到v_{2}这条小路径不是他们之间的最短路径，那么总存在另一条更短的路径可以使得从v_{1}到v_{2}更短，这样就会造成i可以到j，且路径缩短，与大前提&lt; i,j &gt;是最短路径相违背，得证 带权图的单源最短路径Floyd算法\Floyd-Warshall算法Floyd算法.有一说一，讲的可以ヾ(≧▽≦*)o简述一下：初始化图-&gt;加载各点的路径长（矩阵存储）-&gt;顺序引入新的中转点-&gt;遍历更新路径长的矩阵-&gt;继续引入继续更新 for(k=1;k&lt;=n;k++) {//迭代更新新的中转点 for(i=1;i&lt;=n;i++) {//两层循环 遍历更新 路径长度矩阵 的值 for(j=1;j&lt;=n;j++) { if(e[i][j]&gt;e[i][k]+e[k][j]) { e[i][j]=e[i][k]+e[k][j]; } } } } 边权非负情形Dijkstra算法源点该算法使用的是贪心策略：每次都找出剩余顶点中与源点距离最近的一个顶点。参考：Dijkstra算法 算法思想带权图G=令S为已确定了最短路径顶点的集合，则可用V-S(V集合减去S集合)表示剩余未确定最短路径顶点的集合。假设V0是源点，则初始 S={V0}。用数组Distance表示源点V0到其余顶点的路径长度，用数组pre[i]表示最短路径序列上顶点i的前一个顶点。初始时，pre[i]都是源点的下标。接下来需重复两个步骤： 从当前Distance[i]找出最小的一个，记录其下标v=i，源点V0到顶点Vv的最短路径即已确定，把Vv加入S。更新源点到剩余顶点的最短路径长度。更新方法是：以上一步的顶点Vv为中间点，若Distance[v]+weight(v,i)小于Distance[i]，则修改值：pre[i]=v;Distance[i]=Distance[v]+weight(v,i);重复以上两个步骤，直至所有顶点的最短路径都已找到.。需要指出，Dijkstra算法求解的不仅是有向图，无向图也是可以的。下面给出一个完整的有向带权图的实例： 边上权值为任意值的情形（不可有带负权边的回路）SPFA算法\Bellman-Ford算法 核心思想：当图中没有由带负权值的边组成的回路时，有n个顶点的图中任意两点之间如果存在最短路径，此路径最多有n-1条边。这是显然的，从i到j的路径,最多就是把其他所有节点经历过一遍，何况是最短路径 算法思想：记dist[u]为源点v到其他顶点u的最短路径长度 先看一下红线标注的文字，最多这两个字很精髓，这样的话可以发现dist^{i}[u]包含了dist^{i}[u]（最多包含三条边当然范围比最多包含两条边大了既然范围大，那么其利用的条件与资源就可以越充分）为什么最终目的是要求出dist^{n-1}[u]呢，由上面的理解可以知道，dist^{i}[u]的i越大，其范围越大，所使用的的资源越充分，更接近于答案递归关系式： dist^{k}[u]=min\{dist^{k-1}[u],min\{dist^{k-1}[j]+Edge[j][u]\}\}递归出口：dist^{1}[u]=Edge[v][u] 当然，可以使用循环从最底层逐层往上计算 Johnson算法A*算法]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HNUACM10011]]></title>
    <url>%2F2019%2F12%2F04%2FHNUACM10011%2F</url>
    <content type="text"><![CDATA[Problem descriptionThe inversion number of an integer sequence a1, a2, . . . , an is the number of pairs (ai, aj) that satisfy i &lt; j and ai &gt; aj . Given n and the inversion number m, your task is to find the smallest permutation of the set { 1, 2, . . . , n }, whose inversion number is exactly m.A permutation a1, a2, . . . , an is smaller than b1, b2, . . . , bn if and only if there exists an integer k such that aj = bj for 1 &lt;= j &lt; k but ak &lt; bk.InputThe input consists of several test cases. Each line of the input contains two integers n and m. Both of the integers at the last line of the input is −1, which should not be processed. You may assume that 1 &lt;= n &lt;= 50000 and 0 &lt;= m &lt;= n(n − 1)/2. OutputFor each test case, print a line containing the smallest permutation as described above, separates the numbers by single spaces. Sample Input5 97 3-1 -1Sample Output4 5 3 2 11 2 3 4 7 6 5Problem SourceShanghai-P 2004 题目中文解释就是说给你两个数n和m，你需要给出1~n得到某种排列方式，使得该序列的逆序数的对数为m，如果有多个符合条件的序列的话，应当给出最小的序列（序列大小比较规则类似字典序） 解法来源介绍额…我暂时没看过题解，走在上课路上无聊，大概比划了一下，给出了一种解法，等一下给出适当的证明 解法简介（有些名词自己起的，比较中二，但我喜欢）先来看看两个最特殊的序列例如n=7的时候序列1：1234567序列2：7654321 为什么说特殊呢，是因为这分别是序列集合的上边界和下边界，因为他们的逆序数的对数为min=0和max=\frac{n\cdot (n-1)}{2} 由于是要求符合条件的最小序列，那么根据贪心的思想答案序列的最终样子大概会长这样子：【一段顺序递增序列】【一段比较乱序的序列，大体上是倒序的】，额..为什么这么描述呢，因为我就是这么想的 随便给一个m=3 我先在这里给出两个我思考时候定义的名词 下界序列 增量因子再给出一些会用到的公式：定义NUM[n]为 长度为n的序列的最大逆序数的对数根据max公式知道： NUM[n]=\frac{n\cdot (n-1)}{2} NUM[n-1]=\frac{(n-1)\cdot (n-2)}{2} 有序区间 和 无序区间 分别对应 【一段顺序递增序列】【一段比较乱序的序列，大体上是倒序的】 的左半部分和右半部分有一说一，确实很中二，但是很形象， 由于m=3,那么看一下怎样的序列可以满足序列一：1 2 3 4 5 6 | 7:0序列二：1 2 3 4 5 | 7 6:1序列三：1 2 3 4 | 7 6 5:3序列四：1 2 3 | 4 5 6 7:6 是的，序列三就满足了，但是为什么要算到序列四呢？嗯，这里再给出m=4时的思考过程 注意到之所以序列三能带来3的逆序数对数，是因为他的无序区间长度为3，或者说7 6 5 这一无序区间内的元素个数为3因而就有NUM[3]=3，再比如序列四的逆序数对数就应该是NUM[4]=4*3/2=6 这时候发现m=4并没有对应的序列，但是同时可以思考到，它夹在3和6之间，是不是可以调节一下，使得它们变成需要的序列呢？ 这里叫3为 m=4的下界，对应的下界序列为7 6 5，下界序列的元素个数为3 ， 由于7 6 5 这个序列的逆序数对数的极限大小就是3了，所以必须要引入改变，才可能变多 在尽可能保持有序序列有序性的情况下，以下界序列所在的序列（这里是序列三）为基础，引入增量因子（即再动用一个数，向序列四靠近，这里是序列三中最靠近分界线的4）： 思考的精髓之处动用4 对序列三做调节，向序列四靠近，即由下界向上界的移动 1 2 3 | 4 | 7 6 5 做准备的序列三 4作为增量因子去置换7 6 5 将会产生如下的变化：4 不换 那就是序列三 NUM=3 4 置换5 那么就有 5 | 7 6 4，不论 7 6 4 如何变化 5 作为增量因子 能带来的逆序数对数的增量为1，因为他是这四个数里倒数第二大的，肯定比7 6 小，但是比4大，这时序列的逆序数对数为NUM= 增量因子5带来的增量delta=1 + numOf(7,6,4)所产生的最大对数3，这是为了维持下界序列提供的逆序数对数，故将7 6 4 三个元素排列为 降序 也就是7 6 4 4 置换6 那么就有 6 | 7 4 5，同样的分析，增量因子6能带来的增量为常值2，调节后NUM=2+3=5 这样的两种调节分别产生了 NUM=4 NUM=5 均介于 3 和 6之间 还可以 没做任何优化 没任何算法 一次通过 排名55 nice 下面贴上代码和纸上的一些演示：还有一段证明没给出，有时间再写吧（要写作业了） # include&lt;iostream&gt; # include&lt;algorithm&gt; # include&lt;cmath&gt; # include&lt;vector&gt; using namespace std; vector&lt;long long int&gt;result; int main() { bool IsFirstIn=true; while(1) { long long int n,m; cin&gt;&gt;n&gt;&gt;m; if(n==-1&amp;&amp;m==-1) return 0; //n*(n-1)=m*2; if(IsFirstIn) IsFirstIn=false; else{ cout&lt;&lt;endl; } long long int downBorder=(1+sqrt(1+8*m))/2; //123 4 765 for(int i=1;i&lt;=n-(downBorder+1);i++) { result.push_back(i); } long long int Goal_Delta=m- downBorder*(downBorder-1)/2; //3~6---7 delta of 6 is 2 6-&gt;2 5-&gt;1 4-&gt;0 result.push_back(n-downBorder+Goal_Delta); //cout&lt;&lt;n-downBorder+Goal_Delta; for(int i=n;i&gt;=n-downBorder;i--) { if(i!=n-downBorder+Goal_Delta) result.push_back(i); } for(int i=0;i&lt;result.size();i++) { if(i==0) { cout&lt;&lt;result[i]; }else cout&lt;&lt;&#39; &#39;&lt;&lt;result[i]; } result.erase(result.begin(),result.end()); } return 0; } 我还得写数电作业和完成CPU的综合设计，忙中刷题，抽空会维护以前的题解的。]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HNUACM10007]]></title>
    <url>%2F2019%2F12%2F03%2FHNUACM10007%2F</url>
    <content type="text"><![CDATA[include# include&lt;ctime&gt; # include&lt;algorithm&gt; # include&lt;map&gt; using namespace std; typedef long long ll; map&lt;ll, int&gt;m; const int mod = 10000019; const int times = 50;//测试50次 ll mul(ll a, ll b, ll m) //求a*b%m { ll ans = 0; a %= m; while(b) { if(b &amp; 1)ans = (ans + a) % m; b /= 2; a = (a + a) % m; } return ans; } ll pow(ll a, ll b, ll m) //a^b % m { ll ans = 1; a %= m; while(b) { if(b &amp; 1)ans = mul(a, ans, m); b /= 2; a = mul(a, a, m); } ans %= m; return ans; } bool Miller_Rabin(ll n, int repeat)//n是测试的大数，repeat是测试重复次数 { if(n == 2 || n == 3)return true;//特判 if(n % 2 == 0 || n == 1)return false;//偶数和1 //将n-1分解成2^s*d ll d = n - 1; int s = 0; while(!(d &amp; 1)) ++s, d &gt;&gt;= 1; //srand((unsigned)time(NULL));在最开始调用即可 for(int i = 0; i &lt; repeat; i++)//重复repeat次 { ll a = rand() % (n - 3) + 2;//取一个随机数,[2,n-1) ll x = pow(a, d, n); ll y = 0; for(int j = 0; j &lt; s; j++) { y = mul(x, x, n); if(y == 1 &amp;&amp; x != 1 &amp;&amp; x != (n - 1))return false; x = y; } if(y != 1)return false;//费马小定理 } return true; } ll gcd(ll a, ll b) { return b == 0 ? a : gcd(b, a % b); } ll pollard_rho(ll n, ll c)//找到n的一个因子 { ll x = rand() % (n - 2) + 1; ll y = x, i = 1, k = 2; while(1) { i++; x = (mul(x, x, n) + c) + n;//不断调整x2 ll d = gcd(y - x, n); if(1 &lt; d &amp;&amp; d &lt; n) return d;//找到因子 if(y == x) return n;//找到循环，返回n，重新来 if(i == k)//一个优化 { y = x; k &lt;&lt;= 1; } } } void Find(ll n, ll c) { if(n == 1)return;//递归出口 if(Miller_Rabin(n, times))//如果是素数，就加入 { m[n]++; return ; } ll p = n; while(p &gt;= n) p = pollard_rho(p, c--);//不断找因子，知道找到为止，返回n说明没找到 Find(p, c); Find(n / p, c); } void PrintMin(ll n) { srand((unsigned)time(NULL)); m.clear(); Find(n, rand() % (n - 1) + 1);//这是自己设置的一个数 //cout&lt;&lt;n&lt;&lt;&quot; = &quot;; ll min= m.begin()-&gt;first; for(map&lt;ll ,int&gt;::iterator it = m.begin(); it != m.end();it++) { //cout&lt;&lt;it-&gt;first&lt;&lt;&quot; ^ &quot;&lt;&lt;it-&gt;second; if((*it).first&lt;min) { min=(*it).first; } } cout&lt;&lt;min&lt;&lt;endl; } int main() { int t; cin&gt;&gt;t; for(int i=0;i&lt;t;i++) { ll n; cin&gt;&gt;n; if(Miller_Rabin(n, times)) { cout&lt;&lt;&quot;Prime&quot;&lt;&lt;endl; } else{ PrintMin(n); } } return 0; }]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pollard_rho]]></title>
    <url>%2F2019%2F11%2F24%2FPollard-rho%2F</url>
    <content type="text"><![CDATA[大数因式分解 Pollard_rho 算法大致流程：先判断当前数是否是素数（Miller_rabin），如果是则直接返回。如果不是素数的话，试图找到当前数的一个因子（可以不是质因子）。然后递归对该因子和约去这个因子得到的另一个因子进行分解。//gcd 欧几里得算法见BasicAlgorithm post 参考资料 Concepts used in Pollard’s Rho Algorithm: 一、Two numbers x and y are said to be congruent modulo n (x = y modulo n)： 1. if their absolute difference is an integer multiple of n, OR 2. each of them leaves the same remainder when divided by n. //x 和 y 同余 二、The Greatest Common Divisor is the largest number which divides evenly into each of the original numbers.//最大公约数 三、Birthday Paradox: The probability of two persons having same birthday is unexpectedly high even for small set of people.//生日悖论 四、Floyd’s cycle-finding algorithm: If tortoise and hare start at same point and move in a cycle such that speed of hare is twice the speed of tortoise, then they must meet at some point.//如果乌龟和兔子在同一点开始，并且以兔子的速度是乌龟速度的两倍的循环运动，那么它们必须在某个点相遇。 Algorithm: Start with random x and c. Take y equal to x and f(x) = $$x^{2}$$ + c. While a divisor isn’t obtained Update x to f(x) (modulo n) [Tortoise Move] Update y to f(f(y)) (modulo n) [Hare Move] Calculate GCD of |x-y| and n If GCD is not unity If GCD is n, repeat from step 2 with another set of x, y and c Else GCD is our answer //优化版 ll pollard_rho(ll n, ll c)//找到n的一个因子 { ll x = rand() % (n - 2) + 1; ll y = x, i = 1, k = 2; while(1) { i++; x = (mul(x, x, n) + c) + n;//不断调整x2 ll d = gcd(y - x, n); if(1 &lt; d &amp;&amp; d &lt; n) return d;//找到因子 if(y == x) return n;//找到循环，返回n，重新来 if(i == k)//一个优化 { y = x; k &lt;&lt;= 1; } } //原始版 ll pollard_rho2(ll n, int c)//找到n的一个因子 { ll x = rand() % (n - 2) + 1; ll y = x, k = 2; while(1) { x = (mul(x,x, n) + c) + n;//不断调整x2 y= (mul(y,y, n) + c) + n; y= (mul(y,y, n) + c) + n; ll d =gcd(y-x&gt;0?y-x:x-y , n); if(1 &lt; d &amp;&amp; d &lt; n) return d;//找到因子 if(d == n) return n;//找到循环，返回n，重 新来 if(d==1) return n; } } void Find(ll n, ll c) { if(n == 1)return;//递归出口 if(Miller_Rabin(n, times))//如果是素数，就加入 { m[n]++; return ; } ll p = n; while(p &gt;= n) p = pollard_rho(p, c--);//不断找因子，知道找到为止，返回n说明没找到 Find(p, c); Find(n / p, c); }]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PrimeJudge]]></title>
    <url>%2F2019%2F11%2F22%2FPrimeJudge%2F</url>
    <content type="text"><![CDATA[Miller_Rabin（米勒-拉宾）素数判别法：时间复杂度O(logn) 质数:是指在大于1的自然数中，除了1和它本身以外不再有其他因数的自然数。 互质(互素：互质是公约数只有1的两个整数，叫做互质整数。公约数只有1的两个自然数，叫做互质自然数，后者是前者的特殊情形。 同余： 同余式是数论的基本概念之一，设m是给定的一个正整数，a、b是整数，若满足m|(a-b)，则称a与b对模m同余，记为a≡b(mod m)，或记为a≡b(m)。这个式子称为模m的同余式，若m∤ (a-b)，则称a、b对模m不同余，同余概念又常表达为： 1.a=b+km(k∈Z)； 2.a和b被m除时有相同的余数。 同余式的记号由高斯(Gauss,C.F.)于1800年首创,发表在他的数论专著《算术研究》之中。 快速幂：均见BasicAlgorithm post 快速积：均见BasicAlgorithm post 位运算：均见BasicAlgorithm post 费马小定理：设p是素数，a与p互素，则 a^{(p-1)}\equiv 1(mod p) 二次探测：如果p是素数，x是小于p的正整数，且x^{2}\equiv1(modp)那么要么x=1，要么x=p-1值得注意的是有一条推导结论(原论文长达500多页，我找不着，就不看了，只找到了引用该结论的其他文章)在如下算法流程中：有的d,r,n,a几个重要参数其中给出结论： 其中这份代码算是所看博客遇见最为清晰的一份之一了 清晰版# include&lt;cstdio&gt; # include&lt;cstring&gt; # include&lt;algorithm&gt; using namespace std; int prime[10]={2,3,5,7,11,13,17,19,23,29}; int Quick_Multiply(int a,int b,int c) //快速积（和快速幂差不多） { long long ans=0,res=a; while(b) { if(b&amp;1) ans=(ans+res)%c; res=(res+res)%c; b&gt;&gt;=1; } return (int)ans; } int Quick_Power(int a,int b,int c) //快速幂，这里就不赘述了 { int ans=1,res=a; while(b) { if(b&amp;1) ans=Quick_Multiply(ans,res,c); res=Quick_Multiply(res,res,c); b&gt;&gt;=1; } return ans; } bool Miller_Rabin(int x) //判断素数 { int i,j,k; int s=0,t=x-1; if(x==2) return true; //2是素数 if(x&lt;2||!(x&amp;1)) return false; //如果x是偶数或者是0,1，那它不是素数 while(!(t&amp;1)) //将x分解成(2^s)*t的样子 { s++; t&gt;&gt;=1; } for(i=0;i&lt;10&amp;&amp;prime[i]&lt;x;++i) //随便选一个素数进行测试 { int a=prime[i]; int b=Quick_Power(a,t,x); //先算出a^t for(j=1;j&lt;=s;++j) //然后进行s次平方 { k=Quick_Multiply(b,b,x); //求b的平方 if(k==1&amp;&amp;b!=1&amp;&amp;b!=x-1) //用二次探测判断 return false; b=k; } if(b!=1) return false; //用费马小定律判断 } return true; //如果进行多次测试都是对的，那么x就很有可能是素数 } int main() { int x; scanf(&quot;%d&quot;,&amp;x); if(Miller_Rabin(x)) printf(&quot;Yes&quot;); else printf(&quot;No&quot;); return 0; } 简洁版bool isPrime(ll a,ll n) {//a is test ll d =n-1; if(n&lt;1) return false; if(n==2) return true; if(!(n&amp;1)) return false; while(!(d&amp;1)) d&gt;&gt;=1; ll t=Quick_Power(a,d,n); while(d!=n-1&amp;&amp;t!=1&amp;&amp;t!=n-1) { t=Quick_Power(t,2,n); d&lt;&lt;=1; } return (t==n-1)||(d&amp;1); }]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knapsackproblem]]></title>
    <url>%2F2019%2F11%2F21%2FKnapsackproblem%2F</url>
    <content type="text"><![CDATA[0-1 背包问题给定n个重量为w_{1},w_{2},..,w_{n}的物品，价值为v_{1},v_{2},…,v_{n}的物品和容量为C的背包，求这个物品中一个最有价值的子集，使得在满足背包的容量的前提下，包内的总价值最大 方案一:递归求解递归遍历解空间树，枚举，2_{n}（会因为剪枝而大大减少）种情况 answer[1~n]=max(选择物品1:value[1]+answer[2~n] 但是背包容量减少，不选择物品1+answer[2~n]背包容量不变) # include&lt;iostream&gt; # include&lt;algorithm&gt; # include&lt;cmath&gt; using namespace std; int n; int w[100],v[100]; int Enum(int index,int c) { if(c&lt;0) return 0; if(index&gt;n-1) return 0; if(c&gt;=w[index]) { return max( v[index]+Enum(index+1,c-w[index]),Enum(index+1,c) ); } else{ return Enum(index+1,c); } } int main() { cin&gt;&gt;n; int c; cin&gt;&gt;c; for(int i=0;i&lt;n;i++) { cin&gt;&gt;w[i]&gt;&gt;v[i]; } cout&lt;&lt;Enum(0,c); return 0; }]]></content>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LongestIncreasingSubsequence]]></title>
    <url>%2F2019%2F11%2F21%2FLongestIncreasingSubsequence%2F</url>
    <content type="text"><![CDATA[最长上升子序列两种方法介绍法一：dp状态转移法二：贪心+二分做法：思路：1. 求最长递增子序列的长度时，我们使用二分查找的方法求，具体的操作是维护一个辅助数组，这个辅助数组dp[k]中保存着当前已经获得最长递增子序列，k表示当前的辅助数组中递增序列的个数，当处理原数组中下一个元素data[i]时，1）若data[i] &gt; dp[k - 1]时， 直接dp[k++] = data[i];2) 否则，使用二分查找在dp中找到第一个不大于data[i]的元素的位置j, 使得dp[j] = data[i]; 方法1复杂度$O(n^{2})$ 理解State:Length[index][TailHeight]-&gt; 1. Length[index+1][TailHeight] 当前元素不能接在LIS后面，下一状态继承当前状态 2. Length[index+1][Height[index]]+1 or Length[index+1][TailHeight]当前元素能接在LIS后面，下一状态基于当前状态做出两种决策 分析一下最终结果 假如输入序列中有n个元素，那么记最后结果为dp[n]显然 $dp[n]= max(dp[i,i\epsilon[1,n]]+ element[n]&gt;element[i]?1:0; )$ dp[i]为 以element[i]结尾的序列问题的最优解 状态转移方程：$ dp[i]=max(dp[j]+ element[i]]&gt;element[j]?1:0 ),j\epsilon[1,i)$answer:dpcin:element 如果已经求出前n-1个元素的最优解，那么可以求出前n个元素的最优解如果已经求出前n-2个元素的最优解，那么可以求出前n-1个元素的最优解….初始化 #include&lt;cstdio&gt; #include&lt;cstring&gt; #define M 2000000 + 5 int a[M],f[M],b[M]; int ans,n,k; int main(){ scanf(&quot;%d&quot;,&amp;n); for ( int i = 1;i &lt;= n; ++ i){ scanf(&quot;%d&quot;,&amp;a[i]); f[i] = 1; } for ( int i = 2;i &lt;= n; ++ i) for ( int j = 1;j &lt; i; ++ j) if (a[j] &lt; a[i] &amp;&amp; f[j] + 1 &gt; f[i]) f[i] = f[j] + 1; for ( int i = 1;i &lt;= n; ++ i) if (f[i] &gt; ans) ans = f[i]; printf(&quot;%d\n&quot;,ans); } 方法2复杂度 $O(n\log n)$ 1.lower_bound(起始地址，结束地址，要查找的数值) 返回的是大于或等于val的第一个元素位置 2.upper_bound(起始地址，结束地址，要查找的数值) 返回的是返回大于val的第一个元素位置 3.binary_search(起始地址，结束地址，要查找的数值) 返回的是是否存在这么一个数，是一个bool值。 1.2.两个函数的用法类似，在一个左闭右开的有序区间里进行二分查找，需要查找的值由第三个参数给出。 对于upper_bound来说，返回的是被查序列中第一个大于查找值的指针，也就是返回指向被查值&gt;查找值的最小指针，lower_bound则是返回的是被查序列中第一个大于等于查找值的指针，也就是返回指向被查值&gt;=查找值的最小指针。 不过除此之外，这两个函数还分别有一个重载函数，可以接受第四个参数。如果第四个参数传入greater&lt;Type&gt;()，其中Type改成对应类型，那么upper_bound则返回指向被查值&lt;查找值的最小指针，lower_bound则返回指向被查值&lt;=查找值的最小指针。 此外，如果你用上述两个函数三个参数的那种形式，记得那个左闭右开的区间要为非递减的顺序，如果你给第四个参数传入greater&lt;Type&gt;()，则区间为非递增的顺序。 #include&lt;iostream&gt; #include&lt;algorithm&gt; #include&lt;cstring&gt; using namespace std; int greedy[1000+5]; int a[1000+5]; int main() { memset(greedy,0,1005*sizeof(int)); memset(a,0,1005*sizeof(int)); int n; cin&gt;&gt;n; for(int i=0;i&lt;n;i++) { cin&gt;&gt;a[i]; } greedy[0]=a[0]; int endIndex=0; for(int i=1;i&lt;n;i++) { if(a[i]&gt;greedy[endIndex]) { endIndex++; greedy[endIndex]=a[i]; } else{ if(a[i]&lt;greedy[endIndex]) {//??? a[upper_bound(greedy,greedy+endIndex+1,a[i])-&amp;(a[0])]=a[i]; } } } cout&lt;&lt;endIndex+1; /*cout&lt;&lt;binary_search(a,a+n,3)&lt;&lt;endl; cout&lt;&lt;upper_bound(a,a+n,3)-&amp;(a[0]); cout&lt;&lt;endl&lt;&lt;lower_bound(a,a+n,3)-a;*/ return 0; } ​]]></content>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Search]]></title>
    <url>%2F2019%2F11%2F20%2FSearch%2F</url>
    <content type="text"><![CDATA[检索 假设$k_{1},k_{2}……,k_{n}$是互不相同的关键码值，有一个包含n条记录的集合C，形式如下$(k_{1},I_{1}),(k_{2},I_{2}),……,(k_{n},I_{n})$。其中I_{j}是与关键码值绑定的value 匹配分类精确匹配：检索关键码值与某个特定值匹配的记录范围查询：检索关键码值在某个指定范围内得到所有记录 检索算法顺序表和线性表方法顺序查找和折半查找顺序查找的时间性能： 等概率$ASL_{ss}=$ 不等概率 二分检索法将dataList[i].Key与给定值K作比较三种情况 Key=K,检索成功 返回dataList[i] Key&gt;K,则改为查找dataList[i]的前半个表中查找 Key&lt;K,则改为查找dataList[i]的后半个表中查找 散列：理想情况下O(1) 树索引：启发式规则：初始序列：ABCDEFGH访问模式：FDFGEGFADF 计数统计方法保持线性表按照访问频率排序：A(1)B(0)C(0)D(2)E(2)F(4)G(3)H(0) 排序后：FGDEABCH 移至前端访问某条记录，将该记录移至前端（刚访问过的元素很可能再被访问）F:FABCDEGHD:DFABCEGHF:FDABCEGHG:GFDABCEHE:EGFDABCH…ANSWER:EGFDABCH 转置将当前访问的记录与它在线性表中的前一条记录交换位置(访问某个元素，将其在线性表中的位置前移一位) 与‘移至前端’原理类似 哈希表哈希函数 Key-position映射]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph]]></title>
    <url>%2F2019%2F11%2F15%2FGraph%2F</url>
    <content type="text"><![CDATA[图论V表示点 E表示边 图的两种常见实现方式：1.邻接矩阵2.邻接表 邻接矩阵Edge[n][n]则约定Edge[i][j]仅在 节点i与节点j之间有连线时候为1（有向图和无向图略有区别…无向图有联系即为双向，所以会把Edge[i][j]与Edge[j][i]均置为1，有向图则根据指向置为1） 邻接表无向图：简单来说就是图中每一个点拥有一个自己的链表，这个链表以自己为头结点，链表中其他的节点都是头结点的邻居点dest邻居link指向下一个邻居 有向图：有向图因为具有方向性，所以使用两个邻接表来分别表示某节点的出边和入边。出边表：第i个链表，是以该链表头结点为起点发出的边，指向各个邻居入边表：第i个链表，是以该链表头结点为终点的边，从各个邻居发出，在该头节点结束 术语有向图：边具有方向性，边限定从一个顶点指向另一个顶点尖括号&lt;&gt;表示顶点偶对:&lt; v1,v2 &gt;和&lt; v2,v1 &gt;表示两个不同的边 无向图：边无方向性圆括号()表示顶点偶对：( v1,v2 )和( v2,v1 )表示同一个边 稀疏图，密集图：边数的多少 完全图：若有n个顶点的无向图有n(n-1)/2条边，则此图为无向图（所有顶点两两之间都有边相连接）若有n个顶点的有向图有n(n-1)/2*2=n(n-1)条边，则此图为有向完全图（所有顶点两两之间都有边相连接） 邻接顶点：如果(u,v)是E(G)中的一条边，则称u和v互为邻接顶点权：某些图的边具有与之相关数，成为边的权.这种带权图称为网络 子图：设有两个图G=(V,E)，设有两个图G＝( V,E )和G‘＝( V’,E’)。若 V’属于V且 E’属于E , 则称图G’是图G的子图。即 拿出一个图中的某一些点和某一些边作为一个图（类比子集和全集） 顶点的的度：一个顶点的度是与他相连关联的边的条数。记作TD(v) 在有向图中，定点的度等于该顶点的出度与入度之和（虽然入度与出度的值一正一负来区分，但是这里是绝对值求和） 顶点v的入度和出度：入度：以v为终点的有向边的条数出度：以v为起点的有向边的条数 路径：在图 G＝(V, E) 中, 若从顶点v_{i}出发, 沿一些边经过一些顶点 v_{p1},v_{p2},…,v_{pm}到达顶点v_{j} 。则称顶点序列 ( v_{i} v_{p1} v_{p2} ... v_{pm} v_{j} ) 为从顶点v_{i} 到顶点v_{j}的路径。它经过的边(v_{i} , v_{p1})、(v_{p1} , v_{p2})、...、(v_{pm} , v_{j})应是属于E的边。 路径长度非带权图的路径长度是指此路径上边的条数。带权图的路径长度是指路径上各边的权之和 简单路径若路径上各顶点 v_{1} , v_{2} ,..., v_{m} 均不互相重复, 则称这样的路径为简单路径。回路若路径上第一个顶点v_{1} 与最后一个顶点v_{m} 重合,则称这样的路径为回路或环。 极大连通子图和极小连通子图的定义及讲解极大连通子图，极小连通子图分量，显然是可能存在多个的但是若是极大连通子图，表征了其不应该被其他分量所包含（如果真有，那么就应该是那位来当极大连通子图了）极小连通子图类比补充：强连通图只有一个强连通分量，即本身。 连通图与连通分量在无向图中, 若从顶点v_{1}到顶点v_{2}有路径, 则称顶点v_{1}与v_{2}是连通的。如果图中任意一对顶点都是连通的, 则称此图是连通图。非连通图的极大连通子图叫做连通分量 强连通图与强连通分量在有向图中, 若对于每一对顶点v_{i} 和v_{j} , 都存在一条从v_{i} 到v_{j} 和从v_{j} 到v_{i} 的路径, 则称此图是强连通图。非强连通图的极大强连通子图叫做强连通分量。 生成树一个连通图的生成树是它的极小连通子图，在n个顶点的情形下，有n-1条边。但有向图则可能得到它的由若干有向树组成的生成森林。 不讨论的图（课程） 图的遍历从已给的连通图中某一顶点出发，沿着一些边访遍图中所有的顶点，且使每个顶点仅被访问一次，就叫做图的遍历 ( Graph Traversal )。 图中可能存在回路，且图的任一顶点都可能与其它顶点相通，在访问完某个顶点之后可能会沿着某些边又回到了曾经访问过的顶点。 为了避免重复访问，可设置一个标志顶点是否被访问过的标志位，它的初始状态为 0，在图的遍历过程中，一旦某一个顶点 i被访问，就立即让该顶点标志位为 1，防止它被多次访问。 DFS（一直往前走，直到走不通）栈实现从深度优先搜索遍历连通图的过程来看，类似于树的先根遍历； 邻接表表示时，查找所有顶点的邻接点所需时间为O(E)，访问顶点的邻接点所花时间为O（V）,此时，总的时间复杂度为O(V+E)。 邻接矩阵表示时，查找每个顶点的邻接点所需时间为O(V)，要查找整个矩阵，故总的时间度为O(V^2)。 v为图的顶点数，E为边数。 BFS队列实现（水面上滴一滴水，中心逐层向外扩展）邻接表形式存储时，每个顶点均需搜索一次，时间复杂度T1=O（v），从一个顶点开始搜索时，开始搜索，访问未被访问过的节点。最坏的情况下，每个顶点至少访问一次，每条边至少访问1次，这是因为在搜索的过程中，若某结点向下搜索时，其子结点都访问过了，这时候就会回退，故时间复 杂度为O(E)，算法总的时间复 度为O(|V|+|E|)。 邻接矩阵存储方式时，查找每个顶点的邻接点所需时间为O(V)，即该节点所在的该行该列。又有n个顶点，故算总的时间复杂度为O(|V|^2)。 有向图的拓扑排序例：大学生的专业培养计划（学习概率论的基础是高数1和高数2，即前置课程） 定义AOV网——用顶点表示活动，用弧表示活动间优先关系的有向图称为顶点表示活动的网(Activity On Vertex network)，简称AOV网，若&lt; vi,vj &gt;是图中有向边，则vi是vj的直接前驱；vj是vi的直接后继AOV网中不允许有回路，这意味着某项活动以自己为先决条件 拓扑排序把AOV网络中各顶点按照它们相互之间的优先关系排列成一个线性序列的过程 检测AOV网中是否存在环方法：对有向图构造其顶点的拓扑有序序列，若网中所有顶点都在它的拓扑有序序列中，则该AOV网必定不存在环（） 拓扑排序的方法 在有向图中选一个没有前驱的顶点且输出之 从图中删除该顶点和所有以它为起点的弧 重复上述两步，直至全部顶点均已输出；或者当图中不存在无前驱的顶点为止即：当前可以进入排序序列的节点一定符合条件：其没有前驱节点（要么是十分基础的课程，没有前置课程就能开始学习；要么就是其前置课程已经学习完毕，可以开始学习该课程） Kruskal 算法Kruskal 算法也是一个简单的贪心算法考虑问题的出发点为使得生成树的权值和最小，则应尽量使得每条加入生成树的边尽可能小 首先，将顶点集分为|V| 个等价类，每个等价类包括一个顶点 然后，以权的大小为序处理各条边。如果某条边连接两个不同等价类的顶点，则这条边被加入MST ，两个等价类也被合为一个3.反复执行此过程直至只余下一个等价类。 Kruskal 算法的关键技术:对边(权)进行贪心 取权值最小的边首先对边进行完全排序，使用最小值堆来实现，一次取一条边。实际上在完成MST 前仅需访问一小部分边。 确定两个顶点是否属于同一等价类可以用树的基于父指针表示法的UNION/FIND算法，可以看树post里关于并查集的介绍 例子：]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LuoguP1090]]></title>
    <url>%2F2019%2F11%2F10%2FLuoguP1090%2F</url>
    <content type="text"><![CDATA[P1090 合并果子水题 (๑•̀ㅂ•́)و✧ 优先队列 直接贪心从头来过… #include&lt;iostream&gt; #include&lt;queue&gt; #include&lt;vector&gt; using namespace std; priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt;fruit; int main() { int n; cin&gt;&gt;n; for(int i=0;i&lt;n;i++) { int weight; cin&gt;&gt;weight; fruit.push(weight); } int ans=0; while(fruit.size()!=1) { int w1,w2; w1=fruit.top(); fruit.pop(); w2=fruit.top(); fruit.pop(); ans+=w1+w2; fruit.push(w1+w2); } cout&lt;&lt;ans; return 0; }]]></content>
      <tags>
        <tag>Luogu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Greedy]]></title>
    <url>%2F2019%2F11%2F10%2FGreedy%2F</url>
    <content type="text"><![CDATA[贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择，选择的贪心策略必须具备无后效性，即某个状态以前的过程不会影响以后的状态，只与当前状态有关 P1090 合并果子优先队列（使用stl写好的priority_queue）先出队列元素不是先进队列的元素，而是队列中优先级最高的元素代码 priority_queue基本用法： //升序队列 push小的优先 小的在堆顶 priority_queue &lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; q; //降序队列 push大的优先 大的在堆顶 也是默认情况 priority_queue &lt;int,vector&lt;int&gt;,less&lt;int&gt; &gt;q; struct node{ int x; bool operator&lt;(const tmp1&amp; a) const { return x &lt; a.x; // this object 的优先级 小于 a的优先级 } }]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BinSearchTree]]></title>
    <url>%2F2019%2F11%2F10%2FBinSearchTree%2F</url>
    <content type="text"><![CDATA[二叉检索树二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树）它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。可以实现字典的功能，通过将关键码与值绑定（可以将不可比较大小的值，变得可比，大概～(￣▽￣～)(～￣▽￣)～）二分检索树的时间复杂度为\theta (\log n) 插入操作key值如果小于当前节点的k值，往右边深入，否则往左边深入，直到成为叶子节点 删除操作step1:deletemin()//删除树中的最小值根据树的特性，一直向左子树深入，知道某个节点没有左子节点，则该节点为树中最小节点.则欲删除该节点，只需使其父节点的leftChild指针 直接指向该节点的右子树，即可完成删除.且不破坏BST树的结构（其父节点一定大于等于其左子树中各节点） step2:1.如果要删除的节点R没有子节点，那么只需要把其父节点指向R的指针，指向NULL即可2.如果要删除的节点R只有一个子节点，那么只需要把其父节点指向R的指针，指向R唯一的一颗子树即可3.如果要删除的节点R有两个子节点，较好地解决办法是从某棵子树中，找出一个R的替代者替代者的选取：大于(或等于)被替换值的最小者]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Huffman]]></title>
    <url>%2F2019%2F11%2F09%2FHuffman%2F</url>
    <content type="text"><![CDATA[Huffman树设根树T有t片树叶v_{1},v_{2},..,v_{t},给每片树叶赋一个权值w_{1},w_{2},...,w_{t},则成为T赋权二叉树，其中l(v_{i})为叶子节点v_{i}的长度（节点到根节点路径长度），如果存在一种赋权方式，使得\sum_{i=1}^{t}w_{i}l(v_{i})的值（带权路径和）达到最小，则称这棵树为最优二叉树，或称Huffman树. 构造方法(1).对所有权值从低到高排序.(2).找出两个最小的权值，记为w_{1},w_{2}(3).用(w_{1}+w_{2} 代替w_{1}与w_{2}),产生新的队列(4).若队列中的点数大于1，则回到第一步，否则进行下一步. (5).逆序将以上组合过程画出来得到Huffman树 注意点构造时为保证唯一性，要求”左小右大，组合优先，左0右1，不足补0”左小右大：值权值小的画左边，权值大的画右边组合优先：如果某两个权值相同，则组合出来的节点优先（作为左子节点，画在左边）左0右1：是指向左儿子的边的编号为0， 指向右儿子的边编号为1不足补零：译码时使用，在末尾添加适当0以使得可以译码码]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LuoguP1162]]></title>
    <url>%2F2019%2F11%2F09%2FLuoguP1162%2F</url>
    <content type="text"><![CDATA[模板题目：和Leetcode T130的解法只需要改一下参数就可 #include&lt;cstdio&gt; #include&lt;vector&gt; #include&lt;cstring&gt; #include&lt;iostream&gt; using namespace std; void DFS(int r,int c,vector&lt;vector&lt;char&gt; &gt;&amp; board) { if(r&lt;0||r&gt;=board.size()||c&lt;0 ||c&gt;=board[0].size()||board[r][c]==&#39;1&#39; ||board[r][c]==&#39;H&#39;) { return ; } board[r][c]=&#39;H&#39;; DFS(r+1,c,board); DFS(r-1,c,board); DFS(r,c+1,board); DFS(r,c-1,board); } void solve(vector&lt;vector&lt;char&gt; &gt;&amp; board) { if(board.size()&lt;1||board[0].size()&lt;1) return ; for(int i=0;i&lt;board[0].size();i++) DFS(0,i,board); for(int i=0;i&lt;board[0].size();i++) DFS(board.size()-1,i,board); for(int i=0;i&lt;board.size();i++) DFS(i,0,board); for(int i=0;i&lt;board.size();i++) DFS(i,board[0].size()-1,board); for(int i=0;i&lt;board.size();i++) { for(int j=0;j&lt;board[0].size();j++) { if(board[i][j]==&#39;H&#39;) board[i][j]=&#39;0&#39;; else{ if(board[i][j]==&#39;0&#39;) board[i][j]=&#39;2&#39;; } } } } int main() { int n; cin&gt;&gt;n; vector&lt;vector&lt;char&gt; &gt;board; vector&lt;char&gt;temp; for(int i=0;i&lt;n;i++) { for(int j=0;j&lt;n;j++) { char ch; cin&gt;&gt;ch; temp.push_back(ch); } board.push_back(temp); temp.erase(temp.begin(),temp.end()); } solve(board); for(int i=0;i&lt;n;i++) { for(int j=0;j&lt;n;j++) { if(j==0) cout&lt;&lt;board[i][j]; else{ cout&lt;&lt;&#39; &#39;&lt;&lt;board[i][j]; } } if(i!=n-1) cout&lt;&lt;endl; } return 0; } 真就是只改了下参数。。。。水题]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode127]]></title>
    <url>%2F2019%2F11%2F09%2FLeetcode127%2F</url>
    <content type="text"><![CDATA[思路分析转化为图来做两节点能转化，说明两节点连通，所以该问题变成，求图中begin节点和end节点是否可达，可达的最短路径求解1.建图2.BFS遍历图 class Solution { public: map]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode130]]></title>
    <url>%2F2019%2F11%2F08%2FLeetcode130%2F</url>
    <content type="text"><![CDATA[130.被围绕的区域被围绕的区域.题目还是很简单的：就沿着边沿做几次DFS，把所有和边沿连同的路径上的’O’标记出来，剩下的’O’就是被包围起来的，将其翻转为’X’即可 class Solution { public: void DFS(int r,int c,vector]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LuoguP1141]]></title>
    <url>%2F2019%2F11%2F08%2FLuoguP1141%2F</url>
    <content type="text"><![CDATA[P1141 01迷宫题目描述有一个仅由数字0与1组成的n×n格迷宫。若你位于一格0上，那么你可以移动到相邻4格中的某一格1上，同样若你位于一格1上，那么你可以移动到相邻4格中的某一格0上。你的任务是：对于给定的迷宫，询问从某一格开始能移动到多少个格子（包含自身）。 输入格式第1行为两个正整数n,m 下面n行，每行n个字符，字符只可能是0或者1，字符之间没有空格。 接下来m行，每行2个用空格分隔的正整数i,j，对应了迷宫中第i行第j列的一个格子，询问从这一格开始能移动到多少格。 输出格式m行，对于每个询问输出相应答案。 输入输出样例输入 2 2 01 10 1 1 2 2 输出 4 4 先分析一下，可以发现，对于某一种遍历方式，所经过的节点都共享该遍历方式；（无论从哪一点出发，总会按照该方式遍历迷宫）并且很容易知道，某点有且仅有一种遍历迷宫方式（因为每种方案总是完全的） 所以可以少算很多点，打表做 # include&lt;cstdio&gt; # include&lt;cstring&gt; # include&lt;iostream using namespace std; int n,m,ans[100002],x,y,node[1002][1002]; char s[1002][1002]; void dfs(int row,int col,int val,int index){ if (row&lt;0 || row&gt;=n || col&lt;0 || col&gt;=n || node[row][col]!=-1/*已经被一种方案遍历过*/ || s[row][col]-&#39;0&#39;!=val) return; node[row][col]=index;//路径上的点指向同一answer,共享答案 ans[index]++;//改节点为ans[index]贡献一个节点数 dfs(row-1,col,!val,index); dfs(row+1,col,!val,index); dfs(row,col-1,!val,index); dfs(row,col+1,!val,index); } int main() { cin&gt;&gt;n&gt;&gt;m; for (int i=0;i&lt;n;i++) scanf(&quot;%s&quot;,s[i]); memset(node,-1,sizeof(node)); for(int i=0;i&lt;m;i++) { cin&gt;&gt;x&gt;&gt;y; x--; y--; if (node[x][y]==-1) dfs(x,y,s[x][y]-&#39;0&#39;,i); else ans[i]=ans[node[x][y]]; } for (int i=0;i&lt;m;i++) cout&lt;&lt;ans[i]&lt;&lt;endl; return 0; }]]></content>
      <tags>
        <tag>Luogu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BFSandDFS]]></title>
    <url>%2F2019%2F11%2F07%2FBFSandDFS%2F</url>
    <content type="text"><![CDATA[BFSandDFSa)迷宫求解（最少步数）b)水池数目(NYOJ27)c)图像有用区域(NYOJ92)d)树的前序中序后序遍历 DFS深度优先搜索上例子寻找二叉树上从根结点到给定结点的路径： 一.递归实现/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: stack&lt;TreeNode*&gt;P,Q; bool PFlag=false,QFlag=false; void DFS(TreeNode* root, TreeNode* goal,stack&lt;TreeNode*&gt;&amp;road,bool&amp;HasFind) { /*对树DFS寻找两个目标节点，然后存储路径，仅为寻找到最近的共同祖先（要不然root就是所有的祖先，那还找啥）*/ if(HasFind)//已经在别的地方找到了 return; if(root==goal)//现在刚刚找到 { HasFind=true;//flag置为1 road.push(root); return; } road.push(root); if(root-&gt;left)//DFS非空左子树 { DFS(root-&gt;left,goal,road,HasFind); if(!HasFind) road.pop();//子树中未找到目标节点，还原路径 } if(root-&gt;right)//DFS非空右子树 { DFS(root-&gt;right,goal,road,HasFind); if(!HasFind) road.pop();//子树中未找到目标节点，还原路径 } } TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { DFS(root,p,P,PFlag); DFS(root,q,Q,QFlag); stack&lt;TreeNode*&gt;Proad,Qroad; while(!P.empty()) { Proad.push(P.top()); P.pop(); } while(!Q.empty()) { Qroad.push(Q.top()); Q.pop(); } P=Proad; Q=Qroad; int MAXSize=P.size(); if(MAXSize&lt;Q.size()) { MAXSize=Q.size(); } TreeNode* temp=root; while(1) { if(P.empty()||Q.empty()) break; if(P.top()==Q.top()) { temp=P.top(); P.pop(); Q.pop(); }else break; } return temp; } }; 给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” 例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5] 示例 1: 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 输出: 6 解释: 节点 2 和节点 8 的最近公共祖先是 6。 写这个题的时候，额。。。没看树的类型，直接敲了树的DFS，其实可以利用二叉检索树的特性的，服了，算了，反正目的达到了。下回有机会再写吧。 非递归实现BFSLeetcode 127.解法. 题目给定两个单词（beginWord 和 endWord）和一个字典，找到从 beginWord 到 endWord 的最短转换序列的长度。转换需遵循如下规则： 每次转换只能改变一个字母。转换过程中的中间单词必须是字典中的单词。说明: 如果不存在这样的转换序列，返回 0。所有单词具有相同的长度。所有单词只由小写字母组成。字典中不存在重复的单词。你可以假设 beginWord 和 endWord 是非空的，且二者不相同。示例 1: 输入:beginWord = “hit”,endWord = “cog”,wordList = [“hot”,”dot”,”dog”,”lot”,”log”,”cog”] 输出: 5 解释: 一个最短转换序列是 “hit” -&gt; “hot” -&gt; “dot” -&gt; “dog” -&gt; “cog”, 返回它的长度 5。示例 2: 输入:beginWord = “hit”endWord = “cog”wordList = [“hot”,”dot”,”dog”,”lot”,”log”] 输出: 0 解释: endWord “cog” 不在字典中，所以无法进行转换。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/word-ladder著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ProcessOfEquivalentPair]]></title>
    <url>%2F2019%2F11%2F06%2FProcessOfEquivalentPair%2F</url>
    <content type="text"><![CDATA[连通区域标记－行程扫描算法— 等价对的处理过程将等价对转换为若干个等价序列,比如有如下等价对: 只要两个数在同一序偶中，就用线连起来，化成一棵树经过等价对处理之后,应得到的等价序列为: list1:1-2-5-6-8-10-11-12-13-14-15 list2:3-7-9 list3:4 主要思路:将1-15个点都看成图的节点,而等价对说明节点1和2之间有通路,而且形成的图是无向图.我们需要遍历图,找到其中的所有连通图,主要采用的是图的深度优先遍历法,进行等价序列的查找. 如上图所示,从节点1开始查找,它有三个路径1-2,1-6,1-8;2和6后面没有路径,8后面有两个路径8-10,8-11;10后面没有路径,11后面有5条路径分别为11-5,11-12,11-13,11-14,11-15,到此,等价表1查找完毕. 等价表2是从3开始查找,它的后面有两条路径3-7,3-9. 等价表3只有一个孤立的点. 并查集父节点表示法对于树中的每个结点都保存一个指针域指向父结点。（缺点：不能准确地找到给定的结点的子结点信息。优点：可以解决判断俩个结点是否在同一个树中的问题；用并查算法合并俩个集合。） 基本操作查询元素a和元素b是否属于同一组：由于采用父节点表示法，使用递归，一直向上检索，判断两个节点能否走到同一个根（查询是否有共同祖先祖先），即可知道是否在一组中。 在下图，2和5都走到了1，因此他们为同一组。另一方面，由于7走到的是6，因此与2和5属于不同组 合并元素a和元素b所在的组：像下图一样，从一个组的根向另一个组的根连边，这样两棵树就变成了一棵树，也就是把两个组合并为了一个组。将其中一颗树的根节点作为另一颗树的子节点： 并查集实现中的注意点(优化)(降低rank）退化问题在树形数据结构中，如果发生了退化的情况，那么复杂度就会变得很高。因此有必要避免退化（呈线性，树形结构不明显，丧失树形优势）1.对于每棵树，记录这棵树的高度（rank）2.合并时如果两个数的rank不同，那么从rank小的向rank大的连边。（大的连在小rank树上，会在大rank基础上增加深度；小的连在大的上，就可能会被包含，并不增加rank） 路径压缩不论是所查询的节点，还是在查询过程中向上经过的其他所有节点，都直接改为直接连到根上。这样再次查询这些节点时，就可以很快知道根是谁了。（降低rank）]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BasicAlgorithm]]></title>
    <url>%2F2019%2F11%2F02%2FBasicAlgorithm%2F</url>
    <content type="text"><![CDATA[一些板子 一些基本算法位运算左移variable = variable &lt;&lt; numvariable &lt;&lt;=num观察可以发现，左移一位的结果就是原值乘2，左移两位的结果就是原值乘4。右移variable = variable &gt;&gt; numvariable &gt;&gt;=num右移一位的结果就是原值除2，左移两位的结果就是原值除4，注意，除了以后没有小数位的，都是取整。按位与：字面意思variable &amp; 1 -&gt; 拿到最低位的数字variable &amp; 00000001 -&gt;0000000a 快速积(取模)精髓：化乘法为多次加法FastSpeedMultiplying typedef long long int ll; ll Mulitply(ll a,ll b,ll c) { ll ans=0; while(b) { if(b&amp;1) ans= (ans+a)%c; a= (a&lt;&lt;1)%c; //感觉还是可能会在a+a做加法的时候溢出，必要的时候做一个 (a mod c + a mod c)mod c b&gt;&gt;=1; } return ans; } 快速幂(取模)战神级讲解 ll Quick_Power(ll a,ll b,ll c) { ll ans=1; while(b) { if(b&amp;1) ans= Mulitply(ans,a,c); a=Mulitply(a,a,c); b&gt;&gt;=1; } return ans; } a)欧几里德算法求最大公约数 实现一：# include &lt;iostream&gt; using namespace std; int gcd(int m, int n) { int r = 0; while(n!=0) { r=m%n; m=n; n=r; } return m; } int main() { int m,n; cout&lt;&lt;&quot;请输入2个正整数：&quot;&lt;&lt;endl; cin&gt;&gt;m&gt;&gt;n; cout&lt;&lt;endl&lt;&lt;&quot;最大公约数为：&quot;&lt;&lt;gcd(m,n)&lt;&lt;endl; return 0; } 实现二：int gcd(int a,int b) {//b是上一层的余数 a%b是这一层的余数 a是除数 return b==0?a:( gcd(b,a%b) ); } b)筛法求素数（朴素筛和快速线性筛）埃拉托斯特尼筛法，简称埃氏筛或爱氏筛，是一种由希腊数学家埃拉托斯特尼所提出的一种简单检定素数的算法。要得到自然数n以内的全部素数，必须把不大于根号n的所有素数的倍数剔除，剩下的就是素数。为啥终止是根号n呢， 首先我们要明确，假设一个合数x能表示为两个数的乘积，他必定有一个小于等于sqrt(x)的因子，这可以用归谬证明法证明。如果两个因子都大于sqrt(x)，那么乘积大于x,这和假设矛盾。 然后就是，根据埃氏筛的思想，拿到当前序列的第一个新的素数，然后往后遍历筛选。因而对于序列较后面的数，在小于等于根号n的范围内总有机会被 其小于等于sqrt(x)的因子 筛一次（如果这个数真是合数的话），进而被筛去 大致过程：(1).把2到n的自然数放入a[2]到a[n]中(所放入的数与下标号相同) ; (2).在数组元素中以下标为序，按顺序找到未曾找过的最小素数minp和它的位置p(即下标号); (3).从p+1开始，把凡是能被minp整除的各元素值从a数组中划去(筛掉)，也就是把该元素标记为0;举个例子：minp=3 就把 3*3 3*4 3*5….筛掉，即PrimeFlag标记为0，is not a prime（也是缺点来源） 至于为什么从minp*minp开始筛:不需要从i*j（且j &lt; i）开始，因为i*j，在遇到j时已经被标记了，因为j比i小，所以遇到j比遇到i要早。同时因为从i*i开始标记，所以i终止条件也为sqrt(n)，否则i*i将大于n。（算是第二个方面限制i的范围在根号n内吧） (4).让p=p+1，重复执行第(2) (3)步骤，知道，minp&gt;floor(sqrt(n))为止; (5).打印输出a数组中留下来的数，未被筛掉的各元素值; int isPrime_sieve(int n){ int* isPrimes = (int*)malloc(sizeof(int)*(n+1)); int i,j; int sqrtn = sqrt(n); if(n&lt;=1) return 0; for(i=2;i&lt;=n;i++){ //初始化都为素数 isPrimes[i] = 1; } //从2开始，将素数的倍数标记为非素数 //从i的平方开始标记即可，不需要从i*j(且j&lt;i)开始，因为i*j至少在遇到j时已经被标记过了 for(i=2;i&lt;=sqrtn;i+=1){ if(isPrimes[i]==0) //不是素数，说明i可以分解为两个因子相乘，那么在遇到这两个因子的较小者时，i的倍数已经被标记过 continue; for(j=i*i;j&lt;=n;j+=i){ //j是i的倍数 isPrimes[j] = 0; } } return isPrimes[n]; } 这种求素数的算法很容易被理解，其时间复杂度介于O(n)~O(nlogn)是一种比较流行的方法。但是同样的，这种算法也存在先天性的缺陷，我们简单分析：对于一个数30，可分解为30=2*15=3*10=5*6，显然，当循环,2,3,5,6,10,15时都会筛除一次30这个数，而当n很大时，就会出现许多的冗余操作， 快速线性筛：(1).开一个n+1大小的数组num[]来存放每一个元素的筛留情况(num[i]用来表示i这个数是不是素数对于任意num[i] 有num[i]=0,num[i]=1两种情况，如果num[i]=0则是素数，反之num[i]=1时是合数); (2).再开一个数组prime[n]来存放筛出的素数以便最后输出结果; (3).对于一个数i,总是进行从i*prime[0]~i*prime[j](由小到大来乘)，直到if(i%prime[j]==0)成立时break掉 对（3）解释一下：第一，所有数（2~n）都会被作为i拿来判定，即第一层循环第二，为什么是从i*prime[0]~i*prime[j](由小到大来乘) 前提是：一个合数 i=p1*p2*…*pn, pi都是素数（2&lt;=i&lt;=n）,pi&lt;=pj ( i&lt;=j )p1是最小的系数。这样每一个合数就有一个确定的表示方法，不会重复。（像12=223）No.1：我们现在规定一个合数由两个数得到。NO.2：那么合数有两种。1.素数*素数=合数2.一个最小的素数*合数=合数 至于为什么if(i%prime[j]==0)成立时break掉 我们约定一下：记作 c*i而 c=a*b（b为c的最小质因数），按照结论，对于c这一组，只应当把i的值取到b因为假如取到了b+1 记为d=b+1, 则 迭代到 c*d而c=a*b 故 迭代到了 a*b*d 即 a*d*b -&gt; c’*b因而c*d==c’*b 这个数被多次判定 造成冗余 举个例子，对于一个数9，9*2=18将18标记为合数，循环继续；9*3=27将27标记为合数，此时发现9%3=0，循环退出。如果将循环继续下去会出现筛除9*5=45的情况，会对45进行一次筛选，而45=15*3，在15时会被在筛去一次，故不可行 # include&lt;iostream&gt; # include&lt;cstdio&gt; # include&lt;ctime&gt; # include&lt;cmath&gt; # define inf 20000005 using namespace std; int n; bool a[inf+1]; bool num[inf+1]={1,1}; int prime[inf+1]={0},number=0; //number 记录素数个数 void putongshaifa() //普通筛法求素数 { clock_t begin,end; begin=clock(); for(int i=0;i&lt;=n;++i) a[i]=true; //初始化 全是素数 a[1]=false; //1不是素数 for(int i=2;i&lt;sqrt(n);++i) if(a[i])//新的第一个素数 for(int j=2;j&lt;=n/i;++j) a[i*j]=false; //i*1 i*2 i*3...一直往后筛 end=clock(); /*for(int i=2,t=0;i&lt;=n;++i) if(a[i]) { cout&lt;&lt;i&lt;&lt;&quot; &quot;; ++t; if(t%10==0) cout&lt;&lt;endl; } cout&lt;&lt;endl;*/ printf(&quot;普通筛法-Time used:%d ms\n&quot;,end-begin); return; } void kuaisushaifa() //快速筛法求素数 { clock_t begin,end; begin=clock(); for(int i=2;i&lt;=n;++i) //遍历所有2~n数 -&gt;i { if(!num[i]) prime[number++]=i; /*先用number的值再自加*/ for(int j=0;j&lt;number &amp;&amp; i*prime[j]&lt;=n;j++) { num[i*prime[j]]=1; //把所有合数标记为 1 if(!(i%prime[j]) /*i%prime[j]==0*/ ) // *为保证不重复筛选* break; } } end=clock(); /*for(int i=0;i&lt;number;i++) { if(i%10==0) printf(&quot;\n&quot;); printf(&quot;%3d&quot;,prime[i]); } */ printf(&quot;快速筛法-Time used:%d ms\n&quot;,end-begin); return; } int main() { //freopen(&quot;prime.txt&quot;,&quot;w&quot;,stdout); scanf(&quot;%d&quot;,&amp;n); int _test=10; while(_test--) { putongshaifa(); kuaisushaifa(); cout&lt;&lt;endl; } return 0; } # include&lt;iostream&gt; using namespace std; const int MAX=1000; bool IsPrime[MAX]; int Prime[MAX]; int main() { int n; cin&gt;&gt;n; int *a= new int[n+5]; for(int i=1;i&lt;=n;i++) { a[i]=i; IsPrime[i]=1; } int CurrPrime=0; IsPrime[1]=0; IsPrime[2]=1; for(int i=2;i&lt;=n;i++) { if(IsPrime[i]) { Prime[CurrPrime]=i; CurrPrime++; } for(int j=0;j&lt;CurrPrime&amp;&amp;i*Prime[j]&lt;=n;j++) { IsPrime[i*Prime[j]]=0; if(i%Prime[j]==0) { break; } } } for(int i=2;i&lt;=n;i++) { if(IsPrime[i]) { cout&lt;&lt;i&lt;&lt;&quot; is Prime\n&quot;; }else{ cout&lt;&lt;i&lt;&lt;&quot; is not a prime\n&quot;; } } return 0; } c)康托展开公式： rank = a_{n}\cdot(n-1)!+a_{n-1}\cdot(n-2)!+...+a_{1}\cdot 0!举例套用公式和理解：1,2,3,4的各个全排列按照字典序排列得到的rank 从0开始，即：（1234）的rank=0 其中，a_{i}的取值：先取在本排列中的第i位数字（最右边为第1位数字），在还未出现的元素中序列中，求出该数字的排名。 举个例子：（3214)\_{rank}= 2\*(4-1)!+1\*(4-2)!+0\*(4-3)!+0\*0!=14 设已经出现（或者说已经参与排列的元素）集合为S 初始化S={} 补集！S={1，2，3，4} 开始从给出的排列序列的最高位判断 3：处在3的位置，对应a_{3}的求解， 在！S中 元素3 大小排在第三位，a_{3}=3-1=2 （或者理解为！S中比3小的数字有2个） S={3} ！S={1，2，4} 2：处在2的位置，对应a_{2}的求解， 在！S中 元素2 大小排在第二位，a_{2}=2-1=1 （或者理解为！S中比2小的数字有1个） S={3，2} ！S={1，4} 1：处在1的位置，对应a_{1}的求解， 在！S中 元素1 大小排在第一位，a_{1}=1-1=0 （或者理解为！S中比1小的数字有0个） S={3，2，1} ！S={4} 4：处在0的位置，对应a_{0}的求解， 在！S中 元素4 大小排在第一位，a_{0}=1-1=0 （或者理解为！S中比4小的数字有0个） S={3，2，1，4} ！S={} 其实，最后一位的值不用判断总是取0，因为此时S中只会剩下这唯一一个元素 rank=14 最后，如果要求3214在全排列中排第几位，应当写 rank+1 = 15 d)逆康托展开辗转相除：（没有找到严谨的数学推理，先这样吧）还是上面的例子：3，2，1，4 rank=14 14 % (3!)=2...2 2 % (2!)=1...0 0 % (1!)=0...0 0 % (0!)=0...0 (最后一行同样不需要计算) 则还原过程： 初始化S={} 补集！S={1，2，3，4} 排名为2 +1：从！S中取出升序排列第3的数字：3 S={3} 补集！S={1，2，4} 排名为1 +1：从！S中取出升序排列第2的数字：2 S={3，2} 补集！S={1，4} 排名为0 +1：从！S中取出升序排列第1的数字：1 S={3，2，1} 补集！S={4} 排名为0 +1：从！S中取出升序排列第1的数字：4 S={3，2，1，4} 补集！S={} rank为14的序列为 3，2，1，4 e)同余定理两个整数除以同一个整数，若得相同余数，则二整数同余 数论中的一个重要概念。给定一个正整数m，如果两个整数a和b满足a-b能够被m整除，即(a-b)/m得到一个整数，那么就称整数a与b对模m同余，也读作a同余于b模m，记作a≡b(mod m)。对模m同余是整数的一个等价关系a=m*d+fb=m*e+f=&gt;(a-b)%m=(d-e)*m%m=d-e (假设a&gt;=b) 2019/11/7/21：32 to be continued… f)次方求模—快速幂(取模)算法对于普通类型的a_{n}求法是，自乘n次a 快速幂：1）当b是奇数时，那么有 a^{b}=a\cdot a^{b}2）当b是偶数时，那么有 a^{b}=a^{b/2}\cdot a^{b/2} 递归法：举个例子 2^{10}= (2^{\frac{10}{2}})^{2} 2^{5} = 2\cdot (2^{\frac{4}{2}})^{2}long long int binaryPow(long long int a, long long int b, long long int m){ if(b == 0) return 1; else if(b % 2 == 1) return a * binaryPow(a, b - 1, m) % m; else{ long long int num = binaryPow(a, b/2, m) % m; //优化 return num * num % m; // 不直接写成return binaryPow(a, b/2, m) * binaryPow(a, b/2, m) } } 迭代法：13=8*1+4*1+2*0+1*1 = 2^{3}*1+2^{2}*1+2^{1}*0+2^{0}*1 long long int binaryPow(long long int a, long long int b, long long int m){ long long int ans = 1; while(b &gt; 0){ if(b &amp; 1){//判断当前b的二进制位最低位 是否为1 从而提供乘的依据 ans = ans * a % m;//根据需要 是否乘2^{i} } a = a * a % m;//平方计数器 b &gt;&gt;= 1; //右移一位 } return ans; }]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LearningPath]]></title>
    <url>%2F2019%2F11%2F02%2FLearningPath%2F</url>
    <content type="text"><![CDATA[http://history.ccfccsp.org.cn/临时路线. 需要掌握以下基本算法：a)欧几里德算法求最大公约数b)筛法求素数c)康托展开d)逆康托展开e)同余定理f)次方求模计算几何初步a)三角形面积b)三点顺序 学会简单计算程序的时间复杂度与空间复杂度二分查找法简单的排序算法a)冒泡排序法b)插入排序法贪心算法经典题目]]></content>
  </entry>
  <entry>
    <title><![CDATA[MaxHeap]]></title>
    <url>%2F2019%2F11%2F01%2FGensokyo%2F</url>
    <content type="text"><![CDATA[最大堆和最小堆最大堆和最小堆是二叉堆的两种形式。（完全二叉树） 最大堆：根结点的键值是所有堆结点键值中最大者，且每个结点的值都比其孩子的值大。 最小堆：根结点的键值是所有堆结点键值中最小者，且每个结点的值都比其孩子的值小。构建方式不唯一 下面对书本上的bulid过程分析并实现：（仅仅分析最大堆，则最小堆可以重载大于号等进行构建） 关键操作：siftdown(节点下沉、下拉) 构建的算法原理基础：归纳证明假设根的两个子树都是最大堆了，设根的元素为R，则有两种情况：一、R的值大于或者等于其两个子节点，此时最大堆结构完成二、否则，R与两个子节点中大的那一个交换位置，此时最大堆结构完成，或者重复第二步直到完成 为了使得子树已经是最大堆，则可以采用递归（我们不用），或者倒着循环上来下面采用循环实现BuildHeap： 初始化curr=HeapSize/2（最后一个分支节点，开始开倒车） BuildMaxHeap# include&lt;iostream&gt; # include&lt;cstring&gt; # include&lt;algorithm&gt; using namespace std; int a[1000]; void OrderPrint(int n) { int line=2; for(int i=1;i&lt;=n;i++) { cout&lt;&lt;&#39; &#39;&lt;&lt;a[i]&lt;&lt;&#39; &#39;; if(i==line-1) {// 1 3 7 cout&lt;&lt;endl; line*=2; } } } int main() { int n; cin&gt;&gt;n; memset(a,0,1000*sizeof(int)); for(int i=1;i&lt;=n;i++) { cin&gt;&gt;a[i]; } int HeapSize=n; for(int curr=HeapSize/2;curr&gt;=1;curr--) { int son=curr*2;//son 其实可以理解为 变动元素的下一个待判断的位置 //a[0]=a[curr]; while(son&lt;=HeapSize)// //待判断的位置 的 上限 { if(a[son]&lt;a[son+1]) { son++; } if(a[son/2]&gt;=a[son]) {/*父节点大于最大子节点 堆结构 构造完成*/ break;//停止下沉 } else{//节点小于最大子节点 swap(a[son/2],a[son]);//父变子 子变父 son*=2;//待判断的位置 移动到下一层 } } } OrderPrint(HeapSize); return 0; } 插入：添加到数组尾部（n+1处）比较简单和其父节点比较一路 上浮 到合适的位置即可可以调用OrderPrint方法查看上浮全程（其实我拿来debug的，因为我这a[0]的元素会干扰，a[0]并非堆中的节点） void MaxHeapInsert(int element,int &amp; HeapSize) { HeapSize++; a[HeapSize]=element; int curr=HeapSize; while(curr/2&gt;0&amp;&amp;a[curr/2]&lt;a[curr]) { //OrderPrint(HeapSize); swap(a[curr/2],a[curr]); curr/=2;//节点上浮 //OrderPrint(HeapSize); } } 删除：最大堆的删除，只能删除堆的最大值（根节点）把最后一个元素移动到根节点来当老大，然后siftdown对堆重排]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PythonNote]]></title>
    <url>%2F2019%2F10%2F27%2FpythonLearning%2F</url>
    <content type="text"><![CDATA[python的一些常用接口、机制 lambda 排序 广播机制 lambda普通函数 12def fuc(x): return x\*x 匿名函数 1testFuc=lambda x:x*x 测试 12345print(fuc(5))print(testFuc(5))2525 多关键字排序123456arr=[('a',1),('b',5),('c',3),('d',3),('d',7),('e',2),('f',10)]arr=sorted(arr,key=lambda x:(x[1],x[0]))print(arr)[('a', 1), ('e', 2), ('c', 3), ('d', 3), ('b', 5), ('d', 7), ('f', 10)] 匿名函数：lambda x:(x[1],x[0])，规定了arr中的元组元素x，以x[1]作为第一关键字，x[0]为第二关键字，均为升序。对于数值类型，我们可以通过添加负号实现降序，如：lambda x :(x[1],-x[0])），其指定第二关键字为x[0]的相反数，实现降序。 广播机制numpy 矩阵与向量的运算 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npmat=np.random.randint(1,10,size=(5,4))print(mat)vec=np.random.randint(1,10,size=(5,1))print(vec)print(mat/vec)vec=np.random.randint(1,10,size=(1,4))print(vec)print(mat/vec)形状为5x4的矩阵mat:[[9 1 4 2] [9 2 3 6] [3 2 4 4] [5 9 5 7] [2 8 6 1]]形状为5x1的向量vec:[[4] [4] [7] [4] [6]]mat/vec:mat的每一个列向量与vec逐元素相除[[2.25 0.25 1. 0.5 ] [2.25 0.5 0.75 1.5 ] [0.42857143 0.28571429 0.57142857 0.57142857] [1.25 2.25 1.25 1.75 ] [0.33333333 1.33333333 1. 0.16666667]] 形状为1x4的行向量vec:[[6 2 2 6]]mat/vec:mat的每一行向量与vec逐元素相除[[1.5 0.5 2. 0.33333333] [1.5 1. 1.5 1. ] [0.5 1. 2. 0.66666667] [0.83333333 4.5 2.5 1.16666667] [0.33333333 4. 3. 0.16666667]] 排序1list.sort(cmp=None, key=None, reverse=False) 该方法没有返回值 步长为step遍历123test=[1,2,3,4]step=2demo=[test[i:i+step] for i in range(0,len(test),step)] 累积操作 e.g., 累乘/累加itertools.accumulate(iterable[, func]) 1234a=[1,2,3,4,5]b=list(accumulate(a))b=list(accumulate(a,operator.mul)) 随机数123random.randint(a,b)random.random() 返回[0.0,1.0)random.uniform(a,b) 返回[a,b]的随机浮点 二分查找bisect库 12bisect_left(a,x,lo=0, hi=len(a))bisect_right(a,x,lo=0,hi=len(a)) 深拷贝1matCopy=copy.deepcopy(mat) 反转1list.reverse() 1slice op: [::-1]]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
